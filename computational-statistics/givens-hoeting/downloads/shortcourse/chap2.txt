# UNIVARIATE NEWTON'S METHOD
# Example 2.2 on page 25 in Givens and Hoeting
# 
# Key idea:  apply Newton's method to simple function.  Find x to
# maximize: g(x)=log(x)/(1+x) 

#first plot function using grid of values
z=seq(2.63,4,len=200)  
gz=log(z)/(z+1)  
plot(z,gz,type="l",xlab="x",ylab="g(x)")  

#__________________________________________________________________

newton.for.g=function(initial.value,num.its) {
# Performs Newton's method to find the x value that maximizes
# g(x)=log(x)/(1+x). 
#
# Inputs:  
# initial.value = starting value for optimization
# num.its = number of iterations
#
# Outputs:
# out = matrix with two columns
# column 1 = iteration number
# column 2 = the sequence of updated estimates of the maximum of the function

theta=rep(0,10)
theta[1]=initial.value
for (i in 2:10) {
# The h below implements formula (2.12) on page 25 for the increment.
# It is -g'/g''
   h=(1+1/theta[i-1]-log(theta[i-1]))*(1+theta[i-1])/
     (3+4/theta[i-1]+1/theta[i-1]^2-2*log(theta[i-1])) 

   theta[i]=theta[i-1]+h 
}
out=cbind(1:num.its,theta)
dimnames(out)[[2]]=c("iteration","estimate")
return(out)
}

# Try it for 10 iterations starting from 3.0
newton.for.g(3,10)


#__________________________________________________________________
# EXERCISES  2.1
# a. What value of x minimizes the function g(x)=log(x)/(1+x)? (examine
#    the results in the vector theta)
# b. How many iterations did it take for the procedure to converge? 
# c. Try some terrible starting values and try to cause a failure.
# d. Create your own function based on this example so that the
#    algorithm stops when the relative convergence criterion < epsilon,
#    where epsilon is a function argument.  Also make the function
#    more generic so that it can be used to optimize any
#    univariate function with a user-input Newton increment.
#    



#__________________________________________________________________
#__________________________________________________________________
# MULTIVARIATE NEWTON'S METHOD
# Example 2.4 on page 33 in Givens and Hoeting, 2005
# 
# Key idea:  apply Newton's method to find the values that maximize a
# complicated bivariate function.   

# Notes:  The function used below was created to demonstrate the multivariate
# Newton approach 

messy.fcn=function(x,y) {
# This function creates a bivariate surface that is challenging for
# Newton's method
 z=-x*(y-1)-(1-x)^2+y-y^2
 z=z*sin(log(1.5-z+(.3*x-1)^2+(y+1)^2))
 z=z+exp(-(x-1)^2-(y+1)^2)-exp(-((x-2)^2-y^2)/4)
 z}
#__________________________________________________________________
# The next functions plot the objective function that we call messy.fcn
# above.  The resulting plot is similar to Figure 2.7 on page 33.  

# Install the akima package.  
# 
# If you have internet access, follow these menus:
# Packages->Install packages-> akima -> OK
# 
# If you don't have internet access, follow these menus:
# Packages->Install package(s) from local zip files.  When the "Select
# files" window comes up, find the path to the akima zip file that was
# included with this code and then click on the open button.

library(akima)  #has the interpolation function used below

# set up a grid for plotting.  The book's notatin is x=(x1,x2)
# but we use (x,y) here, for simplicity
x=rep(seq(.5,3.5,len=50),rep(50,50))
y=rep(seq(-2,1,len=50),50)          

# In the interp function below, we add additional constraints to
# produce an image plot with a smoother surface as compared to using
# the defaults in interp

hh=interp(x,y,messy.fcn(x,y),xo=seq(min(x),max(x),len=200),
	yo=seq(min(y),max(y),len=200)) 

image(hh,col=topo.colors(50),xlab="x",ylab="y") 
h=interp(x,y,messy.fcn(x,y))
contour(h,add=T,drawlabels=FALSE,nlevels=30)

#__________________________________________________________________

derivs=function(x,y) {
# This function computes the derivatives of "messy.fcn" that are
# required to implement the  multivariate Newton's method. These
# derivatives must be solved by hand. 

# inputs:
# x and y are the x and y coordinates of the function
#
# outputs:
# derivs.vector= the vector of derivatives where, for example, dzdx is
#	the derivative of the objective function,z, with respect to x and
#       d2zdx2 is the 2nd derivative of z with respect to x.

# We did the algebra required to get these components and this function
# computes those derivatives for various values of x and y.  In the code
# below the variables q,a,b, and cc are components of messy.fcn (see the
# code for messy.fcn).  Then the d's below stand for derivative.  So,
# for example, dqdx is the derivative of q with respect to x.  The
# objective function is called z here.

  q=-x*(y-1)-(1-x)^2+y-y^2
  a=-q+1.5+(.3*x-1)^2+(y+1)^2
  b=-(x-1)^2-(y+1)^2
  cc=(-(x-2)^2+y^2)/4
  dqdx=-(y-1)+2*(1-x)
  d2qdx2=-2*x
  dqdy=-x+1-2*y
  d2qdy2=-2
  d2qdxdy=-1
  dadx=-dqdx+.6*(.3*x-1)
  d2adx2=-d2qdx2+.6*.3
  dady=-dqdy+2*(y+1)
  d2ady2=-d2qdy2+2
  d2adxdy=-d2qdxdy
  dbdx=-2*(x-1)
  dbdy=-2*(y+1)
  d2bdx2=-2
  d2bdy2=-2
  d2bdxdy=0
  dccdx=(-2*(x-2))/4
  dccdy=(2*y)/4
  d2ccdx2=-.5
  d2ccdy2=.5
  d2ccdxdy=0
  #
  dzdx=dqdx*sin(log(a))+q*cos(log(a))*dadx/a+dbdx*exp(b)-dccdx*exp(cc)
  dzdy=dqdy*sin(log(a))+q*cos(log(a))*dady/a+dbdy*exp(b)-dccdy*exp(cc)
  d2zdx2=d2qdx2*sin(log(a))+dqdx*cos(log(a))*dadx/a+
         dqdx*cos(log(a))*dadx/a+q*(-sin(log(a)))*(dadx^2)/a^2+
         q*cos(log(a))*(dadx^2)/(-a^2)+q*cos(log(a))*d2adx2/a+
         d2bdx2*exp(b)+exp(b)*dbdx^2-d2ccdx2*exp(cc)-exp(cc)*dccdx^2
  d2zdy2=d2qdy2*sin(log(a))+dqdy*cos(log(a))*dady/a+
         dqdy*cos(log(a))*dady/a+q*(-sin(log(a)))*(dady^2)/a^2+
         q*cos(log(a))*(dady^2)/(-a^2)+q*cos(log(a))*d2ady2/a+
         d2bdy2*exp(b)+exp(b)*dbdy^2-d2ccdy2*exp(cc)-exp(cc)*dccdy^2
  d2zdxdy=d2qdxdy*sin(log(a))+dqdx*cos(log(a))*dady/a+
         dqdy*cos(log(a))*dadx/a+q*(-sin(log(a)))*dady*dadx/a^2+
         q*cos(log(a))*dadx*dady/(-a^2)+q*cos(log(a))*d2adxdy/a+
         d2bdxdy*exp(b)+exp(b)*dbdx*dbdy-
         d2ccdxdy*exp(cc)-exp(cc)*dccdx*dccdy
  derivs.vector=c(dzdx,dzdy,d2zdx2,d2zdy2,d2zdxdy)
derivs.vector }


# Now we are ready to implement the multivariate Newton's method 

steps=function(start,n) {
# This function implements the multivariate Newton's method (equation
#  2.33 on page 33).

# inputs
# start = starting value
# n = number of iterations
#
# outputs
# a 3 column matrix:
# column 1: iteration number
# columns 2 & 3: coordinates of current solution
# 
  stepz=matrix(0,n,2)
   stepz[1,]=start
   for (i in 2:n) {
     g=derivs(stepz[i-1,1],stepz[i-1,2])
     fprime=rbind(g[1],g[2])
     hess=rbind(c(g[3],g[5]),c(g[5],g[4]))
     stepz[i,]=c(cbind(stepz[i-1,])-solve(hess)%*%fprime) 
    }
   out=cbind(1:n,stepz)
   dimnames(out)[[2]]=c("iteration","x.estimate","y.estimate")
   return(out)
}

# Start the steps function at x=3.049362 and y=-0.2748447 and run for
# 25 iterations
steps(c(3.049362,-0.2748447),25)

#__________________________________________________________________
	
# These commands create a plot similar to Figure 2.7 on page 33
x=rep(seq(.5,3.5,len=50),rep(50,50))
y=rep(seq(-2,1,len=50),50)

library(akima)  

hh=interp(x,y,messy.fcn(x,y),xo=seq(min(x),max(x),len=200),
       yo=seq(min(y),max(y),len=200))
# adding these additional constraints produces an image plot with a
#    smoother surface 

image(hh,col=topo.colors(50),xlab="x",ylab="y") 
h=interp(x,y,messy.fcn(x,y))
contour(h,add=T,drawlabels=FALSE,nlevels=30)

# To mirror figure 2.7 on page 33, we use two different starting values.
# badnewt uses starting values that converge to a local maximum and
# goodnewt uses starting values that converge to the true maximum

badnewt=steps(c(3.123693,-0.2143986),12)[,-1]
lines(badnewt,lwd=3,lty=4)
points(badnewt,pch=16,cex=.4)

goodnewt=steps(c(3.049362,-0.2748447),12)[,-1]
lines(goodnewt,lwd=3)
points(goodnewt,pch=16,cex=.4)

text(badnewt[1,1]+.2,badnewt[1,2],"bad start",cex=.75)
text(goodnewt[1,1]-.05,goodnewt[1,2]+.2,"good\nstart",cex=.75)
points(badnewt[1,1],badnewt[1,2],pch=19,cex=1.4)
points(goodnewt[1,1],goodnewt[1,2],pch=19,cex=1.4)

points(badnewt[12,1],badnewt[12,2],pch=19,cex=1.4)
points(goodnewt[12,1],goodnewt[12,2],pch=19,cex=1.4)

#__________________________________________________________________
# EXERCISES  2.2
#
# The derivs function above shows that it takes some effort to
# construct the matrix of deriviatives in the multivariate case.  The R
# function "optim" is a general purpose optimization function that
# applies a variety of methods.  The default "Nelder-Mead" approach
# requires no derivatives.  The method="BFGS" implements the BFGS
# approach described on page 40.   Note that the optim function finds 
# the parameter values that MINIMIZE the objective function.  
#
# a. Use optim function to solve this optimization problem.  Select
#    method="BFGS".  Note: you'll need to rewrite messy.fcn() to 
#    accept a vector argument for use with optim(), and insert a
#    minus sign.
# b. If you start from badnewt[1,] and goodnewt[1,], do you get
#    the same results that you get when you use the basic Newton's method?
# c. Try a variety of starting values with our code and/or with optim().
#    Explore regions of divergence.  Plot the function over a wider
#    range to understand convergence/divergence behavior.
# d. Try using method="Nelder-Mead" in optim.  Compare the number of 
#    function calls required, compared to BFGS.  Compare the convergence 
#    behavior.


  
#__________________________________________________________________
#__________________________________________________________________

# NONLINEAR GAUSS-SEIDEL ITERATION
# Example 2.8 on page 43 in Givens and Hoeting, 2005
# 
# Key idea:  apply the Gauss-Seidel method to find the values that
# maximize a complicated bivariate function.   
  
# Note:  see the example "Multivariate Newton's Method" above for the
# objective function and the function to compute the derivatives that
# are required (these are called 'messy.fcn' and 'derivs')

# Gauss-Seidel iteration
# This code performs Gauss-Seidel iteration as described on page 43.
# In this case we optimize over each dimension in turn.  The nlm
# function used below is an R function minimizes a univariate function
# using a Newton-type method.  Examine this code and try to determine
# what each step is doing.  It may help your understanding if you
# perform the a and a1 steps outside the for loop.

start=c(3.049362,-0.2748447)
n=10
stepz=matrix(0,n,2)
stepz[1,]=start
a1=stepz[1,1]
b1=stepz[1,2]
for (i in 2:n) {
  #the ifelse() penalizes excursions outside (0,4) or (-2,2), respectively
  a=function(x) {abs(derivs(x,b1)[1])+ifelse(abs(x-2)>2,1e20,0)}
  a1=nlm(a,.1)$estimate
  b=function(x) {abs(derivs(a1,x)[2])+ifelse(abs(x)>2,1e20,0)}
  b1=nlm(b,ifelse(i==2,0,-.5))$estimate
  stepz[i,]=c(a1,b1) 
}
stepz

#__________________________________________________________________
#These commands create a plot similar to Figure 2.9 on page 44

library(akima) 
x=rep(seq(.5,3.5,len=50),rep(50,50))
y=rep(seq(-2,1,len=50),50)
hh=interp(x,y,messy.fcn(x,y),xo=seq(min(x),max(x),len=200),
	yo=seq(min(y),max(y),len=200))
image(hh,col=topo.colors(50),xlab="x",ylab="y") 
h=interp(x,y,messy.fcn(x,y))
contour(h,add=T,drawlabels=FALSE,nlevels=30)

# These lines show the steps of Gauss-Seidel iteration for messy.fcn in
# two dimentions. 
for (i in 1:9) {
  lines(c(stepz[i,1],stepz[i+1,1]),c(stepz[i,2],stepz[i,2]),lwd=3)
  lines(c(stepz[i+1,1],stepz[i+1,1]),c(stepz[i,2],stepz[i+1,2]),lwd=3)
  points(stepz[i,1],stepz[i,2],pch=16,cex=.4)
  }

points(stepz[10,1],stepz[10,2],pch=16,cex=.4)
text(stepz[1,1],stepz[1,2]+.15,"start")
text(stepz[10,1]-.05,stepz[10,2]+.15,"end")

#__________________________________________________________________
# EXERCISES 2.3
#
# a. Pick a well-understood 2-dim maximum likelihood problem (e.g., linear
#    regression or normal MLE) and solve it via Gauss-Seidel.
# b. Rosenbrock's Banana provides a very difficult 2-d optimization
#    challenge: banana=function(x1,x2) { 100*(x2-x1^2)^2 + (1-x1)^2 }.
#    Plot this over [-2,2]x[-2,2].  Try to get Gauss-Seidel working, from
#    an initial guess of (0,0).  Trace the slow path Gauss-Seidel takes,
#    over a contour plot of the function.  Plot some of the univariate
#    abs(deriv) functions to understand why this problem is hard.
#    HINTS: abandon nlm() and use 1-d calls like 
#    optim(2,[function],method="BFGS").  
#    Amazingly, 2-d BFGS works well in R for this function.
# c. Create a generic function that implements the Gauss-Seidel algorithm.
#    Include stopping criteria based on both a relative convergence criterion
#    and a maximum number of iterations.  As previously, assume that the
#    arguments to the function include the p coordinate derivatives.
#    WARNING: This will be quite challenging if p is itself an argument 
#    to the function.  For an easier project, assume p=3.
