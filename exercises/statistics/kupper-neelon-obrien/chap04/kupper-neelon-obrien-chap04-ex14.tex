
\noindent
\textbf{Exercise 4.14(a)}

\vskip 0.3cm
\noindent
First, note that $\theta = P(Y = 0) = e^{-\lambda}$; hence, $\lambda = -\,\log(\theta)$.
The likelihood function is thus:
\begin{equation*}
L(\theta)
\;=\; \prod_{i=1}^{n}\,P_{Y}(y_{i};\lambda)
\;=\; \prod_{i=1}^{n}\,e^{-\lambda}\dfrac{\lambda^{y_{i}}}{y_{i}!}
\;=\; \left(e^{-\lambda}\right)^{n}\,\dfrac{\lambda^{\sum_{i=1}^{n}y_{i}}}{\prod_{i=1}^{n}\,y_{i}!}
\;=\; \theta^{n}\,\dfrac{\left(-\,\log\theta\right)^{\sum_{i=1}^{n}y_{i}}}{\prod_{i=1}^{n}\,y_{i}!}
\end{equation*}
The log likelihood is thus:
\begin{equation*}
l(\theta)
\;=\; \log L(\theta)
\;=\; - \log K + n\log\theta + \left(\sum_{i=1}^{n}y_{i}\right)\log\!\left(-\,\log\theta\right)
\;=\; - \log K + n\log\theta + S\log\!\left(-\,\log\theta\right)\,,
\end{equation*}
where $S := \sum_{i=1}^{n}Y_{i}$.
Note that $S \sim \textnormal{Poisson}(n\lambda)$.
\begin{equation*}
\dfrac{\partial l}{\partial\theta}
\;=\; \dfrac{n}{\theta} + S\dfrac{1}{-\,\log\theta}\dfrac{\dd}{\dd\theta}\!\left(-\,\log\theta\right)
\;=\; \dfrac{n}{\theta} + S\cdot\dfrac{1}{\left(-\,\log\theta\right)}\left(-\dfrac{1}{\theta}\right)
\;=\; \dfrac{n}{\theta} + \dfrac{S}{\theta\log\theta}
\end{equation*}
\begin{equation*}
\left(\dfrac{\partial l}{\partial\theta}\right)^{2}
\;=\; \left(\dfrac{n}{\theta} + \dfrac{S}{\theta\log\theta}\right)^{2}
\;=\; \dfrac{n^{2}}{\theta^{2}} - 2\cdot\left(\dfrac{n}{\theta}\right)\cdot\left(\dfrac{S}{\theta\log\theta}\right) + \dfrac{S^{2}}{\left(\theta\log\theta\right)^{2}}
\;=\; \dfrac{n^{2}}{\theta^{2}} - \dfrac{2nS}{\theta^{2}\log\theta} + \dfrac{S^{2}}{\left(\theta\log\theta\right)^{2}}
\end{equation*}
\begin{eqnarray*}
E\!\left[\left(\dfrac{\partial l}{\partial\theta}\right)^{2}\right]
&=& \dfrac{n^{2}}{\theta^{2}} + \dfrac{2n\,E\!\left[\,S\,\right]}{\theta^{2}\log\theta} + \dfrac{E\!\left[\,S^{2}\,\right]}{\left(\theta\log\theta\right)^{2}}
\;\;=\;\; \dfrac{n^{2}}{\theta^{2}} + \dfrac{2n\,E\!\left[\,S\,\right]}{\theta^{2}\log\theta} + \dfrac{\textnormal{Var}\!\left[\,S\,\right] + E\!\left[\,S\,\right]^{2}}{\left(\theta\log\theta\right)^{2}}
\\
&=& \dfrac{n^{2}}{\theta^{2}} + \dfrac{2n\,\left(n\lambda\right)}{\theta^{2}\log\theta} + \dfrac{n\lambda + n^{2}\lambda^{2}}{\left(\theta\log\theta\right)^{2}}
\;\;=\;\; \dfrac{n^{2}}{\theta^{2}} - \dfrac{2n^{2}\log\theta}{\theta^{2}\log\theta} - \dfrac{n\log\theta\left(1 - n\log\theta\right)}{\theta^{2}\left(\log\theta\right)^{2}}
\\
&=& \dfrac{n}{\theta^{2}}\left(-\,n-\dfrac{1-n\log\theta}{\log\theta}\right)
\;\;=\;\; \dfrac{n}{\theta^{2}}\left(\dfrac{-n\log\theta-1+n\log\theta}{\log\theta}\right)
\\
&=& -\,\dfrac{n}{\theta^{2}\log\theta}
\;\;=\;\; \dfrac{n}{\theta^{2}\left(-\log\theta\right)}
\end{eqnarray*}
Thus, the Cram\'er-Rao Lower Bound for the variance of any unbiased estimator of $\theta$ is:
\begin{equation*}
\textnormal{CRLB}(\theta)
\;\;=\;\; \left(E\!\left[\left(\dfrac{\partial l}{\partial\theta}\right)^{2}\right]\right)^{-1}
\;\;=\;\; \dfrac{\theta^{2}\left(-\log\theta\right)}{n}
\;\;=\;\; \dfrac{\lambda}{ne^{2\lambda}}
\;\;=\;\; \dfrac{1}{e^{2\lambda}}\cdot\dfrac{\lambda}{n}
\end{equation*}
Next, we claim that $A^{S}$ is an unbiased estimator of $\theta$ for a suitably chosen constant $A$.
To this end, note that, since $S = \sum_{i=1}^{n}Y_{i} \sim \textnormal{Poisson}(n\lambda)$, the moment generating function of $S$ is:
\begin{equation*}
M_{S}(t)
\;\;=\;\; E\!\left[\,e^{tS}\,\right]
\;\;=\;\; e^{n\lambda\left(e^{t}-1\right)}
\end{equation*}
Hence,
\begin{equation*}
E\!\left[\,A^{S}\,\right]
\;\;=\;\; E\!\left[\,e^{(\log A)S}\,\right]
\;\;=\;\; M_{S}\!\left(\log A\right)
\;\;=\;\; e^{n\lambda\left(e^{\log A}-1\right)}
\;\;=\;\; \left(\theta^{-n}\right)^{A-1}
\end{equation*}
Now, we seek constant $A$ such that $E\!\left[\,A^{S}\,\right] = \theta$; so we set
\begin{equation*}
\theta
\;\;=\;\; E\!\left[\,A^{S}\,\right]
\;\;=\;\; \left(\theta^{-n}\right)^{A-1}\,,
\end{equation*}
which implies
\begin{equation*}
\log\theta
\;\;=\;\; -n\,(A-1)\log\theta
\quad\Longrightarrow\quad
A \;\;=\;\; 1 - \dfrac{1}{n} \;\; = \;\; \dfrac{n-1}{n}
\end{equation*}
Thus, we may now conclude:
\begin{equation*}
\widehat{\theta} \;\; := \;\; \left(\dfrac{n-1}{n}\right)^{S}
\end{equation*}
is an unbiased estimator of $\theta$.
Now,
\begin{eqnarray*}
\textnormal{Var}\!\left[\,\widehat{\theta}\,\right]
&=& E\!\left[\,\widehat{\theta}^{\,2}\,\right] - E\!\left[\,\widehat{\theta}\,\right]^{2}
\;\;=\;\; E\!\left[\,\left(\dfrac{n-1}{n}\right)^{2S}\,\right] - E\!\left[\,\left(\dfrac{n-1}{n}\right)^{S}\,\right]^{2} 
\\
&=& E\!\left[\,\exp\left\{S\cdot2\log\!\left(\dfrac{n-1}{n}\right)\right\}\,\right]
- E\!\left[\,\exp\left\{S\cdot\log\!\left(\dfrac{n-1}{n}\right)\right\}\,\right]^{2}
\\
&=& M_{S}\!\left(2\log\!\left(\dfrac{n-1}{n}\right)\right) - M_{S}\!\left(\log\!\left(\dfrac{n-1}{n}\right)\right)^{2}
\\
&=& \exp\!\left\{n\lambda\left(\left(1-\dfrac{1}{n}\right)^{2}-1\right)\right\}
- \exp\!\left\{2n\lambda\left(\left(1-\dfrac{1}{n}\right)-1\right)\right\}
\\
&=& \exp\!\left\{-2\lambda+\dfrac{\lambda}{n}\right\} - \exp\!\left\{-2\lambda\right\}
\;\;=\;\; \exp\!\left\{-2\lambda\right\}\left(\exp\dfrac{\lambda}{n} - 1\right)
\;\;=\;\; \dfrac{1}{e^{2\lambda}} \cdot \left(e^{\lambda/n}-1\right)
\\
&=& \dfrac{1}{e^{2\lambda}} \cdot \left(1 + \dfrac{\lambda}{n} + \dfrac{(\lambda/n)^{2}}{2!} + \sum_{k=3}^{\infty}\dfrac{(\lambda/n)^{k}}{k!} -1\right)
\;\;=\;\; \dfrac{1}{e^{2\lambda}} \cdot \left(\dfrac{\lambda}{n} + \dfrac{(\lambda/n)^{2}}{2!} + \sum_{k=3}^{\infty}\dfrac{(\lambda/n)^{k}}{k!}\right)
\\
&>& \dfrac{1}{e^{2\lambda}} \cdot \dfrac{\lambda}{n} \;\; = \;\; \textnormal{CRBL}(\theta)
\end{eqnarray*}
Hence, we see that the variance of the unbiased estimator $\widehat{\theta}$ of $\theta$ strictly exceeds the Cram\'er-Rao
Lower Bound, for each $n$. Since $S = \sum_{i=1}^{n}Y_{i}$ is a complete sufficient statistic for $\theta$ (look up complete statistic
for exponential families), we have
\begin{equation*}
E\!\left[\,\left.\widehat{\theta}\,\right\vert\,S\,\right]
\;\;=\;\; E\!\left[\,\left.\left(\dfrac{n-1}{n}\right)^{S}\,\right\vert\,S\,\right]
\;\;=\;\; \left(\dfrac{n-1}{n}\right)^{S}
\;\;=\;\; \widehat{\theta}
\end{equation*}
is the unique minimum variance unbiased estimator (MVUE) of $\theta$.
Since $\widehat{\theta}$ does NOT attain the Cram\'er-Rao Lower Bound for the variance of unbiased estimators of $\theta$,
we may now conclude that no unbiased estimators of $\theta$ attain that bound.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 1.0cm
\noindent
\textbf{Exercise 4.14(b)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 1.0cm
\noindent
\textbf{Exercise 4.14(c)}
