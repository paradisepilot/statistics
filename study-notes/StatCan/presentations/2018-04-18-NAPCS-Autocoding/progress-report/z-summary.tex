          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{center}
\begin{minipage}{5.5in}

	\textbf{Background}
	\begin{itemize}
	\item
		The Enterprise Statistics Division (ESD) has recently implemented a prototype
		NAPCS autocoding module for scanner data from a certain large Canadian
		retailer chain (referred to as Retailer A in this report) using a relatively new
		and very poweful machine learning technique called Xgboost (with linear
		regression as base learner).
		For information on Xgboost, see \cite{Chen2016}, \cite{CRANxgboost} 
		and \cite{deVito2017}.
	\item
		ESD used approximately one half of the available Retailer A data as training
		data, and the other half as testing data.
		ESD has reported that the resulting trained model achieved a 99.44\% accuracy
		based on testing data from Retailer A.
		For more details on ESD's methodology and results, see \cite{Hatko20180302}.
	\item
		ESD furthermore constructed a small additional testing data set from another
		large retailer chain (referred to as Retailer B in this report), and ESD has
		reported that the trained model (trained on Retailer A data) achieved an 82.6\%
		accuracy on the Retailer B testing data set (see \cite{Hatko20180302}). 
	\end{itemize}

	\vskip 0.5cm
	\textbf{Objective of this report}
	\begin{itemize}
	\item
		This report attempts to give an explanation of the (arguably unusually)
		high testing accuracy of 99.44\% mentioned above.
	\end{itemize}

	\vskip 0.5cm
	\textbf{Summary of results}
	\vskip 0.2cm
	The high testing accuracy of 99.44\% may be attributable to the following:
	\begin{itemize}
	\item
		Retailer A scanner data contain one internally used product category code
		(referred to as $Y$ hereinafter), as well as six other variables each of which
		gives a short textual description of the category of each given product.
	\item
		Five (referred to as $X_{1}, X_{2}, ... , X_{5}$ hereinafter) of these six
		product category description variables were used as predictor variables
		(a.k.a. features) for model training. 
	\item
		``Ground truth'' data for Retailer A were generated by ESD by manually
		creating the concordance table from $Y$ (Retailer A's internal product
		category code) to NAPCS code.
	\item
		For each unique combination $(x_{1}, \ldots, x_{5})$ of values of
		$(X_{1}, \ldots, X_{5})$, one can count the number of distinct values of
		$Y$ associated to $(x_{1}, \ldots, x_{5})$.
		This combinatorial exercise reveals that the variables
		$(X_{1}, \ldots, X_{5})$ very nearly unambiguously determine the value of $Y$.
	\item
		Consequently, learning/predicting NAPCS based on features that include
		$(X_{1}, \ldots, X_{5})$ very closely approximates learning/predicting from
		a two-step look-up table: first from $(X_{1}, \ldots, X_{5})$ to $Y$, and
		then from $Y$ to NAPCS, where the second step is an unambiguous, fully
		deterministic look-up table, while the first step happens to be very close
		to one (due to the content of $(X_{1}, \ldots, X_{5})$ and $Y$). 
	\end{itemize}

\end{minipage}
\end{center}


          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
