
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{Response propensity scores}

\begin{proposition}\label{PropnPropensityScore} 
\mbox{}\vskip 0.25cm
\noindent
Set-up:
\begin{itemize}
\item
	$U = \{1,2,\ldots,N\}$.
\item
	$y : U \longrightarrow \Re$ is an $\Re$-valued function defined on $U$.\;
	$\mathbf{x} : U \longrightarrow \Re^{p}$ is an $\Re^{p}$-valued function defined on $U$.
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
\item
	$I_{1},\ldots,I_{N},R_{1},\ldots,R_{N} : (\Omega,\mathcal{A},\mu) \longrightarrow \{0,1\}$
	are Bernoulli random variables defined on $(\Omega,\mathcal{A},\mu)$. 
\item
	Let \,$w : U \longrightarrow (0,\infty)$\, be a positive $\Re$-valued function defined on \,$U$.
	\vskip 0.05cm
	Define the random variable \,$\widehat{T}_{y} : \Omega \longrightarrow \Re$\, by:
	\begin{equation*}
	\widehat{T}_{y}(\omega)
	\;\; := \;\;
		\underset{i \in U}{\sum}\,\;w_{i}\,y_{i}\cdot R_{i}(\omega) \cdot I_{i}(\omega)
	\end{equation*}
\end{itemize}
Suppose:
\begin{itemize}
\item
	For each \,$i \in U$, the conditional distribution of the Bernoulli random variable
	\,$R_{i}$,\, given $I_{1},\ldots,I_{N}$, is completely determined by
	\,$\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}$\,;
	in other words,
	\,$P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,I_{1},\ldots,I_{N}\,\right)$\,
	is a function of \,$\mathbf{x}_{1},\ldots,\mathbf{x}_{N}$.
\item
	For each \,$i \in U$,
	\,$P\!\left(\,
		\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,
		I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right)$\,
	furthermore satisfies:
	\begin{equation*}
	P\!\left(\,
		\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,
		I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right)
	\;\; = \;\;
		P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,I_{i}\;;\,\mathbf{x}_{i}\,\right).
	\end{equation*}
\end{itemize}
Then,
\begin{equation*}
E\!\left[\;\;\widehat{T}_{y}\;\,\right]
\;\; = \;\;
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot P\!\left(\left.\overset{{\color{white}.}}{R_{i}}=1\;\right\vert\,I_{i}=1\;;\,\mathbf{x}_{i}\,\right) 
	\cdot P(\,I_{i}=1\,)
\end{equation*}
\end{proposition}
\proof
First, note that, since each $R_{i}$ is a Bernoulli random variable, we have
\begin{eqnarray*}
&&
	E\!\left(\,
	\left.\overset{{\color{white}.}}{R_{i}}\;\,\right\vert\,
	I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
	\,\right)
\\
& = &
	1 \cdot P\!\left(\,
		\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,
		I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right)
	\; + \;
	0 \cdot P\!\left(\,
		\left.\overset{{\color{white}.}}{R_{i}}=0\;\,\right\vert\,
		I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right)
\\
& = &
	P\!\left(\,
		\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,
		I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right)
\end{eqnarray*}
Similarly,
\begin{eqnarray*}
E\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}\;\,\right\vert\,I_{i}\;;\,\mathbf{x}_{i}\,\right)
& = &
	1 \cdot P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,I_{i}\;;\,\mathbf{x}_{i}\,\right)
	\; + \;
	0 \cdot P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=0\;\,\right\vert\,I_{i}\;;\,\mathbf{x}_{i}\,\right)
\;\; = \;\;
	P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,I_{i}\;;\,\mathbf{x}_{i}\,\right)
\end{eqnarray*}
Hence,
\begin{eqnarray*}
E\!\left[\;\;\widehat{T}_{y}\;\,\right]
& = &
	E\!\left(E\!\left[\;
		\left.\widehat{T}_{y}\;\right\vert\,I_{1},\ldots,I_{N}
		\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right]\,\right)
\;\; = \;\;
	E\!\left(E\!\left[\;\,
		\left.\underset{k \in U}{\sum}\;w_{i}\,y_{i}\,R_{i}\,I_{i}\;\;\right\vert\,I_{1},\ldots,I_{N}
		\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right]\,\right)
\\
& = &
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot
	E\!\left(\,E\!\left[\,
		\left.\overset{{\color{white}.}}{R_{i}\,I_{i}}\;\,\right\vert\,I_{1},\ldots,I_{N}
		\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right]\,\right)
\\
& = &
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot
	E\!\left(\,I_{i} \cdot E\!\left[\,
		\left.\,\overset{{\color{white}.}}{R_{i}}\;\,\right\vert\,I_{1},\ldots,I_{N}
		\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right]\,\right)
\\
& = &
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot
	E\!\left(\,I_{i} \cdot P\!\left(
		\left.\overset{{\color{white}.}}{R_{i}}=1\;\right\vert\,I_{1},\ldots,I_{N}
		\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
		\,\right)\,\right)
\\
& = &
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot
	E\!\left(\,I_{i} \cdot P\!\left(
		\left.\overset{{\color{white}.}}{R_{i}}=1\;\right\vert\,I_{i}
		\;;\,\mathbf{x}_{i}
		\,\right)\,\right)
\\
& = &
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot
	\left\{\;
		1 \cdot P\!\left(\left.
			\overset{{\color{white}.}}{R_{i}}=1\;\right\vert\,I_{i}=1
			\;;\,\mathbf{x}_{i}
			\,\right) \cdot P(\,I_{i}=1\,)
		\; + \;
		0 \cdot P\!\left(\left.
			\overset{{\color{white}.}}{R_{i}}=1\;\right\vert\,I_{i}=0
			\;;\,\mathbf{x}_{i}
			\,\right) \cdot P(\,I_{i}=0\,)
		\;\right\}
\\
& = &
	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i}
	\cdot P\!\left(\left.\overset{{\color{white}.}}{R_{i}}=1\;\right\vert\,I_{i}=1\;;\,\mathbf{x}_{i}\,\right) \cdot P(\,I_{i}=1\,)
%\\
%& = &
%	\underset{k \in U}{\sum}\;\,w_{i}\,y_{i} \cdot \phi_{i} \cdot \pi_{i}
\end{eqnarray*}
\qed

\begin{remark}
\mbox{}\vskip 0.05cm
\begin{itemize}
\item
	In Proposition \ref{PropnPropensityScore},
	if \,$P(\,I_{i}=1\,) >0$\, and \,$P(\,R_{i}=1\;\vert\,I_{i}=1\,) > 0$,\,
	for each \,$i \in U$,\, and the function \,$w : U \longrightarrow (0,\infty)$\, is given by:
	\begin{equation*}
	w_{i}
	\;\; = \;\;
		\dfrac{1}{\overset{{\color{white}.}}{P}(\,R_{i}=1\;\vert\,I_{i}=1\;;\,\mathbf{x}_{i}\,)
		\cdot P(\,I_{i}=1\,)}\,,
	\quad
	\textnormal{for each \,$i \in U$}\,,
	\end{equation*}
	then we furthermore have:
	\begin{equation*}
	E\!\left[\;\widehat{T}_{y}\,\right]
	\;\; = \;\;
		\underset{i \in U}{\sum}\;\,y_{i}
	\;\; =: \;\;
		T_{y}
	\end{equation*}
\item
	The quantity
	\begin{equation*}
	\phi_{i}
	\;\; := \;\;
		P\!\left(\,\left.R_{i}\overset{{\color{white}-}}{=}1\;\right\vert\,I_{i}=1\;;\,\mathbf{x}_{i}\,\right)
	\end{equation*}
	is called the \,\textbf{response propensity score}\, of unit $i \in U$.
\item
	In practical applications of Proposition \ref{PropnPropensityScore}, we assume
	that we know the values of \,$\mathbf{x}_{i} \in \Re^{p}$, for each \,$i \in U$.
	Thus, according Proposition \ref{PropnPropensityScore}, if we somehow
	know the values of the response propensity scores of the respondent selected units
	(i.e. we know \,$w_{i} > 0$ for \,$i \in U$, whenever $I_{i} = R_{i} = 1$),
	then we can construct an unbiased estimator for the population total of
	\,$y : U \longrightarrow \Re$.
\item
	In Proposition \ref{PropnPropensityScore}, the key assumptions are the following:
	\begin{itemize}
	\item
		For each \,$i \in U$, the conditional distribution of the Bernoulli random variable
		\,$R_{i}$,\, given $I_{1},\ldots,I_{N}$, is completely determined by
		\,$\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}$\,;
		in other words,
		\,$P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,I_{1},\ldots,I_{N}\,\right)$\,
		is a function of \,$\mathbf{x}_{1},\ldots,\mathbf{x}_{N}$.
	\item
		For each \,$i \in U$,
		\,$P\!\left(\,
			\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,
			I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
			\,\right)$\,
		furthermore satisfies:
		\begin{equation*}
		P\!\left(\,
			\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,
			I_{1},\ldots,I_{N}\;;\,\mathbf{x}_{1},\ldots,\mathbf{x}_{N}
			\,\right)
		\;\; = \;\;
			P\!\left(\,\left.\overset{{\color{white}.}}{R_{i}}=1\;\,\right\vert\,I_{i}\;;\,\mathbf{x}_{i}\,\right).
		\end{equation*}
	\end{itemize}
	The above assumptions together constitute what is known as the
	\,\textbf{Missing At Random}\, (MAR) assumption.
\item
	A special case of the Missing At Random assumption is that
	$P(\,R_{i} = 1\;\vert\,I_{i}\;;\,\mathbf{x}_{i}\,)$ is furthermore independent of
	\,$\mathbf{x}_{i}$.
	This stronger assumption is known as the
	\,\textbf{Missing Completely At Random}\, (MCAR) assumption.
\item
	Conversely, when the Missing At Random assumptions fails,
	we say that we have \,\textbf{non-ignorable nonresponse}.
\end{itemize}
\end{remark}


          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
