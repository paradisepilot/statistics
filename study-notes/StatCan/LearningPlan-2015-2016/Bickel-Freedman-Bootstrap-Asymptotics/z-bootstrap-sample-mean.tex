
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Bootstrap asymptotics for sample mean}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\newcommand{\Snm}{\mathcal{S}^{(n)}_{m}}
\newcommand{\Xnm}{\overline{X}^{(n)}_{m}}

\begin{theorem}[Bootstrap Central Limit Theorem for I.I.D. sample mean, Theorem 2.1 \cite{BickelFreedman1981}]
\mbox{}\vskip 0.1cm
\noindent
Let $\left(\Omega,\mathcal{A},\nu\right)$ be a probability space.
Let $X, X_{1}, X_{2}, \ldots : \Omega \longrightarrow \Re$ be a sequence
of independent and identically distributed $\Re$-valued random variables
defined on $\Omega$
{\color{red}with finite expectation value $\mu_{X} \in \Re$ and variance $\sigma^{2}_{X} < \infty$}.
%Let $P_{X}$ be the probability measure induced on $\Re$ by $X$.
%Suppose
%\begin{equation*}
%\mu_{X}
%\; := \; E\!\left[\,X\,\right]
%\; = \; \int_{\Re}\; x \; \d P_{X}(x)
%\; = \; \int_{\Omega}\; X(\omega) \; \d\nu(\omega) 
%\; \in \; \Re,
%\quad\textnormal{and}
%\end{equation*}
%\begin{equation*}
%\sigma^{2}_{X}
%\; := \; E\!\left[\,\left(X - \mu_{X}\right)^{2}\,\right]
%\; = \; \int_{\Re}\; (x - \mu_{X})^{2} \; \d P_{X}(x)
%\; = \; \int_{\Omega}\; \left(X(\omega) - \mu_{X}\right)^{2} \; \d\nu(\omega) 
%\; < \; \infty
%\end{equation*}
For each $n \in \N$, define:
\begin{equation*}
\overline{X}_{n} \;:\; \Omega \; \longrightarrow \; \Re \;:\; \omega \; \longmapsto \; \dfrac{1}{n}\sum^{n}_{i=1}\,X_{i}(\omega).
\end{equation*}
For $n, m \in \N$, define $\Snm$ to be the set of all functions from $\{1,2,\ldots,m\} \longrightarrow \{1,2,\ldots,n\}$.
Thus, each
\begin{equation*}
s \; = \; \left(s(1), s(2), \ldots, s(m) \right) \;\in\; \Snm
\end{equation*}
can be regarded as a length-$m$ finite (ordered) sequence of positive integers between $1$ and $n$, inclusive.
Note that $\Snm$ is a finite set with $\vert\;\Snm\;\vert = n^{m}$.
Endow $\Snm$ with the probability space structure induced by the uniform probability function:
\begin{equation*}
P_{\Snm}(s) \;\; := \;\; \dfrac{1}{n^{m}},
\quad\textnormal{for each $s \in \Snm$}.
\end{equation*}
Let $\Omega \times \Snm$ be the product probability space of $\Omega$ and $\Snm$.
Define:
\begin{equation*}
\Xnm
\;:\;\Omega \times \Snm \;\longrightarrow \; \Re
\;:\; (\omega,s) \; \longmapsto \; \dfrac{1}{m}\,\sum^{m}_{j=1}\,X_{s(j)}(\omega).
\end{equation*}
For each $\omega \in \Omega$, define:
\begin{equation*}
\Phi^{(n)}_{m,\omega}
\;:\;
\Snm\;\longrightarrow\; \Re
\;:\;
s
\;\longmapsto\;
\sqrt{m}\left(\,\Xnm(\omega,s) - \overline{X}_{n}(\omega)\,\right)
\end{equation*}
Then,
\begin{equation*}
P\!\left(\;
\Phi^{(n)}_{m,\omega} \overset{d}{\longrightarrow} N(0,\sigma^{2}_{X}),\,\textnormal{as $n,m\rightarrow\infty$}
\;\right)
\;\; = \;\;
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;
\Phi^{(n)}_{m,\omega} \overset{d}{\longrightarrow} N(0,\sigma^{2}_{X}),\,\textnormal{as $n,m\rightarrow\infty$}
\right.
\;\right\}\right)
\;\; = \;\;
1.
\end{equation*}
\end{theorem}

\begin{remark}
\mbox{}\vskip 0.1cm
\noindent
For each fixed $\omega \in \Omega$,
$\left\{\,\Phi^{(n)}_{m,\omega} : \Snm \longrightarrow \Re\,\right\}_{n,m\in\N}$
is a doubly indexed sequence of $\Re$-valued random variables.
Note that their respective domains $\Snm$ are pairwise distinct probability spaces.
The \textbf{Bootstrap Central Limit Theorem for I.I.D. sample mean}
asserts that for almost every $\omega \in \Omega$,
the doubly indexed sequence $\left\{\,\Phi^{(n)}_{m,\omega}\,\right\}$
of $\Re$-valued random variables converges in distribution to
$N(0,\sigma^{2}_{X})$ as $n, m \longrightarrow \infty$.
\end{remark}

\begin{remark}\quad
The following results are well known from classical asymptotic theory:
\vskip 0.1cm
\noindent
By the \textbf{Weak Law of Large Numbers}, $\overline{X}_{n}$ converges in probability to $\mu_{X}$,
as $n \longrightarrow \infty$; in other words,
\begin{equation*}
\lim_{n\rightarrow\infty}\,P\!\left(\;\left\vert\,\overline{X}_{n} - \mu_{X}\,\right\vert \,>\, \varepsilon\;\right)
\;\;=\;\;
\lim_{n\rightarrow\infty}\,\nu\left(\left\{\;\omega\in\Omega\;:\;\left\vert\,\overline{X}_{n}(\omega) - \mu_{X}\,\right\vert \,>\, \varepsilon\;\right\}\right)
\;\;=\;\; 0,
\quad\textnormal{for each $\varepsilon > 0$}.
\end{equation*}
By the \textbf{Strong Law of Large Numbers}, $\overline{X}_{n}$ converges almost surely to $\mu_{X}$,
as $n \longrightarrow \infty$; in other words,
\begin{equation*}
P\!\left(\;\lim_{n\rightarrow\infty}\,\overline{X}_{n} = \mu_{X}\;\right)
\;\;=\;\;
\nu\left(\left\{\;\omega\in\Omega \;\left\vert\; \lim_{n\rightarrow\infty}\,\overline{X}_{n}(\omega) = \mu_{X} \right. \,\right\}\right)
\;\;=\;\; 1.
\end{equation*}
By the \textbf{Central Limit Theorem}, $\sqrt{n}\left(\overline{X}_{n} - \mu_{X}\right)$ converges in distribution to $N(0,\sigma^{2}_{X})$.
\end{remark}

\proof
Let $\mathcal{M}_{1}(\Re,\mathcal{B}(\Re))$ denotes the collection
of probability measures on $(\Re,\mathcal{B}(\Re))$.
Define
\begin{equation*}
\Gamma_{2}
\; := \;
\left\{\;
G \in \mathcal{M}_{1}(\Re,\mathcal{B}(\Re))
\;\;\left\vert\;\;
\int_{\Re}\,x^{2}\,\d G(x) < \infty
\right.
\;\right\}.
\end{equation*}
Define the \textbf{Wasserstein metric} on $\Gamma_{2}$:
\begin{equation*}
d_{2}
\,:\, \Gamma_{2} \times \Gamma_{2} \longrightarrow \Re
\,:\, (G,G^{\prime}) \longmapsto
\inf\left\{\;
\left.
\sqrt{E\!\left[\rho(X,Y)^{2}\right]}
\;\;\right\vert\;\;
(X,Y)\in C(G,G^{\prime})
\;\right\}
\end{equation*}
\mbox{}
\vskip 0.5cm
\noindent
\textbf{Claim 1:}\; $d_{2}$ is indeed a metric on $\Gamma_{2}$.

\mbox{}
\vskip 0.5cm
\noindent
\textbf{Claim 2:}\; For $G, G_{1}, G_{2}, \ldots \in \Gamma_{2}$,
\begin{equation*}
G_{n} \overset{d_{2}}{\longrightarrow} G
\quad\quad\textnormal{if and only if}\quad\quad
G_{n}\longrightarrow G\;\,\textnormal{weakly}
\quad\textnormal{and}\quad
\int_{\Re}\,x^{2}\,\d G_{n}(x) \longrightarrow \int_{\Re}\,x^{2}\,\d G(x)
\end{equation*}

\mbox{}
\vskip 0.5cm
\noindent
\textbf{Claim 3:}\; For $G \in \Gamma_{2}$ and $m \in \N$,
let $G^{(m)}$ be the \textbf{$m$-fold empirical measure} of $G$, i.e.
$G^{(m)}$ is the (empirical) measure of the random variable
\begin{equation*}
S_{m}^{(G)} \;\; := \;\; \dfrac{1}{m^{1/2}} \sum_{i=1}^{m}\left(Z_{i}^{(G)} - \mu_{G}\right),
\end{equation*}
where $\mu_{G} := \int_{\Re}\,x\,\d G(x)$ is the expectation value of the measure $G$,
and $Z^{(G)}_{1},Z^{(G)}_{2},\ldots,Z^{(G)}_{m}$ are independent and
identically distributed random variables with distribution $G$.
Then, for any $G, H \in \Gamma_{2}$, we have
\begin{equation*}
d_{2}\!\left(G^{(m)},H^{(m)}\right)
\;\; \leq \;\;
d_{2}\!\left(\,G,H\,\right)
\end{equation*}

\mbox{}
\vskip 0.5cm
\noindent
\textbf{Claim 4:}\;
\begin{equation*}
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;\;
F_{n}(\omega) \overset{\textnormal{w}}{\longrightarrow} F
\right.
\;\right\}\right)
\;\; = \;\;
1
\end{equation*}
Claim 4 follows from the Glivenko-Cantelli Theorem, which states that:
\begin{equation*}
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;\;
\lim_{n\rightarrow\infty}\,\sup_{t \in \Re}\,\left\vert F_{n}(\omega)(t) - F(t) \right\vert \,=\, 0
\right.
\;\right\}\right)
\;\; = \;\;
1,
\end{equation*}
which implies trivially
\begin{equation*}
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;\;
\lim_{n\rightarrow\infty}F_{n}(\omega)(t) = F(t),
\;\,\textnormal{for each $t\in\Re$}
\right.
\;\right\}\right)
\;\; = \;\;
1,
\end{equation*}
which, in turn, is equivalent to Claim 4.

\mbox{}
\vskip 0.5cm
\noindent
\textbf{Claim 5:}\;
\begin{equation*}
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;\;
\int_{\Re}\,x^{2}\,\d F_{n}(\omega)(x) \longrightarrow \int_{\Re}\,x^{2}\,\d F(x)
\right.
\;\right\}\right)
\;\; = \;\;
1
\end{equation*}
By the Strong Law of Large Numbers, we have
\begin{equation*}
\int_{\Re}\,x^{2}\,\d F_{n}(\omega)(x)
\;\; = \;\; \dfrac{1}{n}\sum_{i=1}^{n}X_{i}(\omega)^{2}
\;\; \overset{\textnormal{a.e.}}{\longrightarrow} \;\; E\!\left[\,X^{2}\,\right]
\;\; = \;\; \int_{\Re}\,x^{2}\,\d F(x)
\end{equation*}

\mbox{}
\vskip 0.6cm
\noindent
\textbf{Claim 6:}\;
\begin{equation*}
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;\;
F_{n}(\omega) \overset{d_{2}}{\longrightarrow} F
\right.
\;\right\}\right)
\;\; = \;\;
\nu\!\left(\left\{\;
\omega \in \Omega
\;\left\vert\;\;
d_{2}\!\left(F_{n}(\omega),F\,\right) \longrightarrow 0
\right.
\;\right\}\right)
\;\; = \;\;
1
\end{equation*}
Immediate by Claims 2, 4, and 5.

\vskip 0.5cm
\noindent
Let $\omega \in \Omega$ be fixed.
\begin{eqnarray*}
d_{2}\!\left(F^{(m)}_{n}(\omega),N(0,\sigma^{2}_{X})\right)
& \leq & d_{2}\!\left(F^{(m)}_{n}(\omega),F^{(m)}\right) \;+\; d_{2}\!\left(F^{(m)},N(0,\sigma^{2}_{X})\right) \\
& \leq & d_{2}\!\left(\,F_{n}(\omega),F\,\right) \;+\; d_{2}\!\left(F^{(m)},N(0,\sigma^{2}_{X})\right) \\
\end{eqnarray*}
Now, $d_{2}\!\left(F^{(m)},N(0,\sigma^{2}_{X}))\right) \longrightarrow 0$ by the classical Central Limit Theorem.

\begin{equation*}
\begin{array}{ccccl}
& d_{2}\!\left(\,F_{n}(\omega),F\,\right) \;+\; d_{2}\!\left(F^{(m)},N(0,\sigma^{2}_{X})\right) &\longrightarrow& 0, & \textnormal{as $n, m \longrightarrow \infty$}
\\ \\
\Longrightarrow& d_{2}\!\left(F^{(m)}_{n}(\omega),N(0,\sigma^{2}_{X})\right) &\longrightarrow& 0, &\textnormal{as $n, m \longrightarrow \infty$}
\\ \\
\Longrightarrow& F^{(m)}_{n}(\omega) & \overset{\textnormal{w}}{\longrightarrow}& N(0,\sigma^{2}_{X}), &\textnormal{as $n, m \longrightarrow \infty$}
\end{array}
\end{equation*}

\qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
Then, for each $\omega \in \Omega$,
$\mathbf{X}_{n}(\omega) := \left(X_{1}(\omega),X_{2}(\omega),\ldots,X_{n}(\omega)\right) \in \Re^{n}$.
Define the \textbf{empirical measure}:
\begin{equation*}
\overline{\delta}_{\mathbf{X}_{n}(\omega)}
\;\;:=\;\;
\dfrac{1}{n}\sum_{i=1}^{n}\,\delta_{X_{i}(\omega)}
\;\;\in\;\;
\mathcal{M}_{1}(\Re,\mathcal{B}(\Re)),
\end{equation*}
where $\delta_{x} \in \mathcal{M}_{1}(\Re,\mathcal{B}(\Re))$ is the Dirac measure concentrated at $x \in \Re$.
Define $\widehat{\theta}_{n} : \Omega \longrightarrow \Re$ by
\begin{equation*}
\widehat{\theta}_{n}(\omega)
\;\;:=\;\;
\theta\!\left(\,\overline{\delta}_{\mathbf{X}_{n}(\omega)}\,\right)
\end{equation*}
Suppose $\widehat{\theta}_{n}$ is a weakly consistent sequence of estimators of $\theta(\nu)$,
i.e. $\widehat{\theta}_{n} \overset{p}{\longrightarrow} \theta(\nu)$;
equivalently, for each $\varepsilon > 0$, we have
\begin{equation*}
\lim_{n\rightarrow\infty}\,
P\!\left(\;
\left\vert\,\widehat{\theta}_{n}(\omega) - \theta(\nu)\,\right\vert > \varepsilon
\;\right)
\;\; = \;\;
\lim_{n\rightarrow\infty}\,
\mu\!\left(\left\{\;
\omega\in\Omega
\;\left\vert\;\;
\left\vert\,\widehat{\theta}_{n}(\omega) - \theta(\nu)\,\right\vert > \varepsilon
\right.
\right\}\right)
\;\; = \;\; 0.
\end{equation*}
\textbf{The Bootstrap is a resampling method that allows one
to estimate asymptotically (as $n \rightarrow \infty$)
the sampling distribution of $\widehat{\theta}_{n}$,
i.e. the probability measure induced on $\Re$ by the $\Re$-valued random variable
$\widehat{\theta}_{n} : \Omega \longrightarrow \Re$.}
\begin{equation*}
F_{n}(t;\mathbf{X}(\omega))
\;\;:=\;\;
\dfrac{1}{n}\sum_{i=1}^{n}\,I_{\{X_{i}(\omega) \leq t\}}
\end{equation*}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
