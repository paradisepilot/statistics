
%%%%%%%%%%

\begin{frame}{\LARGE Overview}

\vskip 0.3cm

\scriptsize
{\large Random forests}
\vskip 0.05cm
\begin{itemize}
\item
	ensemble method based on classification trees
	%\vskip 0.05cm
	%bagging (bootstrap samples), de-correlated trees (random splitting variables)

\vskip 0.25cm
\item
	considered one of the most successful supervised machine learning techniques

\vskip 0.25cm
\item
	introduced by Dr. Leo Breiman (Prof. of Statistics, UCLA \& UC Berkeley) in
	\vskip 0.05cm
	Breiman (2001), Random Forests, \textit{Machine Learning}, 45(1): 5--32

\end{itemize}

\vskip 0.3cm
{\large Classification trees}
\vskip 0.05cm
\begin{itemize}
\item
	The CART algorithm
	\vskip 0.05cm
	Breiman-Friedman-Olshen-Stone (1984),
	\textit{{\color{red}C}lassification {\color{red}a}nd {\color{red}R}egression {\color{red}T}rees},
	Taylor \& Francis

\vskip 0.25cm
\item
	Dr. Charles J. Stone (Prof. of Statistics, UCLA \& UC Berkeley)
	\vskip 0.05cm
	Stone (1977), Consistent Nonparametric Regression, \textit{Annals of Statistics}, 5(4): 595--645
	\vskip 0.02cm
	{\tiny first asymptotic (universal consistency) result for any learning algorithm} %, later on applied to $k$-NN classifiers}

\end{itemize}

\end{frame}
\normalsize

%%%%%%%%%%
