
        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Ken's derivation}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{Probabilistic framework}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\noindent
\textbf{Probability spaces}
\begin{itemize}
\item
    Let
    \,$U \,=\, \{1,2,\ldots,N\}$.\,
    Let
    \,$\mathcal{P}(U) \,=\, \mathcal{P}(\{1,2,\ldots,N\})$\,
    denote the power set (set of all subsets) of \,$U$.\,
    Let
    \,$\left(\,\mathcal{P}(U)\,,\mathcal{P}(\mathcal{P}(U))\,,\overset{{\color{white}-}}{\pi}\,\right)$\,
    be the probability space induced by a sampling design, i.e.,
    a probability mass function defined on
    \,$\mathcal{P}(U)$.\,
\item
    Let \,$\left(\,\Omega,\mathscr{A},\lambda\,\right)$\, be a probability space.
\end{itemize}

\vskip 0.3cm
\noindent
\textbf{Random variables}
\begin{itemize}
\item
    $\Ydot \,=\, (Y_{1},Y_{2},\ldots,Y_{N})^{T} : \Omega \longrightarrow \Re^{N \times 1}$\,
    is an $\Re^{N \times 1}$-valued random variable defined on \,$\Omega$,\,
    i.e., $\mathscr{A}$-measurable map).
\item
    $\Xdotdot\,=\,\left(\,\overset{{\color{white}.}}{X_{ij}}\,\right) : \Omega \longrightarrow \Re^{N \times r}$\,
    is an $\Re^{N \times r}$-valued random variable defined on \,$\Omega$.
\item
    $\Sdot \,=\, (S_{1},S_{2},\ldots,S_{N})^{T} : \mathcal{P}(U) \longrightarrow \{0,1\}^{N \times 1}$\,
    is a $\{0,1\}^{N \times 1}$-valued random variable defined on \,$\mathcal{P}(U)$,\,
    i.e., $\mathcal{P}(\mathcal{P}(U))$-measurable map.
\item
    $\Rdot \,=\, (R_{1},R_{2},\ldots,R_{N})^{T} : {\color{red}\Omega \times \mathcal{P}(U)} \longrightarrow \{0,1\}^{N \times 1}$\,
    is a $\{0,1\}^{N \times 1}$-valued random variable defined on \,$\Omega \times \mathcal{P}(U)$,\,
    i.e., $\left(\mathscr{A} \overset{{\color{white}.}}{\times} \mathcal{P}(\mathcal{P}(U))\right)$-measurable map
    that satisfies:
    \begin{equation*}
    \mu\!\left(\left\{\;
        \omega \in \Omega
        \;\left\vert\;
        S_{k}(s) = 0
        \,\overset{{\color{white}1}}{\Longrightarrow}\,
        R_{k}(\omega,s_{R}) = 0
        \right.
        \;\right\}\right)
    \; = \;
        1\,,
    \quad
    \textnormal{for each \,$k \in U$,\, and each \,$s_{R} \subset s \subset U$}
    \end{equation*}
\end{itemize}

\vskip 0.3cm
\noindent
\textbf{Interpretation}
\begin{itemize}
\item
    $U = \{1,2,\ldots,N\}$
    \,--\,
    index set of the target (finite) population of interest,
    and \,$N$\, is the (finite) size of the target population.
\item
    $\left(\,\Omega,\mathscr{A},\lambda\,\right)$
    \,--\,
    underlying source of stochasticity of the imputation model (or superpopulation model).
\item
    $\Ydot$
    \,--\,
    $\Re$-valued target variable (population characteristic).
\item
    $\Xdotdot$
    \,--\,
    $\Re^{r}$-valued auxiliary variable.
\item
    $\Sdot$
    \,--\,
    $\{0,1\}$-valued binary variable indicating, for each \,$s \subset U$,\,
    whether a population unit \,$k \in U$\, is selected in \,$s$.\,
    More precisely, for each \,$s \subset U$,\, we have:
    \begin{equation*}
    S_{k}(s)
    \;\; = \;\;
        \left\{\begin{array}{cl}
            1\,, & \textnormal{if \,$k \in s$},
            \\
            0\,, & \textnormal{otherwise}.
            \end{array}\right.
    \end{equation*}
\item
    $\Rdot$
    \,--\,
    $\{0,1\}$-valued binary variable indicating, for almost every outcome
    \,$\omega \in \Omega$\,
    and each
    \,$(s,s_{R}) \in \mathcal{P}(U) \times \mathcal{P}(U)$\,
    satisfying
    \,$s_{R} \subset s$,\,
    whether a selected population unit \,$ k \in s$\,
    is a respondent or not.
    % More precisely, for each \,$s \subset U$ and each \,$k \in s$,\, we have:
    % \begin{equation*}
    % R_{k}(\omega,s_{R})
    % \;\; = \;\;
    %     \left\{\begin{array}{cl}
    %         1\,, & \textnormal{if \,$k \in s_{R}$},
    %         \\
    %         0\,, & \textnormal{otherwise}.
    %         \end{array}\right.
    % \end{equation*}
\end{itemize}

\vskip 0.3cm
\noindent
\textbf{Probabilistic assumptions}
\begin{itemize}
\item
    $P(\,S_{i} = 1\,) \,:=\, \pi\!\left(\,\left\{\,\left. s \overset{{\color{white}.}}{\subset} U \,\right\vert\, i \in s\,\right\}\,\right) \,>\, 0$,\,
    for each \,$i \in U$,\,
    and
    \,$w_{i} \,:=\, \dfrac{1}{P(\,S_{i} = 1\,)}$,\, for each \,$i \in U$.
\item
    $\left(\,\Ydot\,,\Xdotdot\,\right)$\, and \,$\Sdot$\, are (unconditionally) independent.
\item
    $\Ydot$\, and \,$(\,\Sdot\,,\Rdot\,)$\, are {\color{red}conditionally independent} given \,$\Xdotdot$.
\end{itemize}

\vskip 0.3cm
\noindent
\textbf{(Non-random) parameters}
\begin{itemize}
\item
    $\dbullet \,=\, (d_{1},d_{2},\ldots,d_{N}) \in \{0,1\}^{N}$\,
    is a (non-random, binary) domain membership indicator.
\end{itemize}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\subsection{Imputation estimator}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Imputation estimator \,$\widetilde{T}_{Y}$]
\mbox{}
\vskip 0.1cm
\noindent
An \textbf{imputation estimator} for the domain total
\;$T_{Y} \,:=\, \underset{i \in U}{\sum}\,d_{i}Y_{i}$\;
is a random variable of the form:
\begin{eqnarray*}
\widetilde{T}_{Y}
\;\; = \;\;
    \widetilde{T}_{Y}(\,
        \Sdot\,,\,
        \Rdot\,,\,
        \Ydot\,,\,
        \Xdotdot\,;\,
        \dbullet
        \,)
& := &
    \underset{i\,\in\,s}{\sum}\;d_{i}\,w_{i}\,\widetilde{Y}_{i}
\;\; = \;\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\left(\,
        R_{i}\,Y_{i}
        \overset{{\color{white}.}}{+}
        (1-R_{i})\,Y^{*}_{i}
        \,\right),
\end{eqnarray*}
where
\;$\widetilde{Y}_{i} \,=\, R_{i}\,Y_{i}+(1-R_{i})\,Y^{*}_{i}$\;
and
\;$Y^{*}_{i} \,=\, Y^{*}_{i}(\,\Sdot,\Rdot,\Ydot,\Xdotdot\,)$.\,
\end{definition}

\begin{lemma}[A formula for \,$\widetilde{T}_{Y}$]
\mbox{}
\vskip 0.1cm
\noindent
\begin{eqnarray*}
\widetilde{T}_{Y}
& = &
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
\end{eqnarray*}
\end{lemma}
\proof
\begin{eqnarray*}
\widetilde{T}_{Y}
& := &
    \underset{i\,\in\,s}{\sum}\;d_{i}\,w_{i}\,\widetilde{Y}_{i}
\;\; = \;\;
    \underset{i\,\in\,s}{\sum}\;d_{i}\,w_{i}\left(\,
        R_{i}\,Y_{i}
        \overset{{\color{white}.}}{+}
        (1-R_{i})\,Y^{*}_{i}
        \,\right)
\\
& = &
    \underset{i\,\in\,s}{\sum}\;d_{i}\,w_{i}\,R_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,s}{\sum}\;d_{i}\,w_{i}\,(1-R_{i})\,Y^{*}_{i}
\;\; = \;\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,R_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,Y^{*}_{i}
\\
& = &
    {\color{red}
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{-}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    }
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,R_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,Y^{*}_{i}
\\
& = &
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{-}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,Y_{i}
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,Y^{*}_{i}
\\
& = &
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    \;\overset{{\color{white}.}}{+}\;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
\end{eqnarray*}
\vskip -0.5cm
\qed

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{lemma}[Overall conditional bias of \,$\widetilde{T}_{Y}$; see Equation (5.2) in \cite{Beaumont2011} and preceding paragraph]
\label{biasTtilde}
\mbox{}
\vskip 0.1cm
\noindent
% Assume that
% \,$Y = (Y_{1},\ldots,Y_{N})$\,
% and
% \,$(I,\rho) = (S_{1},\ldots,S_{N},\rho_{1},\ldots,\rho_{N})$\,
% are conditionally independent given
% \,$\Xobs$.\,
% Then,
\begin{eqnarray*}
\bias\!\left(\;
    \left.
    \widetilde{T}_{Y}
    \;\right\vert\,
    \Xobs
    \,\right)
& := &
    E\!\left[\;
        \left.
        \widetilde{T}_{Y} - T_{Y}
        \,\right\vert\,
        \Xobs
        \,\right]
\;\; = \;\;
    E_{SR}\!\left[\;
        \underset{i\,\in\,{\color{red}s_{M}}}{\sum}\,d_{i}\,w_{i}\,
        E_{Y}\!\left[\;
            \left.
            Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
            \;\right\vert\,
            s,s_{R},\Xobs
            \,\right]
        \,\right]
\end{eqnarray*}
\end{lemma}
%%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
\begin{remark}
\mbox{}
\vskip -0.1cm
\noindent
\begin{itemize}
\item
    In practice, we therefore would like to choose an imputation strategy such that
    \begin{equation*}
    E_{Y}\!\left[\;
        \left.
        Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
        \;\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    \;\; \approx \;\;
        0,
    \quad
    \textnormal{for each \,$i \in s_{M}$}
    \end{equation*}
    See the paragraph after Equation (5.2), p.175, \cite{Beaumont2011}.
\item
    The conditioning on \,$\Xobs$\, is largely suppressed in \cite{Beaumont2011} for notational brevity;
    see the last sentence in the second paragraph of \S4, p.173, \cite{Beaumont2011}.
    We have restored it in our notation in these study notes.
\end{itemize}
\end{remark}
%%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
\proofof Lemma \ref{biasTtilde}
\vskip 0.1cm
\noindent
We prove the Lemma by establishing a series of claims.

\vskip 0.3cm
\noindent
\textbf{Claim 1:}\quad
\begin{eqnarray*}
\bias\!\left(\,
    \left.
    \widetilde{T}_{Y}
    \;\right\vert\,
    \Xobs
    \right)
& = &
    E_{YSR}\!\left[\,
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
        \;\right\vert\,
        \Xobs
        \right]
    \;-\;
        \underset{i\,\in\,U}{\sum}\;
        d_{i}\,E_{Y}\!\left[\,Y_{i}\left\vert\,\Xobs\right.\right]
\\
&&
    +\;
    E_{YSR}\!\left[\,
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
        \;\right\vert\,
        \Xobs
        \right]
\end{eqnarray*}
\vskip 0.1cm
\noindent
Proof of Claim 1:\;\;
\begin{eqnarray*}
\bias\!\left(\;
    \left.
    \widetilde{T}_{Y}
    \;\right\vert\,
    \Xobs
    \,\right)
& := &
    E\!\left[\;
        \left.
        \widetilde{T}_{Y} - T_{Y}
        \,\right\vert\,
        \Xobs
        \,\right]
\;\; = \;\;
    E_{YSR}\!\left[\;
        \left.
        \widetilde{T}_{Y}
        \,\right\vert\,
        \Xobs
        \,\right]
    \,-\,
    E_{YSR}\!\left[\;
        \left.
        T_{Y}
        \,\right\vert\,
        \Xobs
        \,\right]
\\
& = &
    E_{YSR}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
        \;\overset{{\color{white}.}}{+}\;
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
        \,\;\right\vert\,
        \Xobs
        \,\right]
    \,-\,
    {\color{blue}
    E_{YSR}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,Y_{i}
        \;\right\vert\,
        \Xobs
        \,\right]
    }
\\
& = &
    E_{YSR}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
        \;\right\vert\,
        \Xobs
        \,\right]
    \;-\;
    {\color{blue}
    \underset{i\,\in\,U}{\sum}\;
    d_{i}\,E_{Y}\!\left[\,Y_{i}\left\vert\,\Xobs\right.\right]
    }
\\
&&
    +\;
    E_{YSR}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
        \;\right\vert\,
        \Xobs
        \,\right]
\end{eqnarray*}
This completes the proof of Claim 1.

\vskip 0.3cm
\noindent
\textbf{Claim 2:}\quad
$
E_{YSR}\!\left[\;
    \left.
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    \;\right\vert\,
    \Xobs
    \,\right]
\; = \;
    \underset{i\,\in\,U}{\sum}\;d_{i}\,E\!\left[\;Y_{i}\,\left\vert\;\Xobs\right.\,\right]
$
\vskip 0.1cm
\noindent
Proof of Claim 2:\;\;
\begin{eqnarray*}
E_{YSR}\!\left[\;
    \left.
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
    \;\right\vert\,
    \Xobs
    \,\right]
& = &
    E_{YI}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
        \;\right\vert\,
        \Xobs
        \,\right],
    \;\;
    \textnormal{since \,$R_{\bullet}$\, does not appear inside the expectation}
\\
& = &
    E_{S}\!\left[\,E_{Y}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,Y_{i}
        \;\right\vert\,
        s, s_{R}, \Xobs
        \right]\,\right]
\\
& = &
    E_{S}\!\left[\,
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,
        E_{Y}\!\left[\,Y_{i}\,\left\vert\,s,s_{R},\Xobs\right.\right]
        \,\right]
\\
& = &
    E_{S}\!\left[\,
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,
        E_{Y}\!\left[\,Y_{i}\,\left\vert\,\Xobs\right.\right]
        \,\right],
    \;\;
    \textnormal{since \,$Y_{i} \perp (S_{\bullet},R_{\bullet}) \,\vert\, \Xobs$\,}
\\
& = &
    \underset{i\,\in\,U}{\sum}\;\,
    d_{i}\,w_{i}\,
    E_{S}\!\left[\,S_{i}\,\right]\,
    E_{Y}\!\left[\,Y_{i}\,\left\vert\,\Xobs\right.\right]
\\
& = &
    \underset{i\,\in\,U}{\sum}\;\,
    d_{i}\,E_{Y}\!\left[\,Y_{i}\,\left\vert\,\Xobs\right.\,\right],
    \;\;
    \textnormal{since \,$w_{i} \,=\, \dfrac{1}{P(S_{i}=1)} \,=\, \dfrac{1}{E_{S}[\,S_{i}\,]}$}
\end{eqnarray*}
This completes the proof of Claim 2.

\vskip 0.3cm
\noindent
\textbf{Claim 3:}\quad
$
E_{YSR}\!\left[\;
    \left.
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
    \;\right\vert\,
    \Xobs
    \,\right]
\; = \;
    E_{SR}\!\left[\;
        \underset{i\,\in\,{\color{black}s_{M}}}{\sum}\,d_{i}\,w_{i}\,
        E_{Y}\!\left[\;
            \left.
            Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
            \;\right\vert\,
            s,s_{R},\Xobs
            \,\right]
        \,\right]
$
\vskip 0.1cm
\noindent
Proof of Claim 3:\;\;
\begin{eqnarray*}
E_{YSR}\!\left[\;
    \left.
    \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
    \;\right\vert\,
    \Xobs
    \,\right]
& = &
    E_{SR}\!\left[\;
    E_{Y}\!\left[\;
        \left.
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
        \;\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
\\
& = &
    E_{SR}\!\left[\;
        \underset{i\,\in\,U}{\sum}\;d_{i}\,w_{i}\,S_{i}\,(1-R_{i})\,
        E_{Y}\!\left[\;
            \left.
            Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
            \;\right\vert\,
            s,s_{R},\Xobs
            \,\right]
        \,\right]
\\
& = &
    E_{SR}\!\left[\;
        \underset{i\,\in\,{\color{red}s_{M}}}{\sum}\,d_{i}\,w_{i}\,
        E_{Y}\!\left[\;
            \left.
            Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
            \;\right\vert\,
            s,s_{R},\Xobs
            \,\right]
        \,\right]
\end{eqnarray*}
This completes the proof of Claim 3.

\vskip 0.5cm
\noindent
The Lemma now follows immediately from Claim 1, Claim 2, and Claim 3.
\qed

% \begin{eqnarray*}
% \bias\!\left(\;
%     \left.
%     \widetilde{T}_{Y}
%     \;\right\vert\,
%     \Xobs
%     \,\right)
% & := &
%     E\!\left[\;
%         \left.
%         \widetilde{T}_{Y} - T_{Y}
%         \,\right\vert\,
%         \Xobs
%         \,\right]
% % \;\; = \;\;
% %     E_{YSR}\!\left[\;
% %         \left.
% %         \widetilde{T}_{Y} - T_{Y}
% %         \,\right\vert\,
% %         \Xobs
% %         \,\right]
% % \\
% % & = &
% \;\; = \;\;
%     E_{YSR}\!\left[\;
%         \left.
%         \widetilde{T}_{Y}
%         \,\right\vert\,
%         \Xobs
%         \,\right]
%     \,-\,
%     E_{YSR}\!\left[\;
%         \left.
%         T_{Y}
%         \,\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,Y_{i}
%         \;\overset{{\color{white}.}}{+}\;
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \,\;\right\vert\,
%         \Xobs
%         \,\right]
%     \,-\,
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;Y_{i}
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,Y_{i}
%         \;\right\vert\,
%         \Xobs
%         \,\right]
%     \;-\;
%     E_{Y}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;Y_{i}
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% &&
%     +\;
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     E_{SR}\!\left[\,E_{Y}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,Y_{i}
%         \;\right\vert\,
%         s, s_{R}, \Xobs
%         \,\right]\,\right]
%     \;-\;
%     \underset{i\,\in\,U}{\sum}\;E\!\left[\;Y_{i}\,\vert\,\Xobs\,\right]
% \\
% &&
%     +\;
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     E_{SR}\!\left[\,
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,
%         E_{Y}\!\left[\,\left.Y_{i}\,\right\vert s,s_{R},\Xobs\,\right]
%         \,\right]
%     \;-\;
%     \underset{i\,\in\,U}{\sum}\;E\!\left[\;Y_{i}\,\vert\,\Xobs\,\right]
%     % \,+\,
%     % E_{YSR}\!\left[\;
%     %     \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%     %     \,\right]
% \\
% &&
%     +\;
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     \underset{i\,\in\,U}{\sum}\;E_{Y}\!\left[\,\left.Y_{i}\,\right\vert s,s_{R},\Xobs\,\right]
%     \;-\;
%     \underset{i\,\in\,U}{\sum}\;E\!\left[\;Y_{i}\,\vert\,\Xobs\,\right]
%     \;+\;
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     \underset{i\,\in\,U}{\sum}\;E_{Y}\!\left[\,\left.Y_{i}\,\right\vert\Xobs\,\right]
%     \;-\;
%     \underset{i\,\in\,U}{\sum}\;E\!\left[\;Y_{i}\,\vert\,\Xobs\,\right]
%     \;+\;
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     E_{YSR}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         \Xobs
%         \,\right]
% \\
% & = &
%     E_{SR}\!\left[\;
%     E_{Y}\!\left[\;
%         \left.
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,(Y^{*}_{i}-Y_{i})
%         \;\right\vert\,
%         s,s_{R},\Xobs
%         \,\right]
%         \,\right]
% \\
% & = &
%     E_{SR}\!\left[\;
%         \underset{i\,\in\,U}{\sum}\;w_{i}\,S_{i}\,(1-R_{i})\,
%         E_{Y}\!\left[\;
%             \left.
%             Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
%             \;\right\vert\,
%             s,s_{R},\Xobs
%             \,\right]
%         \,\right]
% \\
% & = &
%     E_{SR}\!\left[\;
%         \underset{i\,\in\,{\color{red}s_{M}}}{\sum}\,w_{i}\,
%         E_{Y}\!\left[\;
%             \left.
%             Y^{*}_{i} \overset{{\color{white}.}}{-} Y_{i}
%             \;\right\vert\,
%             s,s_{R},\Xobs
%             \,\right]
%         \,\right]
% \end{eqnarray*}
% \vskip -0.8cm
% \qed

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{lemma}[Decomposition of \,$\MSE(\,\widetilde{T}_{Y}\,\vert\,\Xobs\,)$; see Equation (5.3) in \cite{Beaumont2011}]
\label{DecompositionMSETtilde}
\mbox{}
\vskip 0.1cm
\noindent
\begin{eqnarray*}
\MSE\!\left(\;
    \left.
    \widetilde{T}_{Y}
    \;\right\vert\,
    \Xobs
    \,\right)
& := &
    E\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
\\
& = &
    E_{Y}\!\left[\,
    \Var_{SR}\!\left(\,
        \left.
        \widehat{T}_{Y}
        \,\right\vert\,
        \Ydot,
        \Xobs
        \,\right)
        \,\right]
    \;+\;
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
\\
&&
    +\;2\,
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} \,-\, \widehat{T}_{Y}\,)(\,\widehat{T}_{Y} \,-\, T_{Y}\,)
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
\end{eqnarray*}
\end{lemma}
\proof
\begin{eqnarray*}
&&
\MSE\!\left(\;
    \left.
    \widetilde{T}_{Y}
    \;\right\vert\,
    \Xobs
    \,\right)
% \\
% & := &
\;\; := \;\;
    E\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
\;\; = \;\;
    E_{YSR}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
\\
& = &
    E_{YSR}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} \;{\color{red}-\; \widehat{T}_{Y} \;+\; \widehat{T}_{Y}} \,-\, T_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
\\
& = &
    E_{YSR}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,+\,
        2\,(\,\widetilde{T}_{Y} \,-\, \widehat{T}_{Y}\,)(\,\widehat{T}_{Y} \,-\, T_{Y}\,)
        \,+\,
        (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
\\
& = &
    {\color{blue}
    E_{YSR}\!\left[\,
        \left.
        (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
    }
    \;+\;
    {\color{blue}
    E_{YSR}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        \Xobs
        \,\right]
    }
% \\
% &&
    \;+\;
    2\,E_{YSR}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} \,-\, \widehat{T}_{Y}\,)(\,\widehat{T}_{Y} \,-\, T_{Y}\,)
        \,\right\vert\,
        \Xobs
        \,\right]
\\
& = &
    {\color{blue}
    E_{Y}\!\left[\,
    E_{SR}\!\left[\,
        \left.
        (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        \Ydot,
        \Xobs
        \,\right]
        \,\right]
    }
    \;+\;
    {\color{blue}
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
    }
\\
&&
    +\;2\,
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} \,-\, \widehat{T}_{Y}\,)(\,\widehat{T}_{Y} \,-\, T_{Y}\,)
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
\\
& = &
    E_{Y}\!\left[\,
    {\color{blue}
    \Var_{SR}\!\left(\,
        \left.
        \widehat{T}_{Y}
        \,\right\vert\,
        \Ydot,
        \Xobs
        \,\right)
        }
        \,\right]
    \;+\;
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
\\
&&
    +\;2\,
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} \,-\, \widehat{T}_{Y}\,)(\,\widehat{T}_{Y} \,-\, T_{Y}\,)
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
\end{eqnarray*}
\vskip -0.5cm
\qed

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\noindent
The first term above
\,--\,
i.e.,
$
E_{Y}\!\left[\,
E_{SR}\!\left[\,
    \left.
    (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
    \,\right\vert\,
    \Ydot,
    \Xobs
    \,\right]
    \,\right]
$
\,--\,
is called the \textit{sampling variance} in \cite{Beaumont2011}.
First, note that
\begin{eqnarray*}
E_{Y}\!\left[\,
E_{SR}\!\left[\,
    \left.
    (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
    \,\right\vert\,
    \Ydot,
    \Xobs
    \,\right]
    \,\right]
& = &
    E_{Y}\!\left[\,
    \Var_{SR}\!\left(\,
        \left.
        \widehat{T}_{Y}
        \,\right\vert\,
        \Ydot,
        \Xobs
        \,\right)
        \,\right]
\end{eqnarray*}
It is asserted that
\begin{eqnarray*}
E_{Y}\!\left[\,
E_{SR}\!\left[\,
    \left.
    (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
    \,\right\vert\,
    \Ydot,
    \Xobs
    \,\right]
    \,\right]
& \approx &
    E_{Y}\!\left[\,
    E_{SR}\!\left[\,
        \left.
        (\,\widehat{T}_{Y} - T_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\YsubR,\Xobs
        \,\right]
        \,\right]
\end{eqnarray*}
See \cite{Beaumont2009} as well as Equation (5.5), p.175, \cite{Beaumont2011}.

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{lemma}[Unbiased estimator for sampling variance; Equation (5.5) in \cite{Beaumont2011}]
\label{SamplingVariance}
\mbox{}
\vskip 0.1cm
\noindent
The following statements are true:
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\mbox{}\;\;\,(\theenumi)$\quad$}
\begin{enumerate}
\item
    If
    \;$v(\,s\,;y_{\bullet}\,)$\,
    is an (arbitrary) design-unbiased estimator for the sampling variance, i.e.,
    \begin{equation*}
    E_{S}\!\left[\;
        v(\,s\,;\Ydot\,)
        \,\left\vert\;
        \Ydot,\Xobs
        \right.
        \,\right]
    \;\; = \;\;
        \Var_{SR}\!\left(\,
            \left.
            \widehat{T}_{Y}
            \,\right\vert\,
            \Ydot,
            \Xobs
            \,\right),
    \end{equation*}
    then
    \,$
    E_{Y_{M}}\!\left[\,
        v(\,s\,;\Ydot\,)
        \left\vert\,
        s,s_{R},\YsubR,\Xobs
        \right.
        \right]
    $\,
    is an $(\Sdot,\Rdot,\YsubR)$-unbiased estimator for
    \,$
    E_{Y}\!\left[\,
        \Var_{SR}\!\left(\,
            \left.
            \widehat{T}_{Y}
            \,\right\vert\,
            \Ydot,
            \Xobs
            \,\right)
    \,\right]
    $,\,
    i.e.,
    \begin{equation*}
    E_{SR}\,
    E_{Y_{R}}\,
    E_{Y_{M}}\!\left[\,
        v(\,s\,;\Ydot\,)
        \,\left\vert\;
        s,s_{R},\YsubR,\Xobs
        \right.
        \,\right]
    \;\; = \;\;
        E_{Y}\!\left[\,
            \Var_{SR}\!\left(\,
                \left.
                \widehat{T}_{Y}
                \,\right\vert\,
                \Ydot,
                \Xobs
                \,\right)
            \,\right]
    \end{equation*}
\item
    If
    \,$v(\,s\,;y_{\bullet}\,)$\,
    is taken to be the (design-unbiased) Horvitz-Thompson estimator for the sampling variance, i.e.,
    \begin{equation*}
    v(\,s\,;y_{\bullet}\,)
    \;\; := \;\;
        \underset{k \in s}{\sum}\,
        \underset{l \in s}{\sum}\,
        \dfrac{\pi_{kl} - \pi_{k}\pi_{l}}{\pi_{kl}}\,
        (d_{k}w_{k}y_{k})\,(d_{l}w_{l}y_{l})
    \end{equation*}
    then
    \,$
    E_{Y_{M}}\!\left[\,
        v(\,s\,;\Ydot\,)
        \left\vert\,
        s,s_{R},\YsubR = y_{R},\Xobs
        \right.
        \right]
    $\,
    can be expressed as follows:
    \begin{equation*}
    E_{Y_{M}}\!\left[\,
        v(\,s\,;\Ydot\,)
        \left\vert\,
        s,s_{R},\YsubR=y_{R},\Xobs
        \right.
        \right]
    \;\; = \;\;
        v(\,s\,;\,\widetilde{y}_{\bullet}\,)
        \;+\;
        \underset{k \in s_{M}}{\sum}\;
        d_{k}\left(1-\dfrac{1}{w_{k}}\right)w_{k}^{2}\,\sigma_{k}^{2}\,,
    \end{equation*}
    where
    \,$\widetilde{y}_{\bullet}$\,
    is given by:
    \begin{equation*}
    \widetilde{y}_{k}
    \;\; := \;\;
        \left\{\begin{array}{cl}
            y_{k}, & \textnormal{for \,$k \,\in\, s_{R}$}
            \\
            % \overset{{\color{white}1}}{E_{Y}\!\left[\;Y_{k}\,\left\vert\;\Xobs\right.\right]}, & \textnormal{for \,$k \,\in\, s_{M} \,:=\, s \,\backslash\, s_{R}$}
            \overset{{\color{white}\textnormal{\large1}}}{\mu_{k}}, & \textnormal{for \,$k \,\in\, s_{M} \,:=\, s \,\backslash\, s_{R}$}
            \end{array}\right.,
    \end{equation*}
    and, for \,$k \in s_{M}$,\, the parameters
    \,$\mu_{k}$\, and \,$\sigma_{k}^{2}$\,
    are given by:
    \begin{eqnarray*}
    \mu_{k}
    & := &
        {\color{white}...}E_{Y}\!\left[\;Y_{k}\,\left\vert\;\Xobs\right.\right]
    \\
    \sigma_{k}^{2}
    & := &
        \Var_{Y}\!\left[\;
            Y_{k}
            \,\left\vert\;
            \Xobs
            \right.
            \right]
    \end{eqnarray*}
\end{enumerate}
% We have:
% \begin{eqnarray*}
% E_{Y}\!\left[\,
%     \Var_{SR}\!\left(\,
%         \left.
%         \widehat{T}_{Y}
%         \,\right\vert\,
%         \Ydot,
%         \Xobs
%         \,\right)
%     \,\right]
% & \approx &
%     E_{Y_{M}}\!\left[\,
%         v(\,s\,;y_{\bullet}\,)
%         \left\vert\,
%         s,s_{R},\YsubR,\Xobs
%         \right.
%         \right]
% \;\; = \;\;
%     v(\,y^{\mu}_{\bullet}\,)
%     \;+\;
%     \underset{k \in s_{M}}{\sum}
%     \left(1-\dfrac{1}{w_{k}}\right)
%     w_{k}^{2}\,\sigma_{k}^{2}\,,
% \end{eqnarray*}
% where
\end{lemma}
\proof
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\mbox{}\;\;\,(\theenumi)$\quad$}
\begin{enumerate}
\item
    We simply compute:
    \begin{eqnarray*}
    E_{SR}\,
    E_{Y_{R}}\,
    E_{Y_{M}}\!\left[\,
        v(\,s\,;\Ydot\,)
        \,\left\vert\;
        s,s_{R},\YsubR,\Xobs
        \right.
        \,\right]
    & = &
        E_{SR}\,
        E_{Y}\!\left[\,
            v(\,s\,;\Ydot\,)
            \,\left\vert\;
            s,s_{R},\Xobs
            \right.
            \,\right]
    \\
    & = &
        E_{Y}\,
        E_{SR}\!\left[\,
            v(\,s\,;\Ydot\,)
            \,\left\vert\;
            \Ydot,\Xobs
            \right.
            \,\right]
    \\
    & = &
        E_{Y}\,
        E_{S}\!\left[\,
            v(\,s\,;\Ydot\,)
            \,\left\vert\;
            \Ydot,\Xobs
            \right.
            \,\right]
    \\
    & = &
        E_{Y}\!\left[\,
            \Var_{SR}\!\left(\,
                \left.
                \widehat{T}_{Y}
                \,\right\vert\,
                \Ydot,
                \Xobs
                \,\right)
            \,\right],
    \;\;
    \textnormal{by hypothesis on \,$v(\,s\,;y_{\bullet}\,)$}
    \end{eqnarray*}
\item
    We compute:
    \begin{eqnarray*}
    E_{Y_{M}}\!\left[\,
        v(\,s\,;\Ydot\,)
        \left\vert\,
        s,s_{R},\YsubR=y_{R},\Xobs
        \right.
        \right]
    & = &
        E_{Y_{M}}\!\left[\,
            \left.
            \underset{k \in s}{\sum}\,
            \underset{l \in s}{\sum}\,
            \dfrac{\pi_{kl} - \pi_{k}\pi_{l}}{\pi_{kl}}\,
            (d_{k}w_{k}Y_{k})\,(d_{l}w_{l}Y_{l})
            \;\right\vert\,
            s,s_{R},\YsubR=y_{R},\Xobs
            \right]
    \\
    & = &
        \underset{k \in s}{\sum}\,
        \underset{l \in s}{\sum}\;
        \dfrac{\pi_{kl} - \pi_{k}\pi_{l}}{\pi_{kl}}\;
        d_{k}w_{k}\,d_{l}w_{l}\;
        E_{Y_{M}}\!\left[\;
            Y_{k}Y_{l}
            \,\left\vert\,
            s,s_{R},\YsubR=y_{R},\Xobs
            \right.
            \right]
    \\
    & = &
        \underset{k \in s}{\sum}\,
        \underset{l \in s}{\sum}\;
        \dfrac{\pi_{kl} - \pi_{k}\pi_{l}}{\pi_{kl}}\;
        d_{k}w_{k}\,d_{l}w_{l}\;
        E_{Y_{M}}\!\left[\;
            Y_{k}Y_{l}
            \,\left\vert\,
            \YsubR=y_{R},\Xobs
            \right.
            \right]
    \\
    & = &
        \cdots
        \;\; = \;\;
        v(\,s\,;\widetilde{y}_{\bullet}\,)
        \;+\;
        \underset{k \in s_{M}}{\sum}\;
        d_{k}
        \left(1-\dfrac{1}{w_{k}}\right)
        w_{k}^{2}\,\sigma_{k}^{2}\,,
    \end{eqnarray*}
    where the last equality follows from the observation:
    \begin{eqnarray*}
    E_{Y_{M}}\!\left[\;
        Y_{k}Y_{l}
        \,\left\vert\,
        \YsubR=y_{R},\Xobs
        \right.
        \right]
    & = &
        \left\{\begin{array}{cl}
            y_{k} \cdot y_{l}\,, & \underset{{\color{white}.}}{\textnormal{for \,$k,l \in s_{R}$}}
            \\
            y_{k} \cdot E[\,Y_{l}\,\vert\,\Xobs\,]\,, & \underset{{\color{white}.}}{\textnormal{for \,$k \in s_{R}$,\, $l \in s_{M}$}}
            \\
            E[\,Y_{k}\vert\,\Xobs\,] \cdot y_{l}\,, & \underset{{\color{white}.}}{\textnormal{for \,$k \in s_{M}$,\, $l \in s_{R}$}}
            \\
            E[\,Y_{k}\vert\,\Xobs\,] \cdot E[\,Y_{l}\vert\,\Xobs\,]\,, & \underset{{\color{white}.}}{\textnormal{for \,$k,l \in s_{M}$,\, $k \neq l$}}
            \\
            \Var(\,Y_{k}\,\vert\,\Xobs) + E[\,Y_{k}\,\vert\,\Xobs\,]^{2}\,, & \textnormal{for \,$k = l \in s_{M}$}
            \end{array}\right.
    \end{eqnarray*}
    which is equivalent to the following more compact expression:
    \begin{eqnarray*}
    E_{Y_{M}}\!\left[\;
        Y_{k}Y_{l}
        \,\left\vert\,
        \YsubR=y_{R},\Xobs
        \right.
        \right]
    & = &
        \left\{\begin{array}{cl}
            \sigma_{k}^{2} \,+\, \widetilde{y}_{k}^{2}\,, & \textnormal{for \,$k = l \in s_{M}$}
            \\
            \widetilde{y}_{k} \cdot \widetilde{y}_{l}\,, & \overset{{\color{white}.}}{\textnormal{otherwise}}
            \end{array}\right.
    \end{eqnarray*}
    \vskip -0.7cm
    \qed
\end{enumerate}
% \vskip 1.0cm
% \noindent
% The design-unbiasedness of
% \,$v(\,s\,;y_{\bullet}\,)$\,
% means
% \begin{equation*}
% E_{S}\!\left[\;\overset{{\color{white}.}}{v(\,s\,;\Ydot\,)}\,\right]
% \;\; = \;\;
%     E_{S}\!\left[\,
%         \left.
%         \overset{{\color{white}.}}{v(\,s\,;\Ydot\,)}
%         \,\right\vert\,
%         \Ydot,
%         \Xobs
%         \,\right]
% \;\; = \;\;
%     \Var_{SR}\!\left(\,
%         \left.
%         \widehat{T}_{Y}
%         \,\right\vert\,
%         \Ydot,
%         \Xobs
%         \,\right)
% \end{equation*}
% At this point, we use the estimator:
% \begin{equation*}
% \Var_{SR}\!\left(\,
%     \left.
%     \widehat{T}_{Y}
%     \,\right\vert\,
%     \Ydot,
%     \Xobs
%     \,\right)
% \;\; \approx \;\;
%     \overset{{\color{white}.}}{v(\,s\,;\Ydot\,)}
% \end{equation*}
% \begin{eqnarray*}
% E_{Y}\!\left[\,
%     \Var_{SR}\!\left(\,
%         \left.
%         \widehat{T}_{Y}
%         \,\right\vert\,
%         \Ydot,
%         \Xobs
%         \,\right)
%     \,\right]
% & \approx &
%     E_{Y}\!\left[\,
%         v(\,s\,;\Ydot\,)
%         \,\left\vert\;
%         s,\Xobs
%         \right.
%         \,\right]
% \;\; = \;\;
%     E_{Y}\!\left[\,
%         v(\,s\,;\Ydot\,)
%         \,\left\vert\;
%         s,{\color{blue}s_{R}},\Xobs
%         \right.
%         \,\right]
% \\
% & = &
%     E_{Y_{R}}\,E_{Y_{M}}\!\left[\,
%         v(\,s\,;\Ydot\,)
%         \,\left\vert\;
%         s,s_{R},\Xobs
%         \right.
%         \,\right]
% \\
% & \approx &
%     E_{Y_{M}}\!\left[\,
%         v(\,s\,;\Ydot\,)
%         \,\left\vert\;
%         s,s_{R},\YsubR,\Xobs
%         \right.
%         \,\right]
% \end{eqnarray*}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\clearpage
\begin{definition}[Composite linear imputation estimator;\, see \S3 in \cite{Beaumont2011}]
\label{LinearImputationEstimator}
\mbox{}
\vskip 0.1cm
\noindent
An imputation estimator
\begin{equation*}
\widetilde{T}_{Y}(\,s,s_{R},y_{\bullet}\,;\,X_{\bullet\bullet},\dbullet\,)
\;\; = \;\;
    \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
    \;+\,
    \underset{k \in s_{m}}{\sum}\;d_{k}\,w_{k}\,y_{k}^{*}
\end{equation*}
is called a \textbf{composite linear imputation estimator}
if there exists \,$J \in \N$\, such that
\begin{center}
\begin{minipage}{6.2in}
for each \,$s \subset U$,\, each \,$s_{R} \subset s$,\, each \,$k \in s_{M} := s \,\backslash\, s_{R}$,\,
there exist
\begin{equation*}
\varphi^{(j)}_{0k} = \varphi^{(j)}_{0k}(s,s_{R},X_{\bullet\bullet})
\quad\textnormal{and}\quad
\varphi^{(j)}_{lk} = \varphi^{(j)}_{lk}(s,s_{R},X_{\bullet\bullet}),
\;\;
\textnormal{for \,$j \in \{1,2,\ldots,J\}$,\, $l \in s_{R}$}
\end{equation*}
such that
\begin{equation*}
y_{k}^{*}
\;\; = \;\;
    \varphi^{(j)}_{0k}
    \,+\,
    \underset{l \in s_{r}}{\sum}\,\varphi^{(j)}_{lk}\,y_{l}\,,
\quad
\textnormal{for some \,$j \in \{1,2,\ldots,J\}$\, depending on \,$s,\,s_{R},\,\Xobs$}
\end{equation*}
\end{minipage}
\end{center}

\end{definition}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{lemma}[$\widetilde{T}_{Y} - \widehat{T}_{Y}$ as sum of \,$\Xobs$-conditionally indepdent summands;\, see Equation (5.8) in \cite{Beaumont2011}]
\label{EqnFivePtEight}
\mbox{}
\vskip 0.1cm
\noindent
Recall:
\begin{itemize}
\item
    $
    \widetilde{T}_{Y}(\,s,s_{R}\,;\,y_{\bullet}\,)
    \;\; = \;\;
        \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
        \;+\,
        \underset{k \in s_{m}}{\sum}\;d_{k}\,w_{k}\,y_{k}^{*}
    $
\item
    $
    \widehat{T}_{Y}(\,s\,;\,y_{\bullet}\,)
    \;\; = \;\;
        \underset{k\,\in\,s}{\sum}\;d_{k}\,w_{k}\,y_{k}
    $
\end{itemize}
\vskip 0.2cm
\noindent
Let \,$s \subset U = \{1,2,\ldots,N\}$\, and \,$s_{R} \subset s$\, be given and fixed.
\vskip 0.1cm
\noindent
Suppose that, for each $k \in s_{M} := s \,\backslash\, s_{R}$, there exist
\,$\varphi^{(j)}_{0k} = \varphi^{(j)}_{0k}(s,s_{R})$\,
and
\,$\varphi^{(j)}_{lk} = \varphi^{(j)}_{lk}(s,s_{R})$\,
such that
\begin{eqnarray*}
y_{k}^{*}
& = &
    \varphi^{(j)}_{0k}
    \,+\,
    \underset{l \in s_{r}}{\sum}\,\varphi^{(j)}_{lk}\,y_{l}
% \\
% \widetilde{T}_{Y}(\,s,s_{R}\,;\,y_{\bullet}\,)
% & = &
%     \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
%     \,+\,
%     \underset{k \in s_{m}}{\sum}\;d_{k}\,w_{k}\,y_{k}^{*}
% \\
% \widehat{T}_{Y}(\,s\,;\,y_{\bullet}\,){\color{white}...}
% & = &
%     \underset{k\,\in\,s}{\sum}\;d_{k}\,w_{k}\,y_{k}
\end{eqnarray*}
where \,$j \in \{\,1,2,\ldots,J\,\}$.\,
\vskip 0.2cm
\noindent
Define:
\begin{equation*}
W^{(+)}_{{\color{red}0}}
\;\; := \;\;
    \left(\;
        \overset{J}{\underset{j=1}{\sum}}\;
        \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{{\color{red}0}l}
        \,\right),
\quad\quad
W^{(+)}_{{\color{red}k}}
\;\; := \;\;
    \left(\;
        \overset{J}{\underset{j=1}{\sum}}\;
        \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{{\color{red}k}l}
        \,\right)
\end{equation*}
Then, the following equalities hold:
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\mbox{}\;\;\,(\theenumi)$\quad$}
\begin{enumerate}
\item
    % $
    % \widetilde{T}_{Y}(\,s,s_{R}\,;\,y_{\bullet}\,)
    % \;\; = \;\;
    %     \left(\;
    %         \overset{J}{\underset{j=1}{\sum}}\;
    %         \underset{k \in s_{m}^{(j)}}{\sum}\;
    %         d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
    %         \,\right)
    %     \;+\;
    %     \underset{k \in s_{r}}{\sum}
    %     \left(\;
    %         d_{k}\,w_{k}
    %         \,+\,
    %         \left(\;
    %             \overset{J}{\underset{j=1}{\sum}}\;
    %             \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
    %             \right)
    %         \right)
    %     y_{k}
    % $
    $
    \widetilde{T}_{Y}(\,s,s_{R},y_{\bullet}\,;X_{\bullet\bullet},\dbullet\,)
    \;\; = \;\;
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\!\left(\,d_{k}\,w_{k} \,+\, W^{(+)}_{k}\,\right)y_{k}
    $
\item
    % $
    % \widetilde{T}_{Y}(\,s,s_{R}\,;\,y_{\bullet}\,)
    % \;-\;
    % \widehat{T}_{Y}(\,s\,;\,y_{\bullet}\,)
    % \;\; = \;\;
    %     \left(\;
    %         \overset{J}{\underset{j=1}{\sum}}\;
    %         \underset{k \in s_{m}^{(j)}}{\sum}\,d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
    %         \,\right)
    %     \;+\;
    %     \underset{k \in s_{r}}{\sum}\!
    %     \left(\;
    %         \overset{J}{\underset{j=1}{\sum}}\;
    %         \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
    %         \right)
    %     y_{k}
    %     \;-\;
    %     \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,y_{k}
    % $
    $
    \widetilde{T}_{Y}(\,s,s_{R},y_{\bullet}\,;X_{\bullet\bullet}\,\dbullet\,)
    \;-\;
    \widehat{T}_{Y}(\,s,y_{\bullet}\,;\,\dbullet\,)
    \;\; = \;\;
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}W^{(+)}_{k}\,y_{k}
        \;-\;
        \underset{k \in s_{M}}{\sum}d_{k}\,w_{k}\,y_{k}
    $
\end{enumerate}
% \begin{eqnarray*}
% \widetilde{T}_{Y}{\color{white}....}
% & = &
%     \left(\;
%         \overset{J}{\underset{j=1}{\sum}}\;
%         \underset{k \in s_{m}^{(j)}}{\sum}\;
%         d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
%         \,\right)
%     \;+\;
%     \underset{k \in s_{r}}{\sum}
%     \left(\;
%         d_{k}\,w_{k}
%         \,+\,
%         \left(\;
%             \overset{J}{\underset{j=1}{\sum}}\;
%             \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
%             \right)
%         \right)
%     y_{k}
% \\
% \widetilde{T}_{Y}
% \,-\,
% \widehat{T}_{Y}
% & = &
%     \left(\;
%         \overset{J}{\underset{j=1}{\sum}}\;
%         \underset{k \in s_{m}^{(j)}}{\sum}\,d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
%         \,\right)
%     \;+\;
%     \underset{k \in s_{r}}{\sum}\!
%     \left(\;
%         \overset{J}{\underset{j=1}{\sum}}\;
%         \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
%         \right)
%     y_{k}
%     \;-\;
%     \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,y_{k}
% \end{eqnarray*}
\end{lemma}
\proof
\begin{enumerate}
\item
    Straightforward algebra:
    \begin{eqnarray*}
    \widetilde{T}_{Y}
    \;\; = \;\;
    \widetilde{T}_{Y}\!\left(\,s,s_{R}\,;\,y\,\right)
    & = &
        \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
        \,+\,
        \underset{k \in s_{m}}{\sum}\;d_{k}\,w_{k}\,y_{k}^{*}
    \;\; = \;\;
        \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
        \,+\,
        \overset{J}{\underset{j=1}{\sum}}\;
        \underset{k \in s_{m}^{(j)}}{\sum}\,
        d_{k}\,w_{k}\,y_{k}^{*}
    \\
    & = &
        \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
        \,+\,
        \overset{J}{\underset{j=1}{\sum}}\;
        \underset{k \in s_{m}^{(j)}}{\sum}\,
        d_{k}\,w_{k}\left(\,
            \varphi^{(j)}_{0k}
            \,+\,
            \underset{l \in s_{r}}{\sum}\,\varphi^{(j)}_{lk}\,y_{l}
            \right)
    \\
    & = &
        \left(\;
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{k \in s_{m}^{(j)}}{\sum}\;
            d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
            \,\right)
        \,+\,
        \left(\;
            \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
            \,+\,
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{k \in s_{m}^{(j)}}{\sum}\,
            d_{k}\,w_{k}\left(\,
                \underset{l \in s_{r}}{\sum}\,\varphi^{(j)}_{lk}\,y_{l}
                \right)
            \,\right)
    \\
    & = &
        \left(\;
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{k \in s_{m}^{(j)}}{\sum}\;
            d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
            \,\right)
        \,+\,
        \left(\;
            \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
            \,+\,
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{{\color{red}l}\,\in\,s_{m}^{(j)}}{\sum}\,
            w_{{\color{red}l}}\,d_{{\color{red}l}}\left(\,
                \underset{{\color{red}k}\,\in\,s_{r}}{\sum}\,\varphi^{(j)}_{{\color{red}kl}}\,y_{{\color{red}k}}
                \right)
            \,\right)
    \\
    & = &
        \left(\;
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{k \in s_{m}^{(j)}}{\sum}\;
            d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
            \,\right)
        \,+\,
        \left(\;
            \underset{k \in s_{r}}{\sum}\;d_{k}\,w_{k}\,y_{k}
            \,+\,
            \underset{k \in s_{r}}{\sum}\!\left(\;
                \overset{J}{\underset{j=1}{\sum}}\;
                \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
                \right)
            y_{k}
            \right)
    \\
    & = &
        \left(\;
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{k \in s_{m}^{(j)}}{\sum}\;
            d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
            \,\right)
        \,+\,
        \underset{k \in s_{r}}{\sum}
        \left(\;
            d_{k}\,w_{k}
            \,+\,
            \left(\;
                \overset{J}{\underset{j=1}{\sum}}\;
                \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
                \right)
            \right)
        y_{k}
    \\
    & \overset{{\color{white}\textnormal{\LARGE$1$}}}{=} &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\left(\,w_{k}d_{k} \,+\, W^{(+)}_{k}\,\right)y_{k}
    \end{eqnarray*}
\item
    We again compute:
    \begin{eqnarray*}
    \widetilde{T}_{Y}
    \,-\,
    \widehat{T}_{Y}
    % & = &
    %     \left(\;
    %         \overset{J}{\underset{j=1}{\sum}}\;
    %         \underset{k \in s_{m}^{(j)}}{\sum}\;
    %         d_{k}\,w_{k}\,\varphi^{(j)}_{0k}
    %         \,\right)
    %     \;+\;
    %     \underset{k \in s_{r}}{\sum}
    %     \left(\;
    %         d_{k}\,w_{k}
    %         \,+\,
    %         \left(\;
    %             \overset{J}{\underset{j=1}{\sum}}\;
    %             \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{kl}
    %             \right)
    %         \right)
    %     y_{k}
    %     \;-\;
    %     \underset{k\,\in\,s}{\sum}\;d_{k}\,w_{k}\,y_{k}
    & = &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\!
        \left(\;
            d_{k}\,w_{k}
            \,+\,
            W^{(+)}_{k}
            \right)
        y_{k}
        \;-\;
        \underset{k\,\in\,s}{\sum}\;d_{k}\,w_{k}\,y_{k}
    \\
    & = &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\,d_{k}\,w_{k}\,y_{k}
        \;+\;
        \underset{k \in s_{r}}{\sum}\,W^{(+)}_{k}\,y_{k}
        \;-\;
        \underset{k\,\in\,s}{\sum}\;d_{k}\,w_{k}\,y_{k}
    \\
    & = &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum} W^{(+)}_{k}\,y_{k}
        \;-\;
        \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,y_{k}
    \end{eqnarray*}
    \vskip -0.6cm
    \qed
\end{enumerate}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{lemma}[Unbiased estimator for non-response variance; Equations (5.9) \& (5.10) in \cite{Beaumont2011}]
\label{NonresponseVariance}
\mbox{}
\vskip 0.1cm
\noindent
The following statements are true:
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\mbox{}\;\;\,(\theenumi)$\quad$}
\begin{enumerate}
\item
    $
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    $\,
    is an $(\Sdot,\Rdot)$-unbiased estimator for the non-response variance\\
    \,$
    E_{SR}\!\left[\,
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
        \,\right]
    $.
    % i.e.,
    % \begin{equation*}
    % E_{SR}\!\left[\,
    % E_{Y}\!\left[\,
    %     \left.
    %     (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
    %     \,\right\vert\,
    %     s,s_{R},\Xobs
    %     \,\right]
    %     \,\right]
    % \end{equation*}
\item
    $
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    $\,
    can be expressed in explicit computable form as follows:
    \begin{eqnarray*}
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    & = &
        \left(\,
            \underset{k \in s_{r}}{\sum}\,
                \left(W^{(+)}_{k}\right)^{2}\sigma_{k}^{2}
            \,+\,
            \underset{k \in s_{M}}{\sum}\,
                d_{k}\,w_{k}^{2}\,\sigma_{k}^{2}
            \,\right)
    \\
    &&
        +\;
        \left(\,
            W^{(+)}_{0}
            \,+\,
            \underset{k \in s_{r}}{\sum}\;W^{(+)}_{k}\,\mu_{k}
            \,-\,
            \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,\mu_{k}
            \,\right)^{2},
    \end{eqnarray*}
    where
    \,$
    \mu_{k} \;:=\, E_{Y}\!\left[\;Y_{k}
        \,\left\vert\,
        \Xobs
        \right.
        \,\right]
    $,\,
    \,$
    \sigma_{k}^{2} \;:=\, \Var_{Y}\!\left[\;Y_{k}
        \,\left\vert\,
        \Xobs
        \right.
        \,\right]
    $,\,
    and
    \begin{equation*}
    W^{(+)}_{{\color{red}0}}
    \;\; := \;\;
        \left(\;
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{{\color{red}0}l}
            \,\right),
    \quad\quad
    W^{(+)}_{{\color{red}k}}
    \;\; := \;\;
        \left(\;
            \overset{J}{\underset{j=1}{\sum}}\;
            \underset{l \in s_{m}^{(j)}}{\sum}\,d_{l}\,w_{l}\,\varphi^{(j)}_{{\color{red}k}l}
            \,\right)
    \end{equation*}
\end{enumerate}
\end{lemma}
\proof
\begin{enumerate}
\item
    Nothing to prove.
\item
    \textbf{Claim 1:}\quad
    \begin{equation*}
    E_{Y}\!\left[\,
        \left.
        (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    \;\; = \;\;
        \Var_{Y}\!\left[\,
            \left.
            \widetilde{T}_{Y} - \widehat{T}_{Y}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right]
        \,+\,
        E_{Y}\!\left[\,
            \left.
            \widetilde{T}_{Y} - \widehat{T}_{Y}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right]^{2}
    \end{equation*}
    \vskip 0.1cm
    \noindent
    Proof of Claim 1:\;\;
    Claim 1 follows immediately from the following well-known equality:
    \begin{equation*}
    \Var[\,X\,]
    \; := \;
        E\!\left[\,\overset{{\color{white}.}}{(X - \mu_{X})^{2}}\,\right]
    \; = \;
        E\!\left[\,\overset{{\color{white}.}}{X^{2} - 2\,\mu_{X}\,X + \mu_{X}^{2}}\,\right]
    \; = \;
        \cdots
    \; = \;
        E\!\left[\,X^{2}\,\right] \,-\, \mu_{X}^{2}\,,
    \end{equation*}
    This completes the proof of Claim 1.

    \vskip 0.5cm
    \textbf{Claim 2:}\quad
    \begin{equation*}
    \Var_{Y}\!\left[\,
        \left.
        \widetilde{T}_{Y} - \widehat{T}_{Y}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    \;\; = \;\;
        \underset{k \in s_{r}}{\sum}
            \left(W^{(+)}_{k}\right)^{2}\sigma_{k}^{2}
        \,\;+\;
        \underset{k \in s_{M}}{\sum}\,
            d_{k}\,w_{k}^{2}\,\sigma_{k}^{2}
    \end{equation*}
    \vskip 0.1cm
    \noindent
    Proof of Claim 2:\;\;
    % which implies
    % \,$E\!\left[\,X^{2}\,\right] \,=\, \Var[\,X\,] \,+\,\mu_{X}^{2}$.\,
    % Hence,
    % \begin{eqnarray*}
    % E_{Y}\!\left[\,
    %     \left.
    %     (\,\widetilde{T}_{Y} - \widehat{T}_{Y}\,)^{2}
    %     \,\right\vert\,
    %     s,s_{R},\Xobs
    %     \,\right]
    % & = &
    %     \Var_{Y}\!\left[\,
    %         \left.
    %         \widetilde{T}_{Y} - \widehat{T}_{Y}
    %         \,\right\vert\,
    %         s,s_{R},\Xobs
    %         \,\right]
    %     \,+\,
    %     E_{Y}\!\left[\,
    %         \left.
    %         \widetilde{T}_{Y} - \widehat{T}_{Y}
    %         \,\right\vert\,
    %         s,s_{R},\Xobs
    %         \,\right]^{2}
    % \end{eqnarray*}
    \begin{eqnarray*}
    &&
        \Var_{Y}\!\left[\,
            \left.
            \widetilde{T}_{Y} - \widehat{T}_{Y}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right]
    \\
    & = &
        \Var_{Y}\!\left[\,
            \left.
            W^{(+)}_{0}
            \,+\,
            \underset{k \in s_{r}}{\sum}\,W^{(+)}_{k}\,Y_{k}
            \,-\,
            \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,Y_{k}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right],
        \;\;
        \textnormal{by Lemma \ref{EqnFivePtEight}(ii)}
    \\
    & = &
        \Var_{Y}\!\left[\;
            \left.
            \underset{k \in s_{r}}{\sum}\,W^{(+)}_{k}\,Y_{k}
            \,-\,
            \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,Y_{k}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right],
        \;\;
        \textnormal{since $W^{(+)}_{0}$ is constant upon conditioning on $s, s_{R}$}
    \\
    & = &
        \underset{k \in s_{r}}{\sum}
        \!\left(W^{(+)}_{k}\right)^{2}
        \cdot
        \Var_{Y}\!\left[\;
            Y_{k}
            \,\left\vert\,
            s,s_{R},\Xobs
            \right.
            \right]
        \,\;+\;
        \underset{k \in s_{M}}{\sum}
        \!\left(d_{k}\,w_{k}\right)^{2}
        \cdot
        \Var_{Y}\!\left[\;Y_{k}
            \,\left\vert\,
            s,s_{R},\Xobs
            \right.
            \,\right],
        \;\;
        \textnormal{since \,$Y_{k} \perp Y_{l} \;\;\vert\;\, \Xobs$}
    \\
    & = &
        \underset{k \in s_{r}}{\sum}
        \!\left(W^{(+)}_{k}\right)^{2}
        \cdot
        \Var_{Y}\!\left[\;
            Y_{k}
            \,\left\vert\,
            \Xobs
            \right.
            \right]
        \,\;+\;
        \underset{k \in s_{M}}{\sum}\,
        w_{k}^{2}\,d_{k}
        \cdot
        \Var_{Y}\!\left[\;Y_{k}
            \,\left\vert\,
            \Xobs
            \right.
            \,\right],
        \;\;
        \textnormal{since \,$\Ydot \perp (\Sdot,\Rdot) \;\;\vert\;\, \Xobs$}
    \\
    & = &
        \underset{k \in s_{r}}{\sum}
            \left(W^{(+)}_{k}\right)^{2}\sigma_{k}^{2}
        \,\;+\;
        \underset{k \in s_{M}}{\sum}\,
            d_{k}\,w_{k}^{2}\,\sigma_{k}^{2}\,,
        \;\;
        \textnormal{by definition of
            \,$\sigma_{k}^{2} \;:=\, \Var_{Y}\!\left[\;Y_{k}
                \,\left\vert\,
                \Xobs
                \right.
                \,\right]$}
    \end{eqnarray*}
    This completes the proof of Claim 2.

    \vskip 0.5cm
    \textbf{Claim 3:}\quad
    \begin{equation*}
    E_{Y}\!\left[\,
        \left.
        \widetilde{T}_{Y} - \widehat{T}_{Y}
        \,\right\vert\,
        s,s_{R},\Xobs
        \,\right]
    \;\; = \;\;
        W^{(+)}_{0}
        \,\;+\;
        \underset{k \in s_{r}}{\sum}\;W^{(+)}_{k}\,\mu_{k}
        \,\;-\;
        \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,\mu_{k}
    \end{equation*}
    \vskip 0.1cm
    \noindent
    Proof of Claim 3:\;\;
    \begin{eqnarray*}
    &&
        E_{Y}\!\left[\,
            \left.
            \widetilde{T}_{Y} - \widehat{T}_{Y}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right]
    \\
    & = &
        E_{Y}\!\left[\,
            \left.
            W^{(+)}_{0}
            \,+\,
            \underset{k \in s_{r}}{\sum}\,W^{(+)}_{k}\,Y_{k}
            \,-\,
            \underset{k \in s_{M}}{\sum}\;d_{k}\,w_{k}\,Y_{k}
            \,\right\vert\,
            s,s_{R},\Xobs
            \,\right],
        \quad
        \textnormal{by Lemma \ref{EqnFivePtEight}(ii)}
    \\
    & = &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\;
        W^{(+)}_{k}
        \cdot
        E_{Y}\!\left[\;
            Y_{k}
            \,\left\vert\,
            s,s_{R},\Xobs
            \right.
            \right]
        \,\;-\;
        \underset{k \in s_{M}}{\sum}\;
        d_{k}\,w_{k}
        \cdot
        E_{Y}\!\left[\;Y_{k}
            \,\left\vert\,
            s,s_{R},\Xobs
            \right.
            \,\right]
    \\
    & = &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\;
        W^{(+)}_{k}
        \cdot
        E_{Y}\!\left[\;
            Y_{k}
            \,\left\vert\,
            \Xobs
            \right.
            \right]
        \,\;-\;
        \underset{k \in s_{M}}{\sum}\;
        d_{k}\,w_{k}
        \cdot
        E_{Y}\!\left[\;Y_{k}
            \,\left\vert\,
            \Xobs
            \right.
            \,\right],
        \;\;
        \textnormal{since \,$\Ydot \perp (\Sdot,\Rdot) \;\;\vert\;\, \Xobs$}
    \\
    & = &
        W^{(+)}_{0}
        \;+\;
        \underset{k \in s_{r}}{\sum}\;W^{(+)}_{k}\,\mu_{k}
        \,\;-\;
        \underset{k \in s_{M}}{\sum}\,
            d_{k}\,w_{k}\,\mu_{k}\,,
        \;\;
        \textnormal{by definition of
            \,$\mu_{k} \;:=\, E_{Y}\!\left[\;Y_{k}
                \,\left\vert\,
                \Xobs
                \right.
                \,\right]$}
    \end{eqnarray*}
    This completes the proof of Claim 3.

    \vskip 0.3cm
    \noindent
    The required result now follows immediately from Claim 1, Claim 2, and Claim 3.
    \qed
\end{enumerate}

        %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
