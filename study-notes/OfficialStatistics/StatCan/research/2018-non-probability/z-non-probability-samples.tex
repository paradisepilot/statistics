
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Setting}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}


          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{proposition}
\mbox{}
\vskip 0.1cm
\noindent
Setting:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$\, is a probability space.
\item
	$N \in \N$.
\item
	$R : \Omega \longrightarrow \{0,1\}^{N}$\; and
	$I : \Omega \longrightarrow \{0,1\}^{N}$
	are random variables.
\item
	$Y : \Omega \longrightarrow \Re$,\;
	$X : \Omega \longrightarrow \Re^{p}$,\; and\;
	$Z : \Omega \longrightarrow \Re^{q}$
	are random variables.
\end{itemize}
Suppose:
\begin{itemize}
\item
	$E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,R_{k}=0,R,Y,X,Z\;\right] \, > \, 0$,\;
	for each \,$k = 1, 2, \ldots, N$, and for each $\omega \in \Omega$.
\end{itemize}
\vskip 0.3cm
\noindent
Define:
\begin{itemize}
\item
	$J_{k} \; := \; R_{k} \, + \, (1-R_{k}) \cdot I_{k}$
\end{itemize}
\vskip 0.3cm
\noindent
Then, the following statements hold:
\begin{enumerate}
\item
	$\rho_{k} \; := \; E\!\left[\;\left.\overset{{\color{white}.}}{J}_{k}\;\right\vert\,R,Y,X,Z\;\right] \, > \, 0$,\,
	for each \,$k = 1, 2, \ldots, N$, and for each $\omega \in \Omega$.
	\vskip 0.05cm
	\noindent
	Hence, we may define the random variable
	\,$\widehat{\theta} : \Omega \longrightarrow \Re$\,
	by
	\begin{equation*}
	\widehat{\theta}
	\,\;\; := \;\;
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
		\dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
	\end{equation*}
	And, when conditioned on given values of \,$(R,I,Y,X,Z)$,\,
	the random variable \,$\widehat{\theta}$\, simplifies to:
	\begin{equation*}
	\widehat{\theta}
	\;\; = \;\;
		\underset{k\vert R_{k}=1}{\sum}\; Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0,I_{k}=1}{\sum}\;\, \dfrac{1}{\rho_{k}}\cdot Y_{k}
	\end{equation*}
\item
	\begin{equation*}
	E\!\left[\;\,\left.\widehat{\theta}\;\;\right\vert\;R,Y,X,Z\;\right]
	\;\; = \;\;
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\, Y_{k}	
	\end{equation*}
\item
	\begin{equation*}
	E\!\left[\;\,\left.\widehat{\theta}\;\;\right\vert\;Y,X,Z\;\right]
	\;\; = \;\;
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\, Y_{k}	
	\end{equation*}
\item
	Under the following two additional assumptions:
	\begin{itemize}
	\item
		$I$ and $(Y,X)$ are independent given $Z$, i.e.
		\begin{equation*}
		P\!\left(\,Y,X,I\,\vert\,Z\,\right)
		\;\; = \;\;
			P\!\left(\,Y,X\,\vert\,Z\,\right)
			\cdot
			P\!\left(\,I\,\vert\,Z\,\right)\,,
			\quad
			\textit{and}
		\end{equation*}
	\item
		$R$ and $I$ are independent given $Y,X,Z$, i.e.
		\begin{equation*}
		P\!\left(\,R,I\,\vert\,Y,X,Z\,\right)
		\;\; = \;\;
			P\!\left(\,R\,\vert\,Y,X,Z\,\right)
			\cdot
			P\!\left(\,I\,\vert\,Y,X,Z\,\right)\,,
		\end{equation*}	
	\end{itemize}
	when conditioned on given values of \,$(R,I,Y,X,Z)$,\, the random variable \,$\widehat{\theta}$\, simplifies to:
	\begin{equation*}
	\widehat{\theta}
	\;\; = \;\;
		\underset{k\vert R_{k}=1}{\sum}\; Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0,I_{k}=1}{\sum}\;\, \dfrac{1}{\pi_{k}}\cdot Y_{k}\,,
	\end{equation*}
	where \,$\pi_{k} \; := \; P(\,I_{k}=1\;\vert\;Z\,)$.
\item
	Under the following alternative additional assumptions instead:
	\begin{itemize}
	\item
		$I$ and $(Y,X)$ are independent given $Z$, i.e.
		\begin{equation*}
		P\!\left(\,Y,X,I\,\vert\,Z\,\right)
		\;\; = \;\;
			P\!\left(\,Y,X\,\vert\,Z\,\right)
			\cdot
			P\!\left(\,I\,\vert\,Z\,\right)\,,
			\quad
			\textit{and}
		\end{equation*}
	\item
		\begin{equation*}
		P\!\left(\,\left.\overset{{\color{white}.}}{I}_{k} = 1\;\right\vert\,R_{k}=0,{\color{red}R},Y,X,Z\,\right)
		\;\; = \;\;
			P\!\left(\,\left.\overset{{\color{white}.}}{I}_{k} = 1\;\right\vert\,R_{k}=0,Y,X,Z\,\right),
		\end{equation*}
	\end{itemize}
	when conditioned on given values of \,$(R,I,Y,X,Z)$,\, the random variable \,$\widehat{\theta}$\, simplifies to:
	\begin{equation*}
	\widehat{\theta}
	\;\; = \;\;
		\underset{k\vert R_{k}=1}{\sum}\; Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0,I_{k}=1}{\sum}\;\, \dfrac{1}{\rho_{k}}\cdot Y_{k}\,,
	\end{equation*}
	where
	\begin{equation*}
	\rho_{k}
	\;\; = \;\;
		\dfrac{
			1 - P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=1\;\right\vert\,I_{k}=1,Y,X,Z\,\right)
			}{
			1 - P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=1\;\right\vert\,Y,X,Z\,\right)
			}
		\cdot
		\pi_{k}\,,
	\end{equation*}
	and \,$\pi_{k} \; := \; P(\,I_{k}=1\;\vert\;Z\,)$.
\end{enumerate}
\end{proposition}
\proof
\begin{enumerate}
\item
	Note:
	\begin{eqnarray*}
	\rho_{k}
	& \underset{{\color{white}\textnormal{\Huge 1}}}{:=} &
		E\!\left[\,\left.\overset{{\color{white}.}}{J}_{k}\;\right\vert\,R,Y,X,Z\;\right]
	\;\; = \;\;
		R_{k} \,+\, (1-R_{k}) \cdot E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,R,Y,X,Z\,\right]
	\\
	& = &
		\left\{\begin{array}{cl}
		1\,, & \textnormal{if \,$R_{k} = 1$}
		\\
		\overset{{\color{white}1}}{E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,R_{k}=0,R,Y,X,Z\;\right]}\,,
		& \textnormal{if \,$R_{k} = 0$}
		\end{array}\right.
	\\
	& > &
		\overset{{\color{white}\huge 1}}{0}
	\end{eqnarray*}
	The positivity of each \,$\rho_{k}$\, implies that \,$\widehat{\theta}$\, is indeed well-defined.
	Next, observe that
	\begin{equation*}
	J_{k}
	\;\;\ = \;\;
		\left\{\begin{array}{cl}
		1\,, & \textnormal{if \,$R_{k} = 1$}
		\\
		\overset{{\color{white}1}}{I}_{k}\,, & \textnormal{if \,$R_{k} = 0$}
		\end{array}\right.
	\end{equation*}
	Hence,
	\begin{eqnarray*}
	\widehat{\theta}
	& := &
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
		\dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
	\;\; = \;\;
		\underset{k\vert R_{k}=1}{\sum}\;\; \dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0}{\sum}\;\; \dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
	\\
	& = &
		\underset{k\vert R_{k}=1}{\sum}\; \dfrac{1}{1} \cdot 1 \cdot Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0}{\sum}\;\, \dfrac{1}{\rho_{k}} \cdot I_{k} \cdot Y_{k}
	\\
	& = &
		\underset{k\vert R_{k}=1}{\sum}\; Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0,I_{k}=1}{\sum}\;\, \dfrac{1}{\rho_{k}} \cdot Y_{k}\,,
	\end{eqnarray*}
	as required.
\item
	\begin{eqnarray*}
	E\!\left[\;\,\left.\widehat{\theta}\;\;\right\vert\;R,Y,X,Z\;\right]
	& = &
		E\!\left[\;\,
			\left.
			\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
			\dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
			\,\;\right\vert\;
			R,Y,X,Z
			\;\right]
	\;\; = \;\;
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
		\dfrac{1}{\rho_{k}}
		\cdot
		E\!\left[\;
			\left.
			\overset{{\color{white}.}}{J}_{k} 
			\,\;\right\vert\;
			R,Y,X,Z
			\;\right]
		\cdot Y_{k}
	\\
	& = &
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
		\dfrac{1}{\rho_{k}} \cdot \rho_{k} \cdot Y_{k}
	\;\; = \;\;
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\, Y_{k}
	\end{eqnarray*}
\item
	\begin{equation*}
	E\!\left[\;\,\left.\widehat{\theta}\;\;\right\vert\;Y,X,Z\;\right]
	\;\; = \;\;
		E\!\left[\;\;
			\left.
			E\!\left(\;\left.\widehat{\theta}\;\;\right\vert\,R,Y,X,Z\;\right)
			\;\;\right\vert\;\;
			Y,X,Z
			\;\;\right]
	\;\; = \;\;
		E\!\left[\;\;
			\left.
			\overset{N}{\underset{k\,=\,1}{\sum}}\;\, Y_{k}
			\;\;\right\vert\;\;
			Y,X,Z
			\;\;\right]
	\;\; = \;\;
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\, Y_{k}
	\end{equation*}
\item
	We already know:
	\begin{eqnarray*}
	\widehat{\theta}
	& := &
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
		\dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
	\;\; = \;\; \cdots \;\; = \;\;
		\underset{k\vert R_{k}=1}{\sum}\; Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0,I_{k}=1}{\sum}\;\, \dfrac{1}{\rho_{k}} \cdot Y_{k}
	\end{eqnarray*}
	On the other hand, under the additional assumptions, for each \,$k$\, with \,$R_{k} = 0$,\, we have
	\begin{eqnarray*}
	\rho_{k}
	& = &
		E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,R_{k}=0,R,Y,X,Z\;\right]
	\;\; = \;\;
		E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,Y,X,Z\;\right]
	\;\; = \;\;
		E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,Z\;\right]
	\;\; = \;\;
		P\!\left(\,\left.\overset{{\color{white}.}}{I}_{k}=1\;\right\vert\,Z\,\right)
	\;\; =: \;\;
		\pi_{k}
	\end{eqnarray*}
	The required result follows immediately from the above two observations.
\item
	Again, we already know:
	\begin{eqnarray*}
	\widehat{\theta}
	& := &
		\overset{N}{\underset{k\,=\,1}{\sum}}\;\;
		\dfrac{1}{\rho_{k}} \cdot J_{k} \cdot Y_{k}
	\;\; = \;\; \cdots \;\; = \;\;
		\underset{k\vert R_{k}=1}{\sum}\; Y_{k}
		\; + \;
		\underset{k\vert R_{k}=0,I_{k}=1}{\sum}\;\, \dfrac{1}{\rho_{k}} \cdot Y_{k}
	\end{eqnarray*}
	On the other hand, under the additional assumptions, for each \,$k$\, with \,$R_{k} = 0$,\, we have
	\begin{eqnarray*}
	\rho_{k}
	& = &
		E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,R_{k}=0,R,Y,X,Z\;\right]
	\;\; = \;\;
		E\!\left[\;\left.\overset{{\color{white}.}}{I}_{k}\;\right\vert\,R_{k}=0,Y,X,Z\;\right]
	\;\; = \;\;
		P\!\left(\,\left.\overset{{\color{white}.}}{I}_{k}=1\;\right\vert\,R_{k}=0,Y,X,Z\,\right)
	\\
	& = &
		\dfrac{
			P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=0\;\right\vert\,I_{k}=1,Y,X,Z\,\right)
			\cdot
			P\!\left(\,\left.\overset{{\color{white}.}}{I}_{k}=1\;\right\vert\,Y,X,Z\,\right)
			}{
			P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=0\;\right\vert\,Y,X,Z\,\right)
			}\,,
		\quad
		\textnormal{by Bayes' Theorem}
	\\
	& = &
		\dfrac{
			1 - P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=1\;\right\vert\,I_{k}=1,Y,X,Z\,\right)
			}{
			1 - P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=1\;\right\vert\,Y,X,Z\,\right)
			}
		\cdot
		P\!\left(\,\left.\overset{{\color{white}.}}{I}_{k}=1\;\right\vert\,Z\,\right)
	\\
	& = &
		\dfrac{
			1 - P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=1\;\right\vert\,I_{k}=1,Y,X,Z\,\right)
			}{
			1 - P\!\left(\,\left.\overset{{\color{white}.}}{R}_{k}=1\;\right\vert\,Y,X,Z\,\right)
			}
		\cdot
		\pi_{k}
	\end{eqnarray*}
	%where the fourth equality follows by
	%Bayes' Theorem\footnote{Bayes' Theorem:\; $P(A \vert B) = \dfrac{P(B \vert A)\cdot P(A)}{P(B)}$}.
	The required result follows immediately from the above two observations.
\end{enumerate}
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
