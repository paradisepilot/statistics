
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Backpropagation -- a special case of reverse-mode algorithmic differentiation}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

Backpropagation is a gradient computation technique commonly used
for training neural networks, e.g. multi-layer perceptrons.
Backpropagation is a special case of reverse-mode algorithmic differentiation.

\vskip 0.3cm
\noindent
For simplicity of notation, we assume we are considering a multi-layer perceptron
with three hidden layers.
Then, its loss function during the training of a multi-layer perceptron has the form:
\begin{equation}\label{objectiveFunctionMLP}
f(\,\theta_{1},\theta_{2},\theta_{3}\,)
\;\; = \;\;
	L\!\left(\,
		F_{3}\!\left(\,
			\overset{{\color{white}1}}{F}_{2}\!\left(
				F_{1}(\,\overset{{\color{white}.}}{\theta_{1}}\,;\,\mathbf{x}\,)\,,\,
				\theta_{2}\right),\,
			\theta_{3}\,\right)
		\,;\,\mathbf{y}\,\right)
\end{equation}
where
\begin{itemize}
\item
	$\theta_{1}$, $\theta_{2}$, $\theta_{3}$ represent the ``weights''
	of the first, second and third hidden layers, respectively,
\item
	$\mathbf{x}$ and $\mathbf{y}$ represent the predictor variables and
	response variable(s) in the training data, and
\item
	the maps
	\begin{equation*}
	F_{1} : \Re^{p_{1}} \times \Re^{q} \longrightarrow \Re^{n_{1}}\,,
	\quad\;\;
	F_{2} : \Re^{n_{1}} \times \Re^{p_{2}} \longrightarrow \Re^{n_{2}}\,,
	\quad\;\;
	F_{3} : \Re^{n_{2}} \times \Re^{p_{3}} \longrightarrow \Re^{n_{3}}\,,
	\end{equation*}
	correspond to the first, second, and third hidden layers, respectively,
	and
	\begin{equation*}
	L : \Re^{n_{3}} \times \Re^{d} \longrightarrow \Re
	\end{equation*}
	corresponds to the loss function. 
\end{itemize}

\vskip 0.5cm
\begin{remark}{\color{white}.}
\vskip 0.1cm
\noindent
We remark here that, for regression problems,
\,$L\!\left(\,F_{3}(\;\cdots\;)\,,\,\overset{{\color{white}-}}{\mathbf{y}}\,\right)$\,
is often the sum of squares of errors
(between $\mathbf{y}$ and $F_{3}(\;\cdots\;)$).
The gradient \,$\nabla_{\theta_{3}} L$\, with respective to
the weights \,$\theta_{3}$ of the third (in this example, the last) hidden layer
will involve the difference \;$\mathbf{y} - F_{3}(\;\cdots\;)$.\;
This is why it is common to say that the reverse pass of the backpropagation algorithm
starts by ``{\color{red}feeding} the prediction errors into the output layer.'' 
\end{remark}

\vskip 0.4cm
\noindent
Before we continue, note that the optimization problem that arises from the training
of a multi-layer perceptron is performed over
\,$\theta = (\theta_{1},\theta_{2},\theta_{3})$\,
and this optimization problem treats $\mathbf{x}$ and $\mathbf{y}$ as constants.
Thus, we simplify our notation by suppressing $\mathbf{x}$ and $\mathbf{y}$,
as well as subsuming $L$ into $F_{3}$ in \eqref{objectiveFunctionMLP}.
Under this notational simplification, \eqref{objectiveFunctionMLP} becomes:
\begin{equation*}
f(\,\theta_{1},\theta_{2},\theta_{3}\,)
\;\; = \;\;
	F_{3}\!\left(\,
		\overset{{\color{white}1}}{F}_{2}\!\left(
			F_{1}(\,\overset{{\color{white}.}}{\theta_{1}})\,,\,
			\theta_{2}\,\right),\,
		\theta_{3}\,\right),
\end{equation*}
with
\begin{equation*}
F_{1} : \Re^{p_{1}} \longrightarrow \Re^{n_{1}}\,,
\quad\quad
F_{2} : \Re^{n_{1}} \times \Re^{p_{2}} \longrightarrow \Re^{n_{2}}\,,
\quad\quad
F_{3} : \Re^{n_{2}} \times \Re^{p_{3}} \longrightarrow \Re
\end{equation*}
At first glance, such an \,$f(\,\cdots\,)$\, does not immediately appear to be
a composite function, thus, it does not appear to be of the form
to which reverse-mode algorithmic differentiation is applicable. 
We next show that reverse-mode algorithmic differentiation in fact indeed
is applicable to such an \,$f(\,\cdots\,)$, i.e. we will find suitable maps
\,$G_{1}, G_{2}, G_{3}$\, such that
\begin{equation*}
f(\,\theta_{1},\theta_{2},\theta_{3}\,)
\;\; = \;\;
	G(\,\theta\,)
\;\; = \;\;
	G_{3}\!\left(\,
		\overset{{\color{white}1}}{G}_{2}\!\left(
			G_{1}(\,\overset{{\color{white}.}}{\theta}\,)\right)
		\,\right)
\end{equation*}
To this end, define
\begin{equation*}
G_{1} : \Re^{p_{1}}\times\Re^{p_{2}}\times\Re^{p_{3}}
	\longrightarrow
	\Re^{n_{1}}\times\Re^{p_{2}}\times\Re^{p_{3}}\,,
\quad\;\;
G_{2} : \Re^{n_{1}}\times\Re^{p_{2}}\times\Re^{p_{3}}
	\longrightarrow
	\Re^{n_{2}}\times\Re^{p_{3}}\,,
\quad\;\;
G_{3} : \Re^{n_{2}}\times\Re^{p_{3}} \longrightarrow \Re
\end{equation*}
by:
\begin{eqnarray*}
G_{1}(\,\theta_{1},\theta_{2}\,\theta_{3}\,) & = & \left(\,F_{1}(\,\theta_{1})\,,\,\theta_{2}\,,\,\theta_{3}\,\right)
\\
G_{2}(\,g_{1},\theta_{2},\theta_{3}\,) & = & \left(\,F_{2}(\,g_{1}\,,\,\theta_{2}\,)\,,\,\theta_{3}\,\right)
\\
G_{3}(\,g_{2},\theta_{3}\,) & = & F_{3}\left(\,g_{2}\,,\,\theta_{3}\,\right)
\end{eqnarray*}
Then, writing \,$\theta = (\,\theta_{1},\theta_{2},\theta_{3}\,)$\,, we have:
\begin{eqnarray*}
	G_{3}\!\left(\,
		\overset{{\color{white}1}}{G}_{2}\!\left(
			G_{1}(\,\overset{{\color{white}.}}{\theta}\,)
			\right)
		\,\right)
& = &
	G_{3}\!\left(\,
		\overset{{\color{white}1}}{G}_{2}\!\left(\,
			F_{1}(\,\overset{{\color{white}.}}{\theta}\,)\,,\,\theta_{2}\,,\,\theta_{3}
			\,\right)
		\right)
\;\; = \;\;
	G_{3}\!\left(\,
		\overset{{\color{white}1}}{F}_{2}\!\left(\,
			F_{1}(\,\overset{{\color{white}.}}{\theta}\,)\,,\,\theta_{2}\,\right)
			,\,
			\theta_{3}
		\,\right)
\\
& = &
	F_{3}\!\left(\,
		\overset{{\color{white}1}}{F}_{2}\!\left(\,
			F_{1}(\,\overset{{\color{white}.}}{\theta}\,)\,,\,\theta_{2}\,\right)
			,\,
			\theta_{3}
		\,\right)
\;\; = \;\;
	f(\,\theta_{1},\theta_{2},\theta_{3}\,)\,,
\end{eqnarray*}
as required.

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
