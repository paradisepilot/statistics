
\documentclass{article}

\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
%\usepackage{doublespace}

\usepackage{KenChuArticleStyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{page}{1}

\pagestyle{fancy}

%\input{../CourseSemesterUnique}

%\rhead[\CourseSemesterUnique]{Kenneth Chu (300517641)}
%\lhead[Kenneth Chu (300517641)]{\CourseSemesterUnique}
\rhead[Study Notes]{Kenneth Chu (300517641)}
\lhead[Kenneth Chu (300517641)]{Study Notes}
\chead[]{{\Large\bf  The Central Limit Theorem} \\
\vskip 0.1cm \normalsize \today}
\lfoot[]{}
\cfoot[]{}
\rfoot[]{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Central Limit Theorem}
\setcounter{theorem}{0}

\begin{theorem}[The Central Limit Theorem]\label{CentralLimitTheorem} \mbox{} \vskip 0.1cm \noindent
Let $\left\{\,X_{n}:(\Omega,\mathcal{A},\mathfrak{m})\longrightarrow\Re\,\right\}_{n\in\N}$ be a sequence of independent identically distributed (\emph{iid}) $\Re$-valued random variables whose common variance $\sigma^{2}$ is positive, i.e. $\sigma^{2} \in (0,\infty)$.  Let $\mu\in\Re$ be the common mean of the $X_{n}$'s.  For each $n \in \N$, define
\begin{equation*}
Y_{n} \; := \; \dfrac{X_{1}+\cdots+X_{n}-n\mu}{\sigma\sqrt{n}} \; : \; \left(\Omega,\mathcal{A},\mathfrak{m}\right) \; \longrightarrow \; \Re.
\end{equation*}
Then, the sequence $\left\{\,P_{Y_{n}}\,\right\}_{n\in\N}$ of distribution measures of the random variables $Y_{n}$ converges weakly to the standard normal disctribution measure $P_{N(0,1)}$ on $\Re$.  In other words, the sequence of probability density functions (defined on $\Re=\codomain(X_{n})$) induced by $Y_{n}$ converges weakly to the probability density function (also defined on $\Re$) of the standard normal distribution.
\end{theorem}

\begin{remark}\mbox{} \vskip 0cm
\begin{itemize}
\item  \vskip -0.2cm
          \textbf{{\large Note that the Central Limit Theorem (Theorem \ref{CentralLimitTheorem}) does NOT say that the sequence
          $\left\{\,Y_{n}\,\right\}_{n\in\N}$ of random variables converges to anything at all.}}

          Instead, it is a statement about the (weak) convergence of the sequence $\left\{\,P_{Y_{n}}\,\right\}_{n\in\N}$
          of distribution measures of $\left\{\,Y_{n}\,\right\}_{n\in\N}$.  Recall that each distribution measure $P_{Y_{n}}$ is a probability
          measure defined on the Lebesgue measure space $\left(\,\Re\,,\,\mu_{\textnormal{Lebesgue}}\,\right)$.
\item  Recall that a \underline{\emph{random variable}} is simply a measurable function
          $X : (\Omega,\mathcal{A},\mathfrak{m}) \longrightarrow (E,\mathcal{B})$, whose domain
          $(\Omega,\mathcal{A},\mathfrak{m})$ is a probability space, and whose codomain $(E,\mathcal{B})$ is a measure space.

          The \underline{\emph{distribution measure}} $P_{X}$ of a random variable
          $X : (\Omega,\mathcal{A},\mathfrak{m}) \longrightarrow (E,\mathcal{B})$ is, by definition, the probability measure $P_{X}$ defined
          on $(E,\mathfrak{B})$ as follows:  For each measurable $U \subset E$
          (i.e. the subset $U \subset E$ is a member of the $\sigma$-algebra $\mathcal{B}$),
          \begin{equation*}
          P_{X}(U)
          \; := \; \mathfrak{m}\!\left(X^{-1}(U)\right) \\ 
          \; =  \; \mathfrak{m}\!\left(\left\{\,x\in\Omega\;\vert\;X(x)\in U\;\right\}\right).
          \end{equation*}
\item  In particular, the distribution measure of each $X_{n}$ or $Y_{n}$ as in Theorem \ref{CentralLimitTheorem}
          is a probability measure on $\Re=\codomain(X_{n})=\codomain(Y_{n})$.
\item  Recall that the standard normal distribution measure $P_{N(0,1)}$ on $\Re$ is given by:
          \begin{equation*}
          P_{N(0,1)}\!\left(U\right) \; := \; \int_{U}\, \dfrac{1}{\sqrt{2\pi}}\,e^{-z^{2}/2}\,\d z,
          \;\;\textit{for each Lebesgue-measurable $U \subset \Re$}.
          \end{equation*}
          Equivalently, the probability density function $f_{N(0,1)}(z)$ of the standard normal distribution is the standard Gaussian function:
          \begin{equation*}
          f_{N(0,1)}(z) \; := \; \dfrac{1}{\sqrt{2\pi}}\,e^{-z^{2}/2} \,,
          \;\;\textit{for}\;\; z \in \Re.
          \end{equation*}          
          Equivalently, the cumulative distribution function $F_{N(0,1)}(z)$ of the standard normal distribution is given by:
          \begin{equation*}
          F_{N(0,1)}(z) \; := \; P_{N(0,1)}\!\left(\,(-\infty,z)\,\right) \; = \; \int_{-\infty}^{z}\,\dfrac{1}{\sqrt{2\pi}}\,e^{-z^{2}/2} \, \d z,
          \;\;\textit{for}\;\; z \in \Re.
          \end{equation*}          
\item  \textbf{Definition.}\quad A sequence $\left\{\mu_{n}\right\}_{n\in\N}$ of probability measures on $\Re^{d}$, $d\geq 1$,
          is said to \underline{\emph{converge weakly}} to a probability measure $\mu$ on $\Re^{d}$ if $\mu_{n}$ converges
          weakly to $\mu$ as linear functionals on the vector space $C^{0}(\Re^{d},\Re)\,\bigcap\,L^{\infty}(\Re^{d},\Re)$, i.e.
          \begin{equation*}
          \langle\,\mu_{n}\,,\,f\,\rangle := \int_{\Re^{d}}f(x)\,\mu_{n}(\d x)
          \; \longrightarrow \;
          \langle\,\mu\,,\,f\,\rangle := \int_{\Re^{d}} f(x) \, \mu(\d x),
          \;\;\textit{as}\;\; n \longrightarrow \infty,
          \end{equation*}
          for each $f \in C^{0}(\Re^{d},\Re)\,\bigcap\,L^{\infty}(\Re^{d},\Re)$.
          
          Here, $C^{0}(\Re^{d},\Re)$ denotes the vector space of all continuous $\Re$-valued functions defined on $\Re^{d}$,
          and $L^{\infty}(\Re^{d},\Re)$ denotes the vector space of all bounded $\Re$-valued functions defined on $\Re^{d}$.
          Hence, $C^{0}(\Re^{d},\Re)\bigcap L^{\infty}(\Re^{d},\Re)$ is the vector space of all bounded continuous
          $\Re$-valued functions defined on $\Re^{d}$.
\end{itemize}
\end{remark}

\newpage
\proofoutlineof Theorem \ref{CentralLimitTheorem} \vskip 0.3cm \noindent
CLAIM:\quad $\widehat{P}_{Y_{n}}$ converges pointwise to $\widehat{P}_{N(0,1)}$,\, as $n \longrightarrow \infty$. \\

Here, $\widehat{P}_{Y_{n}}:\Re\longrightarrow\C$ is the Fourier transform of the distribution measure $P_{Y_{n}}$ of the random variable $Y_{n}$, and $\widehat{P}_{N(0,1)}:\Re\longrightarrow\C$ is the Fourier transform of the standard normal distribution measure $P_{N(0,1)}$. \\

%\noindent
By Levy's Continuity Theorem \ref{LeviContinuityTheorem} and the continuity of $\widehat{P}_{N(0,1)}(\theta)=e^{-\theta^{2}/2}$ at $\theta = 0 \in \Re$, the CLAIM implies that there exists a probability measure $\nu$ on $\Re$ such that $\widehat{\nu} = \widehat{P}_{N(0,1)}$, as $\C$-valued functions on $\Re$, and that $P_{Y_{n}}$ converges weakly to $\nu$.  By the injectivity of the Fourier transform on the set of probability measures on $\Re$ (Theorem \ref{UniquenessTheorem}), we must have $\nu = P_{N(0,1)}$.  Therefore, granting the validity of the CLAIM, the proof of Theorem \ref{CentralLimitTheorem} is complete. \qed

\vskip 1.0cm
%\newpage
\proofof Theorem \ref{CentralLimitTheorem}.\quad Given the Outline above, in order to complete the proof of Theorem \ref{CentralLimitTheorem}, it remains only to establish the CLAIM in the Outline.\\

We will show that, for each $\theta\in\Re$, $\widehat{P}_{Y_{n}}(\theta) \longrightarrow \widehat{P}_{N(0,1)}(\theta) = \exp\left(-\,\dfrac{\theta^{2}}{2}\right)$, as $n \longrightarrow \infty$.
\begin{equation*}
          \widehat{P}_{Y_{n}}(\theta)
\;\; =\;\; \widehat{P}_{\frac{1}{\sigma\sqrt{n}}\overset{n}{\underset{i=1}{\sum}}(X_{i}-\mu)}(\theta) 
\;\;=\;\; \widehat{P}_{\overset{n}{\underset{i=1}{\sum}}\frac{1}{\sigma}(X_{i}-\mu)}\left(\frac{\theta}{\sqrt{n}}\right) 
\;\;=\;\; \prod_{i=1}^{n}\,\widehat{P}_{\frac{1}{\sigma}(X_{i}-\mu)}\left(\frac{\theta}{\sqrt{n}}\right)
\;\;=\;\; \prod_{i=1}^{n}\,\psi\!\left(\frac{\theta}{\sqrt{n}}\right)
\;\;=\;\; \left(\psi\!\left(\frac{\theta}{\sqrt{n}}\right)\right)^{n},
\end{equation*}
where $\psi(\theta) := \widehat{P}_{\frac{1}{\sigma}(X_{i}-\mu)}(\theta)$.  The function $\psi(\theta)$ is well-defined (i.e. independent of $i$) because the $X_{i}$'s are identically distributed.   The second equality follows from Theorem \ref{ChangeOfVariables}, whereas the third equality follows from Theorem \ref{FourierHomomorphism} and the independence hypothesis on the $X_{i}$'s.

By hypothesis on the $X_{i}$'s, $E\!\left\{\dfrac{X_{i}-\mu}{\sigma}\right\} = 0$, and $E\!\left\{\left(\dfrac{X_{i}-\mu}{\sigma}\right)^{2}\right\} = 1$.  Corollary \ref{FourierMoments} thus implies
\begin{equation*}
\begin{array}{ccccccc}
                                 E\left\{\dfrac{X_{i}-\mu}{\sigma}\right\}  = 0 < \infty
& \Longrightarrow & \psi'(0) &= & \i\,E\!\left\{\dfrac{X_{i} - \mu}{\sigma}\right\} &=& 0, \\ \\
                                 E\left\{\left(\dfrac{X_{i}-\mu}{\sigma}\right)^{2}\right\} = 1 < \infty
& \Longrightarrow & \psi''(0) &=&  - \, E\left\{\left(\dfrac{X_{i}-\mu}{\sigma}\right)^{2}\right\} &=& -1. \\
\end{array}
\end{equation*}
By Taylor's Theorem,
\begin{equation*}
\psi(\theta) \;=\; \psi(0) + \psi'(0)\theta + \dfrac{1}{2}\psi''(r)\theta^{2} \;=\; 1 + \dfrac{\theta^{2}}{2}\psi''(r),
\;\;\textnormal{for some}\;\; |r| < |\theta|.
\end{equation*}
%\newpage
Thus,
\begin{eqnarray*}
        \lim_{n\rightarrow\infty}\left(\psi\!\left(\frac{\theta}{\sqrt{n}}\right)\right)^{n}
&=& \lim_{n\rightarrow\infty}\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)^{n},
        \quad \textnormal{where}\quad |s| < \dfrac{|\theta|}{\sqrt{n}} \\
&=& \exp\left\{\log\left[\lim_{n\rightarrow\infty}\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)^{n}\right]\right\} 
\;\;=\;\; \exp\left\{\lim_{n\rightarrow\infty}\left[n\,\log\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)\right]\right\} \\
&=& \exp\left\{\lim_{n\rightarrow\infty}\left[n\cdot\dfrac{\theta^{2}}{2n}\psi''(s)\cdot\dfrac{\log\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)-\log(1)}{\dfrac{\theta^{2}}{2n}\psi''(s)}\right]\right\} \\
&=& \exp\left\{\dfrac{\theta^{2}}{2}\cdot\left(\lim_{n\rightarrow\infty}\psi''(s)\right)\cdot\left(\lim_{n\rightarrow\infty}\,\dfrac{\log\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)-\,\log(1)}{\dfrac{\theta^{2}}{2n}\psi''(s)}\right)\right\} \\
\end{eqnarray*}
Now, $s \rightarrow 0$ as $n \rightarrow\infty$, since $|s| < \dfrac{|\theta|}{\sqrt{t}}$.  By Theorem \ref{FourierPartials} and the hypothesis that $E\!\left\{\left(\dfrac{X_{i}-\mu}{\sigma}\right)^{2}\right\} = 1$, we see that $\psi''$ is continuous; furthermore, Corollary \ref{FourierMoments} implies
\begin{equation*}
\lim_{n\rightarrow\infty}\,\psi''(s) \; = \; \psi''(0) \; = \; -1.
\end{equation*}
Now that we know $\underset{n\rightarrow\infty}{\lim}\,\psi''(s)$ is finite, we immediately see that
\begin{equation*}
\lim_{n\rightarrow\infty}\,\dfrac{\log\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)-\,\log(1)}{\dfrac{\theta^{2}}{2n}\psi''(s)}
\;\; = \;\; \lim_{h\rightarrow 0}\,\dfrac{\log(1+h)-\log(1)}{h}
\;\; = \;\; \left.\dfrac{\d}{\d x}\log(x)\,\right|_{x=1}
\;\; = \;\; \left.\dfrac{1}{x}\,\right|_{x=1}
\;\; = \;\; 1.
\end{equation*}
Hence, we may now deduce that
\begin{eqnarray*}
        \lim_{n\rightarrow\infty}\left(\psi\!\left(\frac{\theta}{\sqrt{n}}\right)\right)^{n}
&=& \exp\left\{\dfrac{\theta^{2}}{2}\cdot\left(\lim_{n\rightarrow\infty}\psi''(s)\right)\cdot\left(\lim_{n\rightarrow\infty}\,\dfrac{\log\left(1+\dfrac{\theta^{2}}{2n}\psi''(s)\right)-\,\log(1)}{\dfrac{\theta^{2}}{2n}\psi''(s)}\right)\right\} \\
&=& \exp\left\{\dfrac{\theta^{2}}{2}\cdot\left(-1\right)\cdot\left(1\right)\right\} 
\;\;=\;\; \exp\left\{\,-\,\dfrac{\theta^{2}}{2}\right\} \\
&=& \widehat{P}_{N(0,1)}(\theta)
\end{eqnarray*}
\qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{The Fourier transform of a probability measure on $\Re$ and the characteristic function of an $\Re$-valued random variable}
\setcounter{theorem}{0}

\begin{definition}[Fourier transform of a probability measure on $\Re$] \mbox{} \vskip 0.1cm \noindent
The \underline{\emph{Fourier transform}} $\widehat{\mu}$ of a probability measure $\mu$ on $\Re$ is the $\C$-valued function $\widehat{\mu} : \Re \longrightarrow \C$ defined on $\Re$ via
\begin{equation*}
\widehat{\mu}(\theta) \; := \; E\!\left\{\,e^{\i\theta X}\,\right\} \; = \; \int_{\Re} \, e^{\i\theta x} \, \mu(\d x),
\quad\textnormal{for}\;\; \theta \in \Re.
\end{equation*}
\end{definition}

\begin{definition}[Characteristic function of a random variable] \mbox{} \vskip 0.1cm \noindent
Let $X$ be an $\Re$-valued random variable, and $P_{X}$ its distribution measure on $\Re = \codomain(X)$.  The \underline{\emph{characteristic function}} of $X$ is by definition the Fourier transform of $P_{X}$.  Explicitly, the characteristic function of $X$ is the function $\widehat{P}_{X} : \Re \longrightarrow \C$ defined by:
\begin{equation*}
\widehat{P}_{X}(\theta) \; = \; \int_{\Re} \, e^{\i\theta x} \, P_{X}(\d x),
\quad\textnormal{for}\;\; \theta \in \Re.
\end{equation*}
\end{definition}

\begin{theorem}[Theorem 13.1, \cite{JacodProtter}] \mbox{} \vskip 0.1cm \noindent
The Fourier transform $\widehat{\mu}$ of any probability measure $\mu$ on $\Re$ is a bounded and continuous $\C$-valued function on $\Re$, and $\widehat{\mu}(0) = 1$.
\end{theorem}

\begin{remark} \mbox{} \vskip 0.1cm \noindent
The Fourier transform can thus be regarded as a map from the set\footnote{Note that the set of probability measures on $\Re$ does not form a vector space.} of all probability measures on $\Re$ into the set of all bounded continuous $\C$-valued functions defined on $\Re$.
\end{remark}

\begin{theorem}[Theorem 13.3, \cite{JacodProtter}]\label{ChangeOfVariables} \mbox{} \vskip 0.1cm \noindent
Let $X$ be an $\Re$-valued random variable and $\alpha, \beta \in \Re$.  Then, for any $\theta \in \Re$,
\begin{equation*}
\widehat{P}_{\alpha X + \beta}(\theta) \; = \;
e^{\i\beta\theta}\cdot\widehat{P}_{X}(\alpha\,\theta).
\end{equation*}
\end{theorem}
\proof
\begin{equation*}
        \widehat{P}_{\alpha X + \beta}(\theta)
\;=\;  E\!\left\{e^{\i(\alpha X + \beta)\theta}\right\}
%\;=\;  E\!\left\{e^{\i\beta\theta} \cdot e^{\i\theta\alpha X}\right\}
\;=\;  \int_{\Re}\, e^{\i\beta\theta} \cdot e^{\i(\alpha\theta)x}\,P_{X}(\d x)
\;=\;  e^{\i\beta\theta} \cdot \int_{\Re}\, e^{\i(\alpha\theta)x}\,P_{X}(\d x)
\;=\;  e^{\i\beta\theta}\cdot\widehat{P}_{X}(\alpha\,\theta)
\end{equation*}
\qed

\begin{theorem}[Theorem 15.2, \cite{JacodProtter}]\label{FourierHomomorphism} \mbox{} \vskip 0.1cm \noindent
The characteristic function of the sum of two independent $\Re$-valued random variables is the product of their characteristic functions.  

More precisely, if $X, Y : \Omega \longrightarrow \Re$ are independent $\Re$-valued random variables with respective characteristic functions $\widehat{P}_{X}, \widehat{P}_{Y} : \Re \longrightarrow \C$, then the characteristic function $\widehat{P}_{Z}$ of the random variable $Z := X + Y$ is given in terms of $\widehat{P}_{X}$ and $\widehat{P}_{Y}$ by:
\begin{equation*}
\widehat{P}_{Z}(\theta) \; = \; \widehat{P}_{X}(\theta)\cdot\widehat{P}_{Y}(\theta),
\quad\textnormal{for each}\;\; \theta\in\Re.
\end{equation*}
\end{theorem}

\begin{theorem}[Theorem 13.2, \cite{JacodProtter}]\label{FourierPartials} \mbox{} \vskip 0.1cm \noindent
Let $X$ be an $\Re$-valued random variable and suppose that $E\{|X|^{m}\} < \infty$ for some non-negative integer $m$.  Then the Fourier transform $\widehat{P}_{X}$ of the distribution measure $P_{X}$ has continuous derivatives up to order $m$, and
\begin{equation*}
\dfrac{\d^{m}}{\d \theta^{m}}\widehat{P}_{X}(\theta) \; = \; \i^{m}E\!\left\{X^{m}e^{\i\,\theta X}\right\}
\end{equation*}
\end{theorem}

\begin{corollary}\label{FourierMoments} \mbox{} \vskip 0.1cm \noindent
For an $\Re$-valued random variable $X$,
\begin{eqnarray*}
E\left\{|X|\right\}     < \infty & \Longrightarrow & E\left\{X\right\}       = -\,\i \, \widehat{P}'_{X}(0), \\
E\left\{X^{2}\right\} < \infty & \Longrightarrow & E\left\{X^{2}\right\} = - \widehat{P}_{X}''(0). \\
\end{eqnarray*}
\end{corollary}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{The Fourier transform of the standard normal distribution measure on $\Re$}
\setcounter{theorem}{0}

Recall that probability density function of the standard normal (or standard Gaussian) distribution measure $P_{N(0,1)}$ is
\begin{equation*}
f_{N(0,1)}(z) \; := \; \dfrac{1}{\sqrt{2\pi}} e^{-z^{2}/2} \,,
\;\;\textit{for}\;\; z \in \Re.
\end{equation*}
The Fourier transform $\widehat{f}_{N(0,1)}$ of $f_{N(0,1)}$ is
\begin{equation*}
\widehat{f}_{N(0,1)}(\theta)
\; := \; E\left(\,e^{\i\,\theta Z}\,\right)
\; = \; \int_{-\infty}^{\infty}\, e^{\i\,\theta z} \, f_{N(0,1)}(z) \,\d z
\; = \; \int_{-\infty}^{\infty}\, \left(\cos(\theta z)+\i\,\sin(\theta z)\right) \dfrac{1}{\sqrt{2\pi}} e^{-z^{2}/2} \,\d z
\; = \; e^{-\theta^{2}/2} \,,
\;\;\textit{for}\;\; \theta \in \Re,
\end{equation*}
The function $\widehat{f}_{N(0,1)}$ is also, by definition, the Fourier transform $\widehat{P}_{N(0,1)}$ of the standard Gaussian distribution measure $P_{N(0,1)}$.  In other words, $\widehat{P}_{N(0,1)} = \widehat{f}_{N(0,1)}$.

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Injectivity and continuity of the Fourier transform on the space of probability measures on $\Re$}
\setcounter{theorem}{0}

\begin{theorem}[Injectivity of the Fourier transform on the space of probability measures on $\Re$]\label{UniquenessTheorem} \mbox{} \vskip 0.1cm \noindent
If the Fourier transforms of two probability measures on $\Re^{d}$ are equal (as $\C$-valued functions on $\Re^{d}$), then the two probability measures themselves are equal.
\end{theorem}
See Theorem 14.1, \cite{JacodProtter}.

\begin{remark} \mbox{} \vskip 0.1cm \noindent
Recall that the Fourier transform can be regarded a map from the set of all probability measures on $\Re$ into the set of all bounded continuous $\C$-valued functions defined on $\Re$.  The above injectivity theorem states that this Fourier transform map is injective.
\end{remark}

\begin{theorem}[Levy's Continuity Theorem of the Fourier transform]\label{LeviContinuityTheorem} \mbox{} \vskip 0.1cm \noindent
Let $\{\,\mu_{n}\,\}_{n\in\N}$ be a sequence of probability measures on $\Re^{d}$, $d \geq 1$, and let $\widehat{\mu_{n}}:\Re^{d}\longrightarrow\C$ be the Fourier transform of $\mu_{n}$.
\begin{itemize}
\item  If $\mu_{n}$ converges weakly to a measure $\mu$, then $\widehat{\mu_{n}}$ converges pointwise to $\widehat{\mu}$,
          i.e. $\widehat{\mu_{n}}(\theta)$ converges to $\widehat{\mu}(\theta)$, for each $\theta\in\Re^{d}$.
\item  If $\widehat{\mu_{u}}$ converges pointwise to some function $f:\Re^{d}\longrightarrow\C$,
          and $f$ is continuous at $\mathbf{0}\in\Re^{d}$,
          then there exists a probability measure $\mu$ on $\Re^{d}$ such that $\widehat{\mu} = f$,
          and $\mu_{n}$ converges weakly to $\mu$.
\end{itemize}
\end{theorem}
See Theorem 19.1, \cite{JacodProtter}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{alpha}
%\bibliographystyle{plain}
%\bibliographystyle{amsplain}
\bibliographystyle{acm}
\bibliography{KenChuBioinformatics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

