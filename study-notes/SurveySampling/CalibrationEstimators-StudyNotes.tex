
\documentclass{article}

\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{doublespace}

\usepackage{KenChuArticleStyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{page}{1}

\pagestyle{fancy}

%\input{../CourseSemesterUnique}

%\rhead[\CourseSemesterUnique]{Kenneth Chu (300517641)}
%\lhead[Kenneth Chu (300517641)]{\CourseSemesterUnique}
\rhead[Study Notes]{Kenneth Chu}
\lhead[Kenneth Chu]{Study Notes}
\chead[]{{\Large\bf Calibration Estimators in Survey Sampling} \\
\vskip 0.1cm \normalsize \today}
\lfoot[]{}
\cfoot[]{}
\rfoot[]{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\mbox{}\vskip -0.3cm
\section{Generalized Regression Estimator as a special case of Calibration Estimators}
\setcounter{theorem}{0}

This is a summary of Section 1 of the article \cite{DevilleSarndal1992}.

Let $U = \{1,2,\ldots,N\}$ be a finite population.
Let $y : U \longrightarrow \Re$ be an $\Re$-valued function defined on $U$
(commonly called a ``population parameter").
We will use the common notation $y_{i}$ for $y(i)$.
We wish to estimate $T_{y} := \sum_{i \in U} y_{i}$ via survey sampling.
Let $p:\mathcal{S} \longrightarrow (0,1]$ be our chosen sampling design,
where $\mathcal{S} \subseteq \mathcal{P}(U)$ is the set of all possible
samples in the design, and $\mathcal{P}(U)$ is the power set of $U$.
For each $k \in U$, let $\pi_{k} := \sum_{s \ni k}p(s)$ be the inclusion probability
of $k$ under the sampling design $p$.
We assume $\pi_{k} > 0$ for each $k \in U$.
Then, the Horvitz-Thompson estimator
\begin{equation*}
\widehat{T}^{\textnormal{HT}}_{y}(s)
\;:=\;\sum_{k \in s} \dfrac{y_{k}}{\pi_{k}}
\; =\;\sum_{k \in s} d_{k}y_{k}
\; =\;\sum_{k \in U} I_{ks}\dfrac{y_{k}}{\pi_{k}},
\quad
\textnormal{where $d_{k} := \dfrac{1}{\pi_{k}}$ and}
\;
I_{ks} \; := \; \left\{\begin{array}{cl} 1, & \textnormal{if $k \in s$} \\ 0, &  \textnormal{otherwise} \end{array}\right. 
\end{equation*}
is well-defined and is known to be a design-unbiased estimator of $T_{y}$; in other words,
\begin{equation*}
E_{p}\!\left[\;\widehat{T}^{\textnormal{HT}}_{y}\;\right]
\; = \; \sum_{s \in \mathcal{S}} p(s)\cdot \widehat{T}^{\textnormal{HT}}_{y}(s)
\; = \; \sum_{s \in \mathcal{S}} p(s)\cdot\left(\sum_{k \in U} I_{ks}\dfrac{y_{k}}{\pi_{k}}\right)
\; = \; \sum_{k \in U} \dfrac{y_{k}}{\pi_{k}} \left(\sum_{s \in \mathcal{S}} p(s) I_{ks}\right)
\; = \; \sum_{k \in U} \dfrac{y_{k}}{\pi_{k}}\,\pi_{k}
\; = \; T_{y}.
\end{equation*}
We will call the $d_{k}$'s above the \textit{Horvitz-Thompson weights}.

Roughly, the generalized regression estimator for $T_{y}$ is an estimator of the form:
\begin{equation*}
\widehat{T}^{\textnormal{GREG}}_{y}(s)
\; := \;
\sum_{k \in s} w_{k}(s) y_{k},
\end{equation*}
where the sample-dependent ``calibrated" weights $w_{k}(s)$ are the solution of a
certain constrained minimization problem (see below) where the objective function
depends on the $w_{k}(s)$'s and the Horvitz-Thompson weights $d_{k}$'s, while
the constraints involve the $w_{k}(s)$'s and auxiliary information.
More precisely, the calibrated weights $w_{k}(s)$ solve the following constrained
minimization problem:

\vskip 0.5cm
\begin{center}
\begin{minipage}{6.0in}
\begin{center}
\textbf{\large\underline{Constrained Minimization Problem for the GREG calibrated weights}}
\end{center}
\noindent
\textbf{Conceptual framework:}\;\;
Let $\mathbf{x} : U \longrightarrow \Re^{1 \times J}$ be an $\Re^{1 \times J}$-valued
function defined on $U$.
We use the common notation $\mathbf{x}_{k}$ for $\mathbf{x}(k)$, for each $k \in U$.
\vskip 0.3cm
\noindent
\textbf{Assumptions:}
\begin{itemize}
\item	The population total of $\mathbf{x}$
		\begin{equation*} T_{\mathbf{x}} \; := \; \sum_{k \in U} \mathbf{x}_{k} \in \Re^{1\times J} \end{equation*}
		is known.
\item	For each $s \in \mathcal{S}$, the value $(y_{k},\mathbf{x}_{k})$ can be observed for each $k \in s$ via the sampling procedure.
\end{itemize}
\vskip 0.3cm
\noindent
\textbf{Constrained Minimization Problem:}\;\;
For each $k \in U$, let $q_{k} > 0$ be chosen.
For each $s \in \mathcal{S}$, the calibrated weights $w_{k}(s)$, for $k \in s$, are obtained
by minimizing the following objective function:
\begin{equation*}
f_{s}(w_{k}(s)\;;\,d_{k},q_{k})
\;:=\;\sum_{k \in s}\,\dfrac{(w_{k}(s) - d_{k})^{2}}{d_{k}q_{k}}
\end{equation*}
subject to the (vectorial) constraint on $w_{k}(s)$:
\begin{equation*}
\mathbf{h}(w_{k}(s)\,;\,\mathbf{x}_{k},T_{\mathbf{x}})
\;:=\; - T_{\mathbf{x}} + \sum_{k \in s}\, w_{k}(s)\,\mathbf{x}_{k} \; = \; 0.
\end{equation*}
\end{minipage}
\end{center}
\vskip 1.0cm
The above constrained minimization problem for the calibrated weights can be solved
by the method of Lagrange Multipliers.

\vskip 0.5cm
\noindent
\textbf{Solution of the Constrained Minimization Problem for the Generalized Regression Estimator calibrated weights:}
\vskip 0.3cm
Let $s \in \mathcal{S}$ be fixed. We write the objective function as
\begin{equation*}
f(\{w_{k}(s)\,:k\in s\,\}) \; = \; \sum_{k \in s} \dfrac{(w_{k}(s) - d_{k})^{2}}{d_{k}q_{k}},
\end{equation*}
and we write the constraints on $w_{k}(s)$ as:
\begin{equation*}
h_{j}(\{w_{k}(s)\,:k\in s\,\}) \; = \; \sum_{k \in s} w_{k}(s)\,x_{kj} - T_{x_{j}} = 0,
\quad j = 1, 2, \ldots, J.
\end{equation*}
By the Method of Lagrange Multipliers, if $\mathbf{w}_{0} = \{w_{k}(s)\,:\,k \in s\}$ is a
solution to the constrained minimization problem, then $\mathbf{w}_{0}$ satisfies:
\begin{equation*}
\nabla_{w}f(\mathbf{w}_{0}) \;\in\; \span\!\left\{\,\nabla_{w}h_{j}(\mathbf{w}_{0})\,:\;j = 1,2,\ldots, J\,\right\}.
\end{equation*}
Now,
\begin{equation*}
\dfrac{\partial f}{\partial w_{k}(s)} \; = \; \dfrac{2(w_{k}(s) - d_{k})}{d_{k}q_{k}}
\quad\textnormal{and}\quad
\dfrac{\partial h_{j}}{\partial w_{k}(s)} \; = \; x_{kj}.
\end{equation*}
Thus, we seek $\lambda_{1},\lambda_{2},\ldots,\lambda_{J}$ such that
\begin{equation*}
\dfrac{2(w_{k}(s) - d_{k})}{d_{k}q_{k}}
\;=\; \dfrac{\partial f}{\partial w_{k}(s)}
\;=\; \sum_{j = 1}^{J} 2\,\lambda_{j}\,\dfrac{\partial h_{j}}{\partial w_{k}(s)}
\;=\; \sum_{j = 1}^{J} 2\,\lambda_{j}\,x_{kj},
\end{equation*}
which immediately implies:
\begin{equation*}
w_{k}(s) \;=\; d_{k}\left(1 + q_{k}\sum_{j=1}^{J}\lambda_{j}x_{kj}\right).
\end{equation*}
Substituting the above expression for $w_{k}(s)$ back into the constraints yields,
for each $i = 1, 2, \ldots, J$:
\begin{equation*}
-T_{x_{i}} + \sum_{k \in s} d_{k}\left(1 + q_{k}\,\sum_{j=1}^{J}\lambda_{j}x_{kj}\right)\,x_{ki} = 0,
\end{equation*}
which can be rearranged to be:
\begin{equation*}
\sum_{k \in s}\,d_{k}\,x_{ki} + \sum_{j=1}^{J}\left(\sum_{k \in s}d_{k}q_{k}x_{ki}x_{kj}\right)\lambda_{j} = T_{x_{i}}
\end{equation*}
The preceding equation can be rewritten in vectorial form:
\begin{equation*}
\widehat{T}^{\textnormal{HT}}_{\mathbf{x}}(s) + \mathbf{A}(s)\cdot\mathbf{\lambda} \; = \; T_{\mathbf{x}},
\end{equation*}
where $\mathbf{A}(s) \in \Re^{J \times J}$ is the symmetric matrix with entries:
\begin{equation*}
\mathbf{A}(s)_{ij} = \sum_{k \in s}d_{k}q_{k}x_{ki}x_{kj}.
\end{equation*}
{\color{red}Assuming the matrix $\mathbf{A}(s)$ is invertible}, the vector $\lambda$ of Lagrange multipliers
is given by:
\begin{equation*}
\mathbf{\lambda} \;=\; \mathbf{A}(s)^{-1}\left(\,T_{\mathbf{x}} \,-\, \widehat{T}^{\textnormal{HT}}_{x}(s)\,\right).
\end{equation*}
Hence, the generalized regression estimator $\widehat{T}^{\textnormal{GREG}}_{y}(s)$
is given by:
\begin{eqnarray*}
\widehat{T}^{\textnormal{GREG}}_{y}(s)
&=& \sum_{k \in s}w_{k}(s)y_{k}
\quad = \quad \sum_{k \in s}\,d_{k}(1 + q_{k}\,\mathbf{x}_{k}^{T}\,\mathbf{\lambda})\,y_{k}
\quad = \quad \sum_{k \in s}\,d_{k}y_{k} + \sum_{k \in s}\,d_{k}q_{k}(\mathbf{x}_{k}^{T}\cdot\mathbf{\lambda})\,y_{k}\\
&=& \widehat{T}^{\textnormal{HT}}_{y}(s) + \left(\sum_{k \in s}\,d_{k}q_{k}y_{k}\cdot\mathbf{x}_{k}^{T}\right)\cdot\mathbf{\lambda}\\
&=& \widehat{T}^{\textnormal{HT}}_{y}(s) + \left(\sum_{k \in s}\,d_{k}q_{k}y_{k}\cdot\mathbf{x}_{k}^{T}\right)\cdot\mathbf{A}(s)^{-1}\cdot\left(\,T_{\mathbf{x}} \,-\, \widehat{T}^{\textnormal{HT}}_{x}(s)\,\right).
\end{eqnarray*}
\qed

\vskip 0.5cm
\noindent
\textbf{Example: Ratio estimator as a special case of GREG estimator (hence also of calibration estimator)}
\vskip 0.3cm
\noindent
We first give the definition of the Ratio Estimator.
\vskip 0.3cm
\begin{center}
\begin{minipage}{6.0in}
\noindent
\textbf{Definition 
(Ratio Estimator of the population total $T_{y}$ of a population characteristic $y$ with respect to that of another characteristic $x$)
[See Section 5.6, \cite{Sarndal1992}, p.176; see also Chapter 6, \cite{Cochran1977}.]}
\vskip 0.1cm
Let $U = \{\,1,2,\ldots,N\,\}$ be a finite population.
Let $x, y : U \longrightarrow \Re$ be two population characteristics.
Suppose the population total $T_{x} := \sum_{k=1}^{N}x_{k}$ of $x$ is known.
Let $p : \mathcal{S} \subset \mathcal{P}(U) \longrightarrow (0,1]$ be a sampling design
such that the inclusion probability $\pi_{k} := \sum_{s \ni k}p(s) > 0$, for each $k \in U$.
Hence, $\widehat{T}^{\textnormal{HT}}_{y}(s)$ and $\widehat{T}^{\textnormal{HT}}_{x}(s)$ are well-defined for each sample $s \in \mathcal{S}$.
The \textbf{ratio estimator}, $\widehat{T}^{\textnormal{R}}_{y} : \mathcal{S} \longrightarrow \Re$, of the population $T_{y}$ of $y$ is, by definition,
\begin{equation*}
\widehat{T}^{\textnormal{R}}_{y}(s)
\;:=\;
T_{x}\cdot\dfrac{\widehat{T}^{\textnormal{HT}}_{y}\!(s)}{\widehat{T}^{\textnormal{HT}}_{x}\!(s)},
\quad
\textnormal{for each $s \in \mathcal{S}$}.
\end{equation*}
\end{minipage}
\end{center}

\noindent
Now, we make the following:
\vskip 0.3cm
\begin{center}
\begin{minipage}{6.0in}
\noindent
\textbf{Observation:\quad$\widehat{T}^{\textnormal{GREG}}_{y} = \widehat{T}^{\textnormal{R}}_{y}$, under the choice $d_{i} = 1/\pi_{i}$ and $q_{k} = 1/x_{k}$}
\vskip 0.1cm
Indeed, $\mathbf{A}(s)$ is now a scalar, and we write $A(s)$, and
\begin{equation*}
A(s)
\;=\; \sum_{k \in s}d_{k}q_{k}x_{k}^{2}
\;=\; \sum_{k \in s}\dfrac{1}{\pi_{k}}\dfrac{1}{x_{k}}x_{k}^{2}
\;=\; \sum_{k \in s}\dfrac{x_{k}}{\pi_{k}}
\;=\; \widehat{T}^{\textnormal{HT}}_{x}\!(s).
\end{equation*}
Next, the Lagrange multiplier $\lambda = \lambda(s)$ is now given by:
\begin{equation*}
\lambda \; = \; \lambda(s)
\;=\; \dfrac{1}{A(s)}\left(T_{x} - \widehat{T}^{\textnormal{HT}}_{x}\!(s)\right)
\;=\; \dfrac{1}{\widehat{T}^{\textnormal{HT}}_{x}\!(s)}\left[\,T_{x} - \widehat{T}^{\textnormal{HT}}_{x}\!(s)\,\right]
\;=\; \dfrac{T_{x}}{\widehat{T}^{\textnormal{HT}}_{x}\!(s)} - 1
\end{equation*}
Thus, the Generalized Regression Estimator $\widehat{T}^{\textnormal{GREG}}_{y}$ of $T_{y}$ is given by:
\begin{eqnarray*}
\widehat{T}^{\textnormal{GREG}}_{y}\!(s)
&=& \widehat{T}^{\textnormal{HT}}_{y}\!(s) + \left(\,\sum_{k\in s}\dfrac{1}{\pi_{k}}\dfrac{1}{x_{k}}y_{k}x_{k}\,\right)\lambda
\quad=\quad \widehat{T}^{\textnormal{HT}}_{y}\!(s) + \widehat{T}^{\textnormal{HT}}_{y}\!(s)\left(\,\dfrac{T_{x}}{\widehat{T}^{\textnormal{HT}}_{x}\!(s)} - 1\,\right)\\
&=& T_{x}\cdot\dfrac{\widehat{T}^{\textnormal{HT}}_{y}\!(s)}{\widehat{T}^{\textnormal{HT}}_{x}\!(s)} \\
&=:& \widehat{T}^{\textnormal{R}}_{y}(s),
\end{eqnarray*}
as required. \qed
\end{minipage}
\end{center}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Calibration Estimators}
\setcounter{theorem}{0}

The general calibration estimator $\widehat{T}^{\textnormal{Cal}}_{y}$ is very similar to the
generalized regression estimator $\widehat{T}^{\textnormal{GREG}}_{y}$, in that 
$\widehat{T}^{\textnormal{Cal}}_{y}$ is also the solution to a constrained minimization problem.
The difference is that the objection function in the case of $\widehat{T}^{\textnormal{Cal}}_{y}$
has a more general form.

\begin{center}
\begin{minipage}{6.0in}
\begin{center}
\textbf{\large\underline{Constrained Minimization Problem for Weights of Calibration Estimators}}
\end{center}
\noindent
\textbf{Conceptual framework:}\;\;
Let $\mathbf{x} : U \longrightarrow \Re^{1 \times J}$ be an $\Re^{1 \times J}$-valued
function defined on $U$.
We use the common notation $\mathbf{x}_{k}$ for $\mathbf{x}(k)$, for each $k \in U$.
\vskip 0.3cm
\noindent
\textbf{Assumptions:}
\begin{itemize}
\item	The population total of $\mathbf{x}$
		\begin{equation*} T_{\mathbf{x}} \; := \; \sum_{k \in U} \mathbf{x}_{k} \in \Re^{1\times J} \end{equation*}
		is known.
\item	For each $s \in \mathcal{S}$, the value $(y_{k},\mathbf{x}_{k})$ can be observed for each $k \in s$
		via the sampling procedure.
\item	For each $k \in U$, $G_{k}(w\;;\,d)$ is an $\Re$-valued function which satisfies:
		\begin{enumerate}
		\item	For each $d > 0$, $G_{k}(w\;;\,d)$ is non-negative, differentiable with respect to $w$,
				strictly convex in $w$, defined on an open interval $D_{k}(d)$ containing $d$, and
				such that $G_{k}(k\;;\,k) = 0$.
		\item	$g_{k}(w\;;\,d) := \dfrac{\partial G_{k}(w\;;\,d)}{\partial w}$ is continuous in $w$ and maps
				$D_{k}(d)$ bijectively onto its image.
		\end{enumerate}
\end{itemize}
\vskip 0.3cm
\noindent
\textbf{Constrained Minimization Problem:}\;\;
For each $k \in U$, let $q_{k} > 0$ be chosen.
For each $s \in \mathcal{S}$, the calibrated weights $w_{k}(s)$, for $k \in s$, are obtained
by minimizing the following objective function:
\begin{equation*}
f_{s}(w_{k}(s)\;;\,d_{k})
\;:=\;\sum_{k \in s}\,G_{k}(w_{k}(s)\;;\,d_{k})
\end{equation*}
subject to the (vectorial) constraint on $w_{k}(s)$:
\begin{equation*}
\mathbf{h}(w_{k}(s)\,;\,\mathbf{x}_{k},T_{\mathbf{x}})
\;:=\; - T_{\mathbf{x}} + \sum_{k \in s}\, w_{k}(s)\,\mathbf{x}_{k} \; = \; 0.
\end{equation*}
\end{minipage}
\end{center}

\vskip 0.5cm
\noindent
\textbf{Solution of the Constrained Minimization Problem for weights of calibration estimators:}
\vskip 0.3cm
Let $s \in \mathcal{S}$ be fixed. We write the objective function as
\begin{equation*}
f(\{w_{k}(s)\,:k\in s\,\}) \; = \; \sum_{k \in s} G_{k}(w_{k}(s)\;;\,d_{k}),
\end{equation*}
and we write the constraints on $w_{k}(s)$ as:
\begin{equation*}
h_{j}(\{w_{k}(s)\,:k\in s\,\}) \; = \; \sum_{k \in s} w_{k}(s)\,x_{kj} - T_{x_{j}} = 0,
\quad j = 1, 2, \ldots, J.
\end{equation*}
By the Method of Lagrange Multipliers, if $\mathbf{w}_{0} = \{w_{k}(s)\,:\,k \in s\}$ is a
solution to the constrained minimization problem, then $\mathbf{w}_{0}$ satisfies:
\begin{equation*}
\nabla_{w}f(\mathbf{w}_{0}) \;\in\; \span\!\left\{\,\nabla_{w}h_{j}(\mathbf{w}_{0})\,:\;j = 1,2,\ldots, J\,\right\}.
\end{equation*}
Now,
\begin{equation*}
\dfrac{\partial f}{\partial w_{k}(s)} \; = \; g_{k}(w_{k}(s)\;;\,d_{k})
\quad\textnormal{and}\quad
\dfrac{\partial h_{j}}{\partial w_{k}(s)} \; = \; x_{kj}.
\end{equation*}
Thus, we seek $\lambda_{1},\lambda_{2},\ldots,\lambda_{J}$ such that
\begin{equation*}
g_{k}(w_{k}(s)\;;\,d_{k})
\;=\; \dfrac{\partial f}{\partial w_{k}(s)}
\;=\; \sum_{j = 1}^{J} \lambda_{j}\,\dfrac{\partial h_{j}}{\partial w_{k}(s)}
\;=\; \sum_{j = 1}^{J} \lambda_{j}\,x_{kj}
\;=\; \mathbf{x}_{k}^{T}\cdot\mathbf{\lambda}.
\end{equation*}
By hypothesis, $g_{k}(\;\cdot\;;\,d_{k})$ is bijective on $D_{k}(d_{k})$.
We denote its inverse by $g^{-1}_{k}(\;\cdot\;;\,d_{k})$.
Thus, we have
\begin{equation*}
w_{k}(s)
\;=\; g^{-1}_{k}\!\left(\,\mathbf{x}_{k}^{T}\cdot\mathbf{\lambda}\;;\,d_{k}\,\right)
\;=\; d_{k}\cdot F_{k}\!\left(\,\mathbf{x}_{k}^{T}\cdot\mathbf{\lambda}\,\right),
\end{equation*}
where $F_{k}(\;\cdot\;) := \dfrac{1}{d_{k}}\,g^{-1}_{k}\!\left(\;\cdot\;;\,d_{k}\,\right)$.
The constraint equation can thus be rewritten as follows:
For each $j = 1, 2, \ldots, J$,
\begin{eqnarray*}
\sum_{k \in s}\,w_{k}(s)\,x_{kj} &=& T_{x_{j}} \\
\sum_{k \in s}\,d_{k}F_{k}(\mathbf{x}_{k}^{T}\cdot\lambda)\,x_{kj} &=& T_{x_{j}} \\
\sum_{k \in s}\,d_{k}\left[F_{k}(\mathbf{x}_{k}^{T}\cdot\lambda) - 1\right] x_{kj} &=& T_{x_{j}} -  \sum_{k \in s}\,d_{k}x_{kj} \;\;=\;\; T_{x_{j}} - \widehat{T}^{\textnormal{HT}}_{x_{j}}(s). \\
\end{eqnarray*}
In vectorial form, we have:
\begin{equation*}
\sum_{k \in s}\,d_{k}\left[F_{k}(\mathbf{x}_{k}^{T}\cdot\lambda) - 1\right] \mathbf{x}_{k}
\;\;=\;\; T_{\mathbf{x}} - \widehat{T}^{\textnormal{HT}}_{\mathbf{x}}(s).
\end{equation*}
Note that, for each obtained sample $s \in \mathcal{S}$, the Lagrange multiplier vector
$\lambda = (\lambda_{1},\lambda_{2},\ldots,\lambda_{J}) \in \Re^{J}$
is the only unknown quantity in the above constraint equation.
{\color{red}We now assume the above vectorial constraint equation is solvable for $\lambda$},
and denote its solution by $\lambda^{*}$.
Then, the \textbf{calibration estimator} of $T_{y}$ is given by:
\begin{equation*}
\widehat{T}^{\textnormal{Cal}}_{y}(s)
\;\;=\;\;
\sum_{k \in s}\,d_{k}\cdot F_{k}\!\left(\,\mathbf{x}_{k}^{T}\cdot\lambda^{*}\,\right)\cdot y_{k}
\end{equation*}
\qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{alpha}
%\bibliographystyle{plain}
%\bibliographystyle{amsplain}
\bibliographystyle{acm}
\bibliography{KenChuStatistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

