
\documentclass{article}

\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{doublespace}

\usepackage{KenChuArticleStyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{page}{1}

\pagestyle{fancy}

%\input{../CourseSemesterUnique}

%\rhead[\CourseSemesterUnique]{Kenneth Chu (300517641)}
%\lhead[Kenneth Chu (300517641)]{\CourseSemesterUnique}
\rhead[Study Notes]{Kenneth Chu}
\lhead[Kenneth Chu]{Study Notes}
\chead[]{{\Large\bf Multi-stage Sampling} \\
\vskip 0.1cm \normalsize \today}
\lfoot[]{}
\cfoot[]{}
\rfoot[]{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\mbox{}\vskip 0.0cm
\section{Sample space of two-stage sampling \& its probability function}
\setcounter{theorem}{0}

Let $U^{(1)}$ be a finite set of size $N^{(1)}$.
Let $U^{(2)}_{1}, U^{(2)}_{2},\ldots,U^{(2)}_{N^{(1)}}$ be finite sets
of sizes $N^{(2)}_{1}, N^{(2)}_{2},\ldots,N^{(2)}_{N^{(1)}}$, respectively.
For each $i = 1, 2, \ldots, N^{(1)}$, we enumerate the elements of $U^{(2)}_{i}$ as follows:
\begin{equation*}
U^{(2)}_{i}
\;=\; \left\{\;u_{i1},u_{i2},\ldots,u_{iN^{(2)}_{i}}\;\right\}
\;=\; \left\{\;u_{ik}\;\left\vert\; k = 1,2,\ldots,N^{(2)}_{i}\right.\;\right\}
\end{equation*}
Let
\begin{equation*}
U
\;:=\; \bigsqcup_{i \in U^{(1)}}U^{(2)}_{i}
\;=\;  \left\{\;u_{ik}\;\left\vert\; i=1,2,\ldots,N^{(1)},\,k = 1,2,\ldots,N^{(2)}_{i}\right.\;\right\}
\end{equation*}

\vskip 0.3cm
\begin{remark}\quad
We consider only the case where the two stages of sampling design are independent
of each other, and the sampling designs on $U^{(2)}_{k}$, for all $k \in U^{(1)}$, are
independent.
More precisely, we assume that Equation \eqref{twoStageSampleProbability} is satisfied.
\end{remark}

\vskip 0.3cm
\noindent
Let $p^{(1)}:\mathcal{S}^{(1)} \longrightarrow (0,1]$ be our chosen first-stage sampling design,
where $\mathcal{S}^{(1)} \subseteq \mathcal{P}\!\left(U^{(1)}\right)$
is the set of all possible first-stage samples in the design, and
$\mathcal{P}\!\left(U^{(1)}\right)$ is the power set of $U^{(1)}$.

\vskip 0.3cm
\noindent
For each $i \in U^{(1)}$, let $p^{(2)}_{i} : \mathcal{S}^{(2)}_{i} \longrightarrow (0,1]$ be our chosen second-stage sampling design,
where $\mathcal{S}^{(2)}_{i} \subseteq \mathcal{P}\!\left(U^{(2)}_{i}\right)$
is the set of all possible second-stage samples in the design, and
$\mathcal{P}\!\left(U^{(2)}_{i}\right)$ is the power set of $U^{(2)}_{i}$.

\vskip 0.3cm
\noindent
The sample space $\mathcal{S}$ of the two-stage sampling design is:
\begin{equation*}
\mathcal{S}
\;:=\;
\left\{\;
\left.\left(s^{(1)},\left\{\,s^{(2)}_{i}\,\right\}_{i\in U^{(1)}}\right) \in \mathcal{S}^{(1)} \times \prod_{i \in U^{(1)}}\mathcal{S}^{(2)}_{i}\;\right\vert\;
\begin{array}{ll}
s^{(2)}_{i} \in \mathcal{S}^{(2)}_{i}, &\textnormal{if $i \in s^{(1)}$}
\\
s^{(2)}_{i} = \varemptyset, & \textnormal{if $i \notin s^{(1)}$}
\end{array}
\;\right\}
\end{equation*}
We will use the following abbreviation for an element in $\mathcal{S}$:
\begin{equation*}
s \;=\; \left(\;s^{(1)}\;,\left\{\,s^{(2)}_{i}\,\right\}_{i \in s^{(i)}}\,\right)
\end{equation*}
We now define the probability function $p : \mathcal{S} \longrightarrow (0,1]$ as follows:
For each $s \in \mathcal{S}$,
\begin{equation}
\label{twoStageSampleProbability}
p(s)
\;:=\; p\left(\left(\;s^{(1)}\;,\left\{\,s^{(2)}_{i}\,\right\}_{i \in s^{(1)}}\,\right)\right)
\;=\; p^{(1)}\!\left(s^{(1)}\right)\cdot\prod_{i\in s^{(1)}}p^{(2)}_{i}\!\left(s^{(2)}_{i}\right)
\end{equation}

\begin{lemma}\quad
For each first-stage sample $s^{(1)} \in \mathcal{S}^{(1)}$,
let $\Omega\!\left(s^{(1)}\right)$ $:=$ $\left\{\,\left.s^{(2)}_{i}\in\mathcal{S}^{(2)}_{i}\;\right\vert\;i \in s^{(1)}\,\right\}$,
i.e. $\Omega\!\left(s^{(1)}\right)$ is the collection of all second-stage samples compatible with the first-stage sample
$s^{(1)} \in \mathcal{S}^{(1)}$.
Then, we have:
\begin{equation*}
\sum_{\xi\in\Omega(s^{(1)})}  p\left((\;s^{(1)},\xi\;)\right) \;\; = \;\; p^{(1)}\!\left(s^{(1)}\right).
\end{equation*}
\end{lemma}
\proof
Let $n$ be the number of elements in $s^{(1)}$, we write $s^{(1)} = \{\,i_{1},i_{2},\ldots,i_{n}\,\}$. Then,
\begin{equation*}
p\left(\left(\;s^{(1)}\;,\left\{\,s^{(2)}_{i_{1}},s^{(2)}_{i_{2}},\ldots,s^{(2)}_{i_{n}}\,\right\}\,\right)\right)
\;=\;
p^{(1)}\!\left(s^{(1)}\right)
\cdot p^{(2)}_{i_{1}}\!\left(s^{(2)}_{i_{1}}\right)
\cdot p^{(2)}_{i_{2}}\!\left(s^{(2)}_{i_{2}}\right)
\;\cdots\;
p^{(2)}_{i_{n}}\!\left(s^{(2)}_{i_{n}}\right)
\end{equation*}
Hence,
\begin{eqnarray*}
\sum_{\xi\in\Omega(s^{(1)})}  p\left((\;s^{(1)},\xi\;)\right)
&=&
\sum_{\zeta_{1} \in \mathcal{S}^{(2)}_{i_{1}}}\,
\sum_{\zeta_{2} \in \mathcal{S}^{(2)}_{i_{2}}}\,
\cdots
\sum_{\zeta_{n} \in \mathcal{S}^{(2)}_{i_{n}}}\,
p\left(\left(\;s^{(1)}\;,\left\{\,\zeta_{1},\zeta_{2},\ldots,\zeta_{n}\,\right\}\,\right)\right) \\
&=& 
\sum_{\zeta_{1} \in \mathcal{S}^{(2)}_{i_{1}}}\,
\sum_{\zeta_{2} \in \mathcal{S}^{(2)}_{i_{2}}}
\;\cdots\;
\sum_{\zeta_{n} \in \mathcal{S}^{(2)}_{i_{n}}}\,
p^{(1)}\!\left(s^{(1)}\right)
\cdot p^{(2)}_{i_{1}}\!\left(\zeta_{1}\right)
\cdot p^{(2)}_{i_{2}}\!\left(\zeta_{2}\right)
\;\cdots\;
p^{(2)}_{i_{n}}\!\left(\zeta_{n}\right)
\\
&=&
p^{(1)}\!\left(s^{(1)}\right)
\cdot\left(\sum_{\zeta_{1} \in \mathcal{S}^{(2)}_{i_{1}}}\,p^{(2)}_{i_{1}}\!\left(\zeta_{1}\right)\right)
\cdot\left(\sum_{\zeta_{2} \in \mathcal{S}^{(2)}_{i_{2}}}\,p^{(2)}_{i_{2}}\!\left(\zeta_{2}\right)\right)
\;\cdots\;
\left(\sum_{\zeta_{n} \in \mathcal{S}^{(2)}_{i_{n}}}\,p^{(2)}_{i_{n}}\!\left(\zeta_{n}\right)\right)
\\
&=&
p^{(1)}\!\left(s^{(1)}\right)
\cdot\left(\,1\,\right)
\cdot\left(\,1\,\right)
\;\cdots\;
\left(\,1\,\right)
\\
&=&
p^{(1)}\!\left(s^{(1)}\right)
\end{eqnarray*}
\qed

\begin{proposition}\label{proposition:probability:sampleSpace}
\quad
\begin{equation*}
\sum_{s\in\mathcal{S}}\,p(s) \;\;=\;\; 1
\end{equation*}
\end{proposition}
\proof
\begin{equation*}
\sum_{s \in \mathcal{S}}\,p(s)
\;=\; \sum_{(s^{(1)},\xi) \in \mathcal{S}}\,p(s^{(1)},\xi)
\;=\; \sum_{s^{(1)}\in \mathcal{S}^{(1)}}\,\sum_{\xi\in\Omega(s^{(1)})} \,p(s^{(1)},\xi)
\;=\; \sum_{s^{(1)}\in \mathcal{S}^{(1)}}\,p^{(1)}(s^{(1)})
\;=\; 1,
\end{equation*}
where the second-last equality follows from the preceding Lemma.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Estimation in two-stage sampling}
\setcounter{theorem}{0}

Let $\mathbf{y} : U \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued function defined on $U$
(such a $\mathbf{y}$ commonly called a ``population parameter").
We will use the common notation $\mathbf{y}_{kl}$ for $\mathbf{y}(u_{kl})$,
for $k = 1,2,\ldots,N^{(1)}$ and $l = 1,2,\ldots,N^{(2)}_{k}$.
We wish to estimate
\begin{equation*}
\mathbf{T}_{\mathbf{y}}
\;:=\; \underset{u \in U}{\sum}\,\mathbf{y}(u)
\;=\; \sum_{k \in U^{(1)}}\,\sum_{l \in U^{(2)}_{k}}\,\mathbf{y}_{kl}
\;=\; \sum_{k = 1}^{N^{(1)}}\,\sum_{l = 1}^{N^{(2)}_{k}}\,\mathbf{y}_{kl}
\;\in\; \Re^{m}
\end{equation*}
via two-stage sampling.
We consider estimators for $\widehat{\mathbf{T}}_{\mathbf{y}}$ of the following form:
\begin{equation*}
\begin{array}{cccl}
\widehat{\mathbf{T}}_{\mathbf{y}} : & \mathcal{S} & \longrightarrow & \Re^{m} \\
& \left(s^{(1)},\{s^{(2)}_{k}\}_{k \in s^{(1)}}\right) & \longmapsto &
\underset{k\in s^{(1)}}{\sum}\,
w^{(1)}_{k}(s^{(1)})\,
\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\;=\; \underset{k\in U^{(1)}}{\sum}\,I_{k}(s^{(1)})\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k}),
\end{array}
\end{equation*}
where, for each $k \in U^{(1)}$, $w^{(1)}_{k} : \mathcal{S}^{(1)} \longrightarrow \Re$ and
$\widehat{\mathbf{T}}_{\mathbf{y}\vert k} : \mathcal{S}^{(2)}_{k} \longrightarrow \Re^{m}$ are random variables.

\begin{proposition}
\quad
Suppose:
\begin{itemize}
\item	The first-stage weights $w^{(1)}_{k} : \mathcal{S}^{(1)} \longrightarrow (0,1]$
		satisfy the following:
		\begin{equation*}
		E^{(1)}\!\left[\;\,\widehat{T}_{z}\,\;\right] \;\; = \;\; T_{z} \;\;:=\;\; \sum_{k\in U^{(1)}}z_{k},
		\quad\textnormal{for any function $z : U^{(1)} \longrightarrow \Re$,}
		\end{equation*}
		where $\widehat{T}_{z} : \mathcal{S}^{(1)} \longrightarrow \Re$ is a random variable defined by
		$\widehat{T}_{z}(s^{(1)}) := \underset{k \in s^{(1)}}{\sum}\,w^{(1)}_{k}(s^{(1)})\,z_{k}$.
\item	For each $k \in U^{(1)}$, the random variable
		$\widehat{\mathbf{T}}_{\mathbf{y}\vert k} : \mathcal{S}^{(2)}_{k} : \longrightarrow \Re$
		is a design-unbiased
		estimator for $\mathbf{T}_{\mathbf{y}\vert k}$, i.e.
		\begin{equation*}
		E^{(2)}_{k}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y}\,\vert\,k}\;\right]
		\;\;=\;\; \mathbf{T}_{\mathbf{y}\,\vert k}
		\;\;:=\;\; \sum_{l \in U^{(2)}_{k}}\,\mathbf{y}_{kl}
		\end{equation*}
\end{itemize}
Then, the random variable $\widehat{\mathbf{T}}_{\mathbf{y}}$ is an unbiased estimator for $\mathbf{T}_{\mathbf{y}}$.
\end{proposition}

\proof
\begin{eqnarray*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y}}\;\right]
&=& E^{(1)}\!\left[\;E^{(2)}\!\left[\;\left.\widehat{\mathbf{T}}_{\mathbf{y}}\;\right\vert\;s^{(1)}\;\right]\;\right]
\;\;=\;\; E^{(1)}\!\left[\;E^{(2)}\!\left[\;\left.
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\;\right\vert\;s^{(1)}\;\right]\;\right] \\
&=& E^{(1)}\!\left[\;
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})
\cdot
E^{(2)}\!\left[\;\left.
\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\;\right\vert\;s^{(1)}\;\right]\;\right] \\
&=& E^{(1)}\!\left[\;
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})
\cdot
E^{(2)}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})\;\right]
\;\right] \\
&=& E^{(1)}\!\left[\;
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})
\cdot
\mathbf{T}_{\mathbf{y}\vert k}
\;\right] \\
&=&
\sum_{k\in U^{(1)}}\,\mathbf{T}_{\mathbf{y}\vert k}
\;\;=\;\;
\sum_{k\in U^{(1)}}\,\sum_{l \in U^{(2)}_{k}}\,\mathbf{y}_{kl}
\\
&=&
\mathbf{T}_{\mathbf{y}}
\end{eqnarray*}
\qed

\begin{proposition}
\quad
Suppose that
the first-stage weights $w^{(1)}_{k} : \mathcal{S}^{(1)} \longrightarrow (0,1]$
satisfy the following:
\begin{equation*}
E^{(1)}\!\left[\;\,\widehat{T}_{z}\,\;\right] \;\; = \;\; T_{z} \;\;:=\;\; \sum_{k\in U^{(1)}}z_{k},
\quad\textnormal{for any function $z : U^{(1)} \longrightarrow \Re$,}
\end{equation*}
where $\widehat{T}_{z} : \mathcal{S}^{(1)} \longrightarrow \Re$ is a random variable defined by
$\widehat{T}_{z}(s^{(1)}) := \underset{k \in s^{(1)}}{\sum}\,w^{(1)}_{k}(s^{(1)})\,z_{k}$.
Then, the variance $\Var\!\left(\widehat{\mathbf{T}}_{\mathbf{y}}\right)$ can be expressed as follows:
		\begin{equation*}
		\Var\!\left(\widehat{\mathbf{T}}_{\mathbf{y}}\right)
		\;\;=\;\;
		E^{(2)}\!\left[\;\Var^{(1)}\!\left(\left.
		\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
		\right\vert s^{(2)}\right)\;\right]
		\;\;+\;\;
		\sum_{k\in U^{(1)}}\Var^{(2)}_{k}\!\left[\;\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}\;\right]
		\end{equation*}
\end{proposition}

\proof
\begin{eqnarray*}
\Var\!\left(\widehat{\mathbf{T}}_{\mathbf{y}}\right)
&=&
E^{(2)}\!\left[\;\Var^{(1)}\!\left(\left.\widehat{\mathbf{T}}_{\mathbf{y}}\right\vert s^{(2)}\right)\;\right]
+
\Var^{(2)}\!\left[\;E^{(1)}\!\left(\left.\widehat{\mathbf{T}}_{\mathbf{y}}\right\vert s^{(2)}\right)\;\right] \\
&=&
E^{(2)}\!\left[\;\Var^{(1)}\!\left(\left.
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\right\vert s^{(2)}\right)\;\right]
+
\Var^{(2)}\!\left[\;E^{(1)}\!\left(\left.
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\right\vert s^{(2)}\right)\;\right]
\\
&=&
E^{(2)}\!\left[\;\Var^{(1)}\!\left(\left.
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\right\vert s^{(2)}\right)\;\right]
+
\Var^{(2)}\!\left[\;
\sum_{k\in U^{(1)}}\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\;\right]
\\
&=&
E^{(2)}\!\left[\;\Var^{(1)}\!\left(\left.
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\right\vert s^{(2)}\right)\;\right]
+
\sum_{k\in U^{(1)}}
\Var^{(2)}\!\left[\;
\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\;\right]
\\
&=&
E^{(2)}\!\left[\;\Var^{(1)}\!\left(\left.
\sum_{k\in s^{(1)}}\,w^{(1)}_{k}(s^{(1)})\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}(s^{(2)}_{k})
\right\vert s^{(2)}\right)\;\right]
+
\sum_{k\in U^{(1)}}\Var^{(2)}_{k}\!\left[\;\,\widehat{\mathbf{T}}_{\mathbf{y}\vert k}\;\right]
\end{eqnarray*}
\qed

\begin{definition}
\mbox{}
\vskip 0.1cm
\noindent
A random variable $\widehat{\mathbf{T}}_{\mathbf{y}} : \mathcal{S} \longrightarrow \Re^{m}$
is said to be \underline{\emph{linear} in the population parameter $\mathbf{y} : U \longrightarrow \Re$}
if it has the following form:
\begin{equation*}
\begin{array}{cccl}
\widehat{\mathbf{T}}_{\mathbf{y}} : & \mathcal{S} & \longrightarrow & \Re^{m} \\
     & s &\longmapsto & \underset{k\in s}{\sum}\,w_{k}(s)\,\mathbf{y}_{k} \;=\; \underset{k\in U}{\sum}\,I_{k}(s)\,w_{k}(s)\,\mathbf{y}_{k},
\end{array}
\end{equation*}
where, for each $k \in U$, $w_{k} : \mathcal{S} \longrightarrow \Re$ is itself
an $\Re$-valued random variable, and $I_{k} : \mathcal{S} \longrightarrow \{0,1\}$
is the indicator random variable defined by:
\begin{equation*}
I_{k}(s)
\;=\;
\left\{
\begin{array}{cl}
1, & \textnormal{if $k \in s$}, \\
0, & \textnormal{otherwise}
\end{array}
\right.
\end{equation*}
We call the $w_{k}$'s the \underline{\emph{weights}} of $\widehat{\mathbf{T}}_{\mathbf{y}}$,
and we use the notation $\widehat{\mathbf{T}}_{\mathbf{y};w}$ to indicate that the random
variable depends on the weights $w_{k}$.
\end{definition}

\noindent
\textbf{Nomenclature}
\;
In the context of finite-population probability sampling, under a design
$p : \mathcal{S} \longrightarrow (0,1]$,
an ``estimator" is precisely just a random variable defined on the space
$\mathcal{S}$ of all admissible samples in the design.
%When we speak of a random variable $\widehat{\theta} : \mathcal{S} \longrightarrow \Re$
%as an estimator for a certain population characteristic $\theta \in \Re$, we are merely
%conveying the connotation that $\widehat{\theta}$ is supposed to have an expected
%value equal or close to $\theta$ and that its variance is supposed to be sufficiently small
%that we have some degree of confidence that an actual observed value of $\widehat{\theta}$,
%i.e. $\widehat{\theta}(s)$ where $s \in \mathcal{S}$ is the actual obtained sample, should be
%reasonably close to the actual value of $\theta$.

\begin{proposition}
\label{proposition:Unbiasedness}
\mbox{}
\vskip 0.2cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re^{m}$, with
$\widehat{\mathbf{T}}_{\mathbf{y};w}(s)$
$=$ $\underset{k \in U}{\sum}\,I_{k}(s)\,w_{k}(s)\,\mathbf{y}_{k}$
$=$ $\underset{k \in s}{\sum}\,w_{k}(s)\,\mathbf{y}_{k}$,
be a random variable linear in the population parameter
$\mathbf{y} : U \longrightarrow \Re$.
Then,
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right] \; = \; \mathbf{T}_{\mathbf{y}},\;\textnormal{for arbitrary $\mathbf{y}$}
\quad\Longleftrightarrow\quad
E\!\left[\;I_{k}\,w_{k}\;\right] \; = \; 1,\;\textnormal{for each $k \in U$}.
\end{equation*}
\end{proposition}
\proof
Note:
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;=\; E\!\left[\;\underset{k \in s}{\sum}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; E\!\left[\;\underset{k \in U}{\sum}\,I_{k}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; \underset{k \in U}{\sum}\,E\!\left[\,I_{k}\,w_{k}\,\right]\,\mathbf{y}_{k}
\end{equation*}
Hence, since $\mathbf{y} : U \longrightarrow \Re$ is arbitrary,
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right] \;=\; \mathbf{T}_{\mathbf{y}} \; := \; \underset{k \in U}{\sum}\,\mathbf{y}_{k}
\quad\Longleftrightarrow\quad
\sum_{k \in U}\left(E\!\left[\,I_{k}\,w_{k}\,\right]-1\right)\cdot\mathbf{y}_{k} \; = \; \mathbf{0}
\quad\Longleftrightarrow\quad
E\!\left[\;I_{k}\,w_{k}\;\right] \; = \; 1,\;\textnormal{for each $k \in U$}.
\end{equation*}
The proof of the Proposition is now complete. \qed

\begin{corollary}
\mbox{}
\vskip 0.2cm
\noindent
Let $U = \{1,2,\ldots,N\}$ be a finite population.
For any fixed but arbitrary population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$ and
for any sampling design $p : \mathcal{S} \longrightarrow (0,1]$ such that each of
its first-order inclusion probabilities is strictly positive,
the Horvitz-Thompson estimator $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$ is well-defined
and it is the unique unbiased estimator for $\mathbf{T}_{\mathbf{y}}$, which is linear in $\mathbf{y}$ and
whose weights are constant in $s$.
\end{corollary}

\proof
Recall that the Horvitz-Thompson estimator is defined as:
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}(s)
\;:=\; \sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\mathbf{y}_{k}
\;:=\; \sum_{k \in U}\,I_{k}(s)\,\dfrac{1}{\pi_{k}}\,\mathbf{y}_{k},
\end{equation*}
where $\pi_{k} := E\!\left[\,I_{k}\,\right] = \underset{k \in U}{\sum}\,p(s)\,I_{k}(s) = \underset{s \ni k}{\sum}\,p(s)$
is the inclusion probability of $k \in U$ under the sampling design $p : \mathcal{S} \longrightarrow (0,1]$.
Clearly, $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$ is linear in $\mathbf{y}$ with weights constant in $s$.
Next, note that:
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\mathbf{y}_{k}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in U}\,I_{k}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}\;\right]
\;\;=\;\; \sum_{k \in U}\,E\!\left[\;I_{k}\;\right]\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\;\;=\;\; \sum_{k \in U}\,\pi_{k}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\;\;=\;\; \sum_{k \in U}\,\mathbf{y}_{k} \;\; = \;\; \mathbf{T}_{y}
\end{equation*}
Hence, $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$ is an unbiased estimator for $\mathbf{T}_{\mathbf{y}}$.
Conversely, let
\begin{equation*}
\widehat{\mathbf{T}}_{y;w}(s) \; = \; \sum_{k \in s}\,w_{k}\,\mathbf{y}_{k}
\end{equation*}
be any unbiased estimator for $\mathbf{T}_{\mathbf{y}}$ which linear in $\mathbf{y}$ with weights $w_{k}$ constant in $s$.
Thus,
\begin{equation*}
\sum_{k \in U}\,\mathbf{y}_{k}
\;=\; \mathbf{T}_{\mathbf{y}}
\;=\; E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;=\; E\!\left[\;\sum_{k \in s}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; E\!\left[\;\sum_{k \in U}\,I_{k}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; \sum_{k \in U}\,E\!\left[\;I_{k}\;\right]\,w_{k}\,\mathbf{y}_{k}
\;=\; \sum_{k \in U}\,\pi_{k}\,w_{k}\,\mathbf{y}_{k}.
\end{equation*}
Since $\mathbf{y}$ is arbitrary, the above equation immediately implies that
\begin{equation*}
\pi_{k}w_{k} - 1 = 0,
\end{equation*}
or equivalently, $w_{k} = \dfrac{1}{\pi_{k}}$; in other words,
$\widehat{\mathbf{T}}_{\mathbf{y};w}$ is in fact equal to the Horvitz-Thompson estimator.
The proof of the Corollary is now complete.
\qed

\begin{lemma}
\label{lemma:technical}
\mbox{}
\vskip 0.1cm
\noindent
Let $(\Omega,\mathcal{A},p)$ be a probability space,
$X, Y : \Omega \longrightarrow \Re$ be two $\Re$-valued random variables defined on $\Omega$,
and $\mathbf{u}, \mathbf{v} \in \Re^{m}$ be two fixed vectors in $\Re^{m}$.
Then,
\begin{equation*}
\Cov\!\left(\,X\cdot\mathbf{u}\,,\,Y\cdot\mathbf{v}\,\right)
\;\;=\;\;
\Cov(X,Y)\cdot\mathbf{u}\cdot\mathbf{v}^{T}
\;\;\in\;\; \Re^{m \times m}
\end{equation*}
\end{lemma}

\proof
Note:
\begin{eqnarray*}
\Cov\!\left(\;X\cdot\mathbf{u}\;,\;Y\cdot\mathbf{v}\;\right)
&:=& E\!\left[\;\left(X\,\mathbf{u} - \mu_{X}\mathbf{u}\right)\cdot\left(Y\,\mathbf{v} - \mu_{Y}\mathbf{v}\right)^{T}\;\right]
\;\;=\;\; E\!\left[\;\left(X - \mu_{X}\right)\mathbf{u}\cdot\left(Y - \mu_{Y}\right)\mathbf{v}^{T}\;\right] \\
&=& E\!\left[\;\left(X - \mu_{X}\right)\left(Y - \mu_{Y}\right)\cdot\mathbf{u}\cdot\mathbf{v}^{T}\;\right] 
\;\;=\;\; E\!\left[\;\left(X - \mu_{X}\right)\left(Y - \mu_{Y}\right)\;\right]\cdot\mathbf{u}\cdot\mathbf{v}^{T} \\
&=& \Cov(X,Y)\cdot\mathbf{u}\cdot\mathbf{v}^{T},
\end{eqnarray*}
as required.
\qed

\begin{proposition}
\label{proposition:generalLinear:Var}
\mbox{}
\vskip 0.2cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re$,
with $\widehat{\mathbf{T}}_{\mathbf{y};w}(s)$
$=$ $\underset{k \in s}{\sum}\,w_{k}(s)\,\mathbf{y}_{k}$
$=$ $\underset{k \in U}{\sum}\,I_{k}(s)\,w_{k}(s)\,\mathbf{y}_{k}$,
be a random variable linear in the population parameter $\mathbf{y} : U \longrightarrow \Re$.
Then, the covariance matrix of $\widehat{\mathbf{T}}_{\mathbf{y};w}$ is given by:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;\;=\;\;\sum_{i\in U}\,\sum_{k\in U}\,\Cov\!\left[\;I_{i}\,w_{i}\,,\,I_{k}\,w_{k}\;\right]\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\; \in \Re^{m \times m}
\end{equation*}
%where
%\begin{equation*}
%a_{i}\; := \;\Var\left[\;I_{i}\,w_{i}\;\right],
%\quad
%a_{ij} \; := \; \textnormal{Cov}\left[\;I_{i}w_{i}\,,\,I_{j}w_{j}\;\right].
%\quad\textnormal{and}\quad
%I_{i}(s)
%\;=\;
%\left\{
%\begin{array}{cl}
%1\,, & \textnormal{if $i \in s$} \\
%0\,, & \textnormal{otherwise}
%\end{array}.
%\right.
%\end{equation*}
%Furthermore, if $\widehat{T}_{y;w}$ is an unbiased estimator for $T_{y} := \underset{k \in U}{\sum}y_{k}$
%for arbitrary $y$, then
%\begin{equation*}
%a_{i} \; = \; \Var\!\left[\;I_{i}w_{i}\;\right]
%\; = \; \left(\sum_{s\in\mathcal{S}}\,p(s)\,I_{i}(s)\,w_{i}^{2}(s)\right) - 1
%\end{equation*}
Furthermore, if the first-order and second-order inclusion probabilities of the sampling design
$p : \mathcal{S} \longrightarrow (0,1]$ are all strictly positive,
i.e. $\pi_{k} = \pi_{kk} := \underset{s \ni k}{\sum}\,p(s) > 0$, for each $k \in U$, and
$\pi_{ik} := \underset{s \ni i,k}{\sum}\,p(s) > 0$, for any distinct $i,k \in U$,
then an unbiased estimator for $\Var\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]$ is given by:
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]\!(s)
\;:=\; \sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;=\; \sum_{k \in s}\dfrac{\Var(I_{k}w_{k})}{\pi_{k}}\,\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}
       + \underset{i \neq k}{\sum_{i,k \in s}}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T},
\;\;\textnormal{for each $s \in \mathcal{S}$}.
\end{equation*}
\end{proposition}

\proof
First, note that Lemma \ref{lemma:technical} implies:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;\;=\;\; \Cov\!\left[\;\sum_{i\in U}\,I_{i}\,w_{i}\,\mathbf{y}_{i}\;,\;\sum_{k\in U}\,I_{k}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;\;=\;\; \sum_{i\in U}\,\sum_{k\in U}\,\Cov\!\left[\;I_{i}\,w_{i}\,,\,I_{k}\,w_{k}\;\right]\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;\in\;\;\Re^{m \times m}
\end{equation*}
Next,
\begin{eqnarray*}
E\!\left(\;\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]\;\right)
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]\!(s)
\;\;=\;\; \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(\sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}\right) \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot
\left(\sum_{i,k \in U}I_{i}(s)I_{k}(s)\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}\right) \\
&=& \sum_{i,k \in U}\left(\sum_{s\in\mathcal{S}}\,p(s)I_{i}(s)I_{k}(s)\right)\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{i,k \in U}\left(\sum_{s\ni i, k}\,p(s)\right)\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{i,k \in U}\,\pi_{ik}\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{i,k \in U}\,\Cov(I_{i}w_{i},I_{k}w_{k})\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \Var\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]
\end{eqnarray*}
Lastly, recall that $\pi_{kk} = \pi_{k}$ and $\Cov(I_{k}w_{k},I_{k}w_{k}) = \Var[\,I_{k}w_{k}\,]$,
and the validity of the following identity is thus trivial:
\begin{equation*}
\sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{k \in s}\dfrac{\Var(I_{k}w_{k})}{\pi_{k}}\cdot\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}
       + \underset{i \neq k}{\sum_{i,k \in s}}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\end{equation*}
The proof of the Proposition is complete.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Calibrated linear estimators for (multivariate) population totals}
\setcounter{theorem}{0}

\begin{definition}
\mbox{}
\vskip 0.1cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re^{m}$
be an $\Re^{m}$-valued random variable which is linear in the $\Re^{m}$-valued
population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$, i.e.
\begin{equation*}
\begin{array}{cccl}
\widehat{\mathbf{T}}_{\mathbf{y};w} : & \mathcal{S} & \longrightarrow & \Re^{m} \\
     & s &\longmapsto & \underset{k\in s}{\sum}\,w_{k}(s)\cdot\mathbf{y}_{k} \;\; = \;\; \underset{k\in U}{\sum}\,I_{k}(s)\,w_{k}(s)\cdot\mathbf{y}_{k},
\end{array}
\end{equation*}
where, for each $k \in U$, $w_{k} : \mathcal{S} \longrightarrow \Re$ is itself an $\Re$-valued random variable,
and $I_{k} : \mathcal{S} \longrightarrow \{0,1\}$ is the indicator random variable defined by:
\begin{equation*}
I_{k}(s)
\;\;=\;\;
\left\{
\begin{array}{cl}
1, & \textnormal{if $k \in s$}, \\
0, & \textnormal{otherwise}
\end{array}
\right.
\end{equation*}
Let $x : U \longrightarrow \Re$ be an $\Re$-valued population parameter
and  $T_{x} := \underset{k\in U}{\sum}\,x_{k}$.\\
Then, $\widehat{\mathbf{T}}_{\mathbf{y};w}$ is said to be
\underline{\emph{calibrated with respect to $x$}} if
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k} \; = \; T_{x},
\;\;\textnormal{for {\color{red}each} $s \in \mathcal{S}$}.
\end{equation*}
\end{definition}

\begin{example}
\mbox{}
\vskip 0.1cm
\noindent
If the sampling design has fixed sample size and each of its first-order inclusion probabilities is strictly positive, then Horvitz-Thompson estimator is calibrated with respect to the first-order inclusion probabilities.
\vskip 0.2cm
\noindent
To see this, let $U = \{1,2,\ldots,N\}$ be a finite population,
$\mathbf{y} : U \longrightarrow \Re^{m}$ a population parameter, and
$p : \mathcal{S}\subset\mathcal{P}(U) \longrightarrow (0,1]$ a sampling design
such that $\pi_{k} := \sum_{s \ni k}p(s) > 0$, for each $k \in U$.
The Horvitz-Thompson estimator
$\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}} : \mathcal{S} \longrightarrow \Re$
is then well-defined and is given by:
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\!(s)
\;\; := \;\; \sum_{k \in s}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\end{equation*}
Let $x : U \longrightarrow \Re$ be defined by
\begin{equation*}
x_{k} \; = \; \pi_{k},
\;\;\textnormal{for each $k \in U$},
\end{equation*}
i.e. $x_{k}$ is simply the inclusion probability of $k \in U$
under the sampling design $p : \mathcal{S} \longrightarrow (0,1]$.

Now, suppose that the sampling design has a fixed sample size $n$,
and we shall show that $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$
is consequently calibrated with respect to $x : U \longrightarrow \Re$.
Indeed, recall that the weights of the Horvitz-Thompson estimator are simply
$w_{k}(s) = 1 / \pi_{k}$, for each $k \in U$ and each $s \in \mathcal{S}$.
Hence,
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k}
\;\;=\;\; \sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\pi_{k}
\;\;=\;\; \sum_{k \in s}\,1
\;\;=\;\; \left(\begin{array}{c} \textnormal{sample} \\ \textnormal{size of $s$} \end{array}\right)
\;\;=\;\; n,
\end{equation*}
since the sampling design has fixed size $n$.
On the other hand,
\begin{equation*}
T_{x}
\;\;=\;\;\sum_{k\in U}x_{k}
\;\;=\;\; \sum_{k \in U}\pi_{k}
\;\;=\;\; \sum_{k \in U}E\!\left[\;I_{k}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in U}I_{k}\;\right]
\;\;=\;\; E\!\left[\begin{array}{c} \textnormal{sample} \\ \textnormal{size} \end{array}\right]
\;\;=\;\; n,
\end{equation*}
again since the sample size is fixed and equals $n$.
Therefore, we have, for any $s \in \mathcal{S}$,
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k}
\;\;=\;\; n
\;\;=\;\; T_{x}
\end{equation*}
Therefore, the Horvitz-Thompson estimator, under the assumption of fixed sample size,
is indeed calibrated with respect to the inclusion probabilities $x : U \longrightarrow \Re$,
$x _{k} = \pi_{k} := \sum_{s \ni k}\,p(s)$, for each $k \in U$. \qed
\end{example}

\begin{proposition}
\label{proposition:calibratedLinear:MSE}
\mbox{}
\vskip 0.1cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w,x} : \mathcal{S} \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued random variable 
which is linear in the $\Re^{m}$-valued population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$
and calibrated with respect to the population parameter $x : U \longrightarrow \Re$, with $x_{k} \neq 0$ for each $k \in U$.\\
Then, the mean squared error matrix of $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ as an estimator of $\mathbf{T}_{\mathbf{y}}$ is given by:
\begin{equation*}
\MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
\;=\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\;\in\;\Re^{m \times m},
\;\;\textnormal{where}\;\;
a_{ik} \;:=\; E\!\left[\,\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\,\right].
\end{equation*}
\end{proposition}

\proof
\begin{eqnarray*}
\MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
&=& E\!\left[\;\left(\widehat{\mathbf{T}}_{\mathbf{y};w,x} - \mathbf{T}_{\mathbf{y}}\right)\cdot\left(\widehat{\mathbf{T}}_{\mathbf{y};w,x} - \mathbf{T}_{\mathbf{y}}\right)^{T}\;\right]
\;\;=\;\; E\!\left[\;\left(\sum_{i\in U}(I_{i}w_{i}-1)\,\mathbf{y}_{i}\right)\cdot\left(\sum_{k\in U}(I_{k}w_{k}-1)\,\mathbf{y}_{k}\right)^{T}\;\right] \\
&=& \sum_{i\in U}\,\sum_{k\in U}\,E\!\left[\;\left(I_{i}w_{i}-1\right)\left(I_{k}w_{k}-1\right)\;\right]\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{k \in U}\,a_{kk}\cdot\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T} + \underset{i\neq k}{\sum_{i,k\in U}}\,a_{ik}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{k \in U}\,a_{kk}\left(\dfrac{\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}}{x_{k}^{2}}\right)x_{k}^{2}
+ \underset{i\neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\,x_{i}\,x_{k}
\end{eqnarray*}
On the other hand,
\begin{eqnarray*}
&&   - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\,x_{i}\,x_{k} \\
&=& - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left[\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
- \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
- \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\right]\,x_{i}\,x_{k} \\
&=& - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left[
    \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
+ \underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\,x_{i}\,x_{k}
\end{eqnarray*}
Thus, the proof of the present Proposition will be complete once we show:
\begin{equation*}
\underset{
\dfrac{1}{2}\,\underset{i {\color{red}=} k}{\underset{i,k\in U}{\textnormal{\large$\sum$}}}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
}{
\underbrace{\sum_{k \in U}\,a_{kk}\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}x_{k}^{2}}
}
\;\;=\;\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k},
\end{equation*}
which is equivalent to:
\begin{equation}\label{MSE}
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
\;\;=\;\; 0.
\end{equation}
Observe that
\begin{eqnarray*}
\textnormal{LHS\eqref{MSE}}
&=&
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}x_{i}\,x_{k}
+
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}x_{i}\,x_{k} \\
&=&
2\,\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}x_{i}\,x_{k}
\;\;=\;\; 2\,\sum_{i\in U}\,x_{i}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}\left(\sum_{k\in U}\,a_{ik}x_{k}\right).
\end{eqnarray*}
Hence, \eqref{MSE} follows once we show
\begin{equation}\label{MSE2}
\sum_{k\in U}\,a_{ik}x_{k} = 0,
\quad\textnormal{for each $i \in U$.}
\end{equation}
Lastly, we now claim that \eqref{MSE2} follows from the hypothesis that
$\widehat{T}_{y;w;x}$ is calibrated with respect to $x$.
Indeed,
\begin{eqnarray*}
\sum_{k \in U}\,a_{ik}x_{k}
&=& \sum_{k\in U}\,E\!\left[\;(I_{i}w_{i}-1)(I_{k}w_{k}-1)\;\right] x_{k}
\;\;=\;\; \sum_{k\in U}\,\left[\;\sum_{s\in\mathcal{S}}p(s)(I_{i}(s)w_{i}(s)-1)(I_{k}(s)w_{k}(s)-1)\;\right] x_{k} \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(I_{i}(s)w_{i}(s)-1\right)\cdot\left[\;\sum_{k\in U}\left(I_{k}(s)w_{k}(s)-1\right)\cdot x_{k}\;\right] \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(I_{i}(s)w_{i}(s)-1\right)\cdot\underset{0}{\left[\underbrace{\;\left(\sum_{k\in s}w_{k}(s)\,x_{k}\right) - T_{x}}\;\right]}\\
&=& 0
\end{eqnarray*}
The proof of the present Proposition is now complete. \qed

\begin{proposition}[The Yates-Grundy-Sen Variance Estimator for calibrated linear population total estimators]
\mbox{}
\vskip 0.1cm
\noindent
Let $p : \mathcal{S} \longrightarrow (0,1]$ be a sampling design each of whose
first-order and second-order inclusion probabilities is strictly positively.
Let $\widehat{\mathbf{T}}_{\mathbf{y};w,x} : \mathcal{S} \longrightarrow \Re^{m}$
be a random variable  which is linear in the population parameter
$\mathbf{y} : U \longrightarrow \Re^{m}$
and calibrated with respect to the population parameter $x : U \longrightarrow \Re$,
with $x_{k} \neq 0$ for each $k \in U$.
Suppose that $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ is an unbiased estimator for
$\mathbf{T}_{\mathbf{y}} := \underset{k \in U}{\sum}\mathbf{y}_{k}$, for arbitrary $\mathbf{y}$.
Then, the following is an unbiased estimator of the variance
$\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]$ of $\widehat{\mathbf{T}}_{\mathbf{y};w,x} $:
For each $s \in \mathcal{S}$ admissible in the sampling design $p : \mathcal{S} \longrightarrow (0,1]$,
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]\!(s)
\;\;:=\;\;
-\dfrac{1}{2}\underset{i\neq k}{\sum_{i,k\in s}}\left(w_{i}(s)w_{k}(s) - \dfrac{1}{\pi_{ik}}\right)
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\end{equation*}
\end{proposition}

\noindent
\textbf{Terminology:}\quad
$\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]$ is called the Yates-Grundy-Sen Variance Estimator.

\vskip 0.5cm
\proof
Since $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ is an unbiased estimator for $\mathbf{T}_{\mathbf{y}}$ by hypothesis, we have
$\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right] = \MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]$.
By Proposition \ref{proposition:calibratedLinear:MSE}, we thus have:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
\;\;=\;\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{2}\,x_{i}\,x_{k},
\quad\textnormal{where}\;\;
a_{ik} \;:=\; E\!\left[\;\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\;\right].
\end{equation*}
On the other hand,
\begin{eqnarray*}
E\!\left(\;\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]\;\right)
&=& 
-\dfrac{1}{2}\underset{i \neq k}{\sum_{i,k\in U}}
E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right]
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\end{eqnarray*}
Thus, it remains only to show:
\begin{equation*}
a_{ik} \;=\; E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right].
\end{equation*}
Now,
\begin{equation*}
E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right]
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - \dfrac{1}{\pi_{ik}}E\!\left[\;I_{i}I_{k}\;\right]
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - \dfrac{1}{\pi_{ik}}\,\pi_{ik}
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - 1,
\end{equation*}
and
\begin{eqnarray*}
a_{ik}
&=& E\!\left[\;\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\;\right]
\;\;=\;\; E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - E\!\left[\;I_{i}\,w_{i}\;\right] - E\!\left[\;I_{k}\,w_{k}\;\right] + 1 \\
&=&E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - 1 - 1 + 1
\;\;=\;\; E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - 1 \\
&=& E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right],
\end{eqnarray*}
where third last equality follows from Proposition \ref{proposition:Unbiasedness} and
the unbiasedness hypothesis on $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ as an estimator for $\mathbf{T}_{\mathbf{y}}$.
The proof of the present Proposition is now complete.  \qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Unbiased variance estimators for the Horvitz-Thompson Estimator}
\setcounter{theorem}{0}

Let $U = \{1,2,\ldots,N\}$ be a finite population.
Let $\mathbf{y} = (y_{1},y_{2},\ldots,y_{m}) : U \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued function defined on $U$
(commonly called a ``population parameter").
We will use the common notation $\mathbf{y}_{k}$ for $\mathbf{y}(k)$.
We wish to estimate
$\mathbf{T}_{\mathbf{y}} := \underset{k \in U}{\sum}\,\mathbf{y}_{k} \in \Re^{m}$
via survey sampling.
Let $p:\mathcal{S} \longrightarrow (0,1]$ be our chosen sampling design,
where $\mathcal{S} \subseteq \mathcal{P}(U)$ is the set of all possible
samples in the design, and $\mathcal{P}(U)$ is the power set of $U$.

\begin{proposition}
\mbox{}
\vskip 0.1cm
\noindent
Suppose the first-order and second-order inclusion probabilities of $p:\mathcal{S}\longrightarrow(0,1]$
are all strictly positive, i.e.
\begin{equation*}
\pi_{k} := \sum_{s \ni k}\,p(s) = \sum_{k \in U}\,I_{k}(s)p(s) > 0
\quad\textnormal{and}\quad
\pi_{ik} := \sum_{s \ni i,k}\,p(s) = \sum_{i,k \in U}\,I_{i}(s)I_{k}(s)p(s) > 0,
\end{equation*}
for any $i,k \in U$.
Then, the Horvitz-Thompson estimator for $\mathbf{T}_{\mathbf{y}}$ is:
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}(s)
\;\;:=\;\; \sum_{k \in s} \dfrac{1}{\pi_{k}}\,\mathbf{y}_{k},
\end{equation*}
and the covariance matrix of the Horvitz-Thompson estimator can be given by:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]
\;\;=\;\; \sum_{i,k \in U}\left(\pi_{ik} - \pi_{i}\pi_{k}\right)\cdot
\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T}
\end{equation*}
An unbiased estimator for the covariance matrix of the Horvitz-Thompson estimator is given by:
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]\!(s)
\;\;=\;\;
\sum_{i,k \in s}\left(\dfrac{\pi_{ik} - \pi_{i}\pi_{k}}{\pi_{ik}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T},
\;\;\textnormal{for each $s \in \mathcal{S}$}.
\end{equation*}
Furthermore, if the sampling design has fixed sample size, then an alternative expression
of the covariance matrix of the Horvitz-Thompson estimator is:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]
\;\;=\;\; -\dfrac{1}{2}\sum_{i,k \in U}\left(\pi_{ik} - \pi_{i}\pi_{k}\right)\cdot
\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}} - \dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)\cdot
\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}} - \dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T}
\end{equation*}
and the corresponding Yates-Grundy-Sen variance estimator is:
\begin{equation*}
\widehat{\Var}^{\textnormal{YGS}}\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]\!(s)
\;\;:=\;\; -\dfrac{1}{2}\sum_{i,k \in s}\left(\dfrac{\pi_{ik} - \pi_{i}\pi_{k}}{\pi_{ik}}\right)\cdot
\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}} - \dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)\cdot
\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}} - \dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T}
\end{equation*}
\end{proposition}

\proof
By Proposition \ref{proposition:generalLinear:Var},
for any random variable (a.k.a. estimator) $\widehat{\mathbf{T}}_{\mathbf{y};w}$
linear in the population parameter $\mathbf{y} : \mathcal{S} \longrightarrow \Re^{m}$
with weights $w_{k} : \mathcal{S} \longrightarrow \Re$, $k \in U$, the following
\begin{equation}
\label{eqn:generalLinear:VarEstimator}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]\!(s)
\;:=\; \sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\end{equation}
always gives an unbiased estimator for the covariance matrix of $\widehat{\mathbf{T}}_{y;w}$.
For the Horvitz-Thompson estimator, the weights are $w_{k} = 1/\pi_{k}$, for each $k \in U$,
and the weights are independent of the sample $s \in \mathcal{S}$.
Thus, for the Horvitz-Thompson estimator, the right-hand side of equation
\eqref{eqn:generalLinear:VarEstimator} becomes:
\begin{eqnarray*}
\sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
&=&\sum_{i,k \in s}\dfrac{\Cov(I_{i},I_{k})}{\pi_{ik}}\,\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T} \\
&=&\sum_{i,k \in s}\dfrac{E(I_{i}I_{k})-E(I_{i})E(I_{k})}{\pi_{ik}}\,\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T} \\
&=&\sum_{i,k \in s}\dfrac{\pi_{ik}-\pi_{i}\pi_{k}}{\pi_{ik}}\,\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T},
\end{eqnarray*}
which coincides with the right-hand side of the equation of the conclusion of the present Proposition.
Thus this present Proposition is but a special case of Proposition \ref{proposition:generalLinear:Var},
specialized to the Horvitz-Thompson estimator, and the proof is now complete. \qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Estimation of Domain Totals}
\setcounter{theorem}{0}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Conditional inference in finite-population sampling}
\setcounter{theorem}{0}

In this section, we give a justification for making inference conditional on the observed sample size
for sampling designs with random sample size.

\vskip 0.5cm
\noindent
\textbf{Observation (``mixture" of experiments) [see \cite{Valliant2000}, p.15.]}\vskip 0.1cm
\noindent
Consider a population $\mathcal{U}$ of 1000 units.
We wish to estimate the total $T_{y}$ of a certain population characteristic $\mathbf{y} = (y_{1},y_{2},\ldots,y_{1000})$.
Suppose we use the following two-step sampling scheme:
\begin{itemize}
\item	Step 1: We first flip a fair coin.\\
		Define the random variable $X$ by letting $X = 1$ if the coin lands heads, and $X = 0$ if it lands tails.
\item	Step 2: If $X = 1$, we select an SRS from $\mathcal{U}$ of size 100. If $X = 0$, we take a census on all of $\mathcal{U}$.
\end{itemize}
Let $\mathcal{S} \subset \mathcal{P}(\mathcal{U})$ denote the probability space of all
possible samples induced by the (two-step) sampling design above.
Note that $\mathcal{S} = \mathcal{S}_{0} \sqcup \mathcal{S}_{1}$,
where $\mathcal{S}_{0} = \left\{\,\,\mathcal{U}\,\right\}$ and
$\mathcal{S}_{1}$ is the set of all subsets of $\mathcal{U}$ of size 100.
The sampling design is determined by the following probability distribution on $\mathcal{S}$:
\begin{equation*}
P\!\left(\,\mathcal{U}\,\right) = \dfrac{1}{2}
\quad\textnormal{and}\quad
P\!\left(\,s\,\right) = \frac{1}{2\left(\begin{array}{c}1000\\100\end{array}\right)},
\;\;\textnormal{for each $s \in \mathcal{S}_{1}$}.
\end{equation*}
Let $\widehat{T}_{y}:\mathcal{S} \longrightarrow \Re$ denote our chosen estimator for $T_{y}$.
Then the (unconditional) probability distribution of $\widehat{T}_{y}$ can be ``decomposed" as follows:
\begin{eqnarray*}
P\left(\,\left.\widehat{T}_{y} = t\;\right\vert\,\mathbf{y}\,\right)
&=& P\left(\,\left.\widehat{T}_{y} = t,\,X = 0\;\right\vert\,\mathbf{y}\,\right) + P\left(\,\left.\widehat{T}_{y} = t,\,X = 1\,\right\vert\,\mathbf{y}\,\right) \\
&=& P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 0,\,\mathbf{y}\,\right)\cdot P\left(\,\left.X = 0\;\right\vert\,\mathbf{y}\,\right)
	+ P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 1,\,\mathbf{y}\,\right)\cdot P\left(\,\left.X = 1\;\right\vert\,\mathbf{y}\,\right) \\
&=& P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 0,\,\mathbf{y}\,\right)\cdot P\left(\,X = 0\,\right)
	+ P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 1,\,\mathbf{y}\,\right)\cdot P\left(\,X = 1\,\right),
\end{eqnarray*}
where the last equality follows because the distribution of $X$ is independent of $\mathbf{y}$.
Suppose the observation we make consists of $\left(\,\widehat{T}_{y}\,,\,X\,\right)$.
The unconditional probability distribution of $\widehat{T}_{y}$,
given by $P\left(\,\left.\widehat{T}_{y} = t\;\right\vert\,\mathbf{y}\,\right)$ above,
describes of course the randomness of the estimator $\widehat{T}_{y}$ as induced
by both the randomness of the sample
$s \in \mathcal{S} = \mathcal{S}_{0} \sqcup \mathcal{S}_{1}$
as well as that of $X$ (the outcome of the coin flip in Step 1).
Now, suppose we have indeed carried out the sampling procedure and have obtained
an observation of $\left(\,\widehat{T}_{y}\,,\,X\,\right)$.
Suppose it happened that $X = 1$.
Hence, we know that the estimate $\widehat{T}_{y}(s)$ we actually obtained
was generated from an SRS of size 100 (rather than a census).
Note also that the probability distribution of $X$ is independent of $\mathbf{y}$
and the observation of $X$ gives no information about $\mathbf{y}$.
{\color{red}One school of thought therefore argues that downstream inferences
about $\mathbf{y}$ should be carried out using the conditional probability
$P\!\left(\,\left.\widehat{T}_{y} = t\;\right\vert\;X = 1\,,\,\mathbf{y}\,\right)$,
rather than the unconditional probability
$P\left(\,\left.\widehat{T}_{y} = t\;\right\vert\,\mathbf{y}\,\right)$.}
In other words, in the present example, as far as making inferences about $\mathbf{y}$
is concerned, only the randomness in Step 2 is relevant, and the randomness in Step 1
(i.e. the randomness of $X$, the outcome of the coin flip) is irrelevant to any inference
about $\mathbf{y}$. Consequently randomness of $X$ ``should" be removed in any
inference procedure for $\mathbf{y}$, and this is achieved by conditioning on the
observed value of $X$. \qed

\vskip 0.5cm
\noindent
\textbf{Conditioning on obtained sample size for sample designs with random sample size}\vskip 0.1cm
\noindent
Suppose $\mathcal{U}$ is a finite population.
We wish to estimate the total $T_{y} = \sum_{i\in\mathcal{U}}y_{i}$ of a
population characteristic $\mathbf{y} : \mathcal{U} \longrightarrow \Re$,
using a sample design $p: \mathcal{S} \longrightarrow [\,0\,,1\,]$
and a estimator $\widehat{T} : \mathcal{S} \longrightarrow \Re$.
{\color{red}We make the assumption that the sampling design $p$ is independent of $\mathbf{y}$.}
Let $N : \mathcal{S} \longrightarrow \N\cup\{\,0\,\}$ be the random variable
of sample size, i.e. $N(s)$ $=$ number of elements in $s$,
for each possible sample $s \in \mathcal{S}$. Then,
\begin{eqnarray*}
P\left(\,\left.\widehat{T} = t \,\right\vert\,\mathbf{y}\,\right)
&=&  \sum_{n}\,P\left(\,\left.\widehat{T} = t,\, N = n \,\right\vert\,\mathbf{y}\,\right) \\
&=&  \sum_{n}\,P\left(\,\left.\widehat{T} = t\,\right\vert\,\,N = n,\, \mathbf{y}\,\right)\cdot P\left(\left.\,N = n\,\right\vert\,\mathbf{y}\,\right)\\
&=&  \sum_{n}\,P\left(\,\left.\widehat{T} = t\,\right\vert\,\,N = n,\, \mathbf{y}\,\right)\cdot P\left(\,N = n\,\right),
\end{eqnarray*}
where the last equality follows from the assumed independence of the probability distribution
$p : \mathcal{S} \longrightarrow [\,0\,,\,1\,]$ (hence that of $N$) from $\mathbf{y}$.
The key observation to make now is that:
{\color{red}Although the actual sampling procedure operationally may or may not
have been a two-step procedure, the independence of $p$ from $\mathbf{y}$ makes it probabilistically
equivalent to a two-step procedure, as shown by the above decomposition of
$P\!\left(\,\left.\widehat{T} = t\;\right\vert\,\mathbf{y}\right)$}
--- Step (1): randomly select a sample size $N = n$ according to the distribution $P(N = n)$, and then
Step (2): randomly select a sample $s$ of size $n$ chosen in Step (1) according to the distribution
$P\!\left(\,s\;\vert\,N = n\,\right)$.
By the statistical reasoning explained in the preceding observation, it follows that
post-sampling inference about $\mathbf{y}$ should be made based on the conditional
distribution $P\!\left(\,\left.\widehat{T} = t\;\right\vert\,N = n\,,\,\mathbf{y}\,\right)$,
rather than the unconditional distribution $P\!\left(\,\left.\widehat{T} = t\;\right\vert\,\mathbf{y}\,\right)$.
This is because the sampling scheme is probabilistically equivalent to a two-step procedure,
with the probability distribution of the first step (choosing a sample size) independent of the parameters
of interest ($T_{y}$), and thus only the probability distribution of the second step (choosing a sample
of the size chosen in first step) should be used to make inference about $T_{y}$.
\qed

\vskip 0.5cm
\noindent
\textbf{Caution}\vskip 0.1cm
\noindent
In more formal parlance, the random variable $N : \mathcal{S} \longrightarrow \N\cup\{\,0\,\}$
is \underline{ancillary} to the parameter $\mathbf{y}$.
Thus, conditioning on sample size, for finite-population sampling schemes with random
sample size, \emph{partially} conforms to the \textbf{Conditionality Principle},
which states that statistical inference about a parameter should be made conditioned on
observed values of statistics ancillary to that parameter.
The conformance is only partial due to the (obvious) fact that it is the sample $s$ itself
which is ancillary to the parameter of interest $\mathbf{y}$, not just its sample size $N(s)$.
Thus, full conformance to the Conditionality Principle would require inference about
$\mathbf{y}$ be made conditioned on the observed sample $s$ itself (rather than its
size $N(s)$).
However, if we did condition on the obtained sample $s$ itself, the domain of the estimator
$\widehat{T}$ would be restricted to the singleton $\{\,s\,\}$, and $\widehat{T}$ could then
attain only one value under conditioning on $s$, and no randomization-based
(i.e. design-based) inference --- apart from the observed value of $\widehat{T}(s)$ --- could
be made any longer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{alpha}
%\bibliographystyle{plain}
%\bibliographystyle{amsplain}
\bibliographystyle{acm}
\bibliography{KenChuStatistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

