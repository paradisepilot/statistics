
\documentclass{article}

\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
%\usepackage{doublespace}

\usepackage{KenChuArticleStyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{page}{1}

\pagestyle{fancy}

%\input{../CourseSemesterUnique}

%\rhead[\CourseSemesterUnique]{Kenneth Chu (300517641)}
%\lhead[Kenneth Chu (300517641)]{\CourseSemesterUnique}
\rhead[Study Notes]{Kenneth Chu}
\lhead[Kenneth Chu]{Study Notes}
\chead[]{{\Large\bf Survey Sampling Theory} \\
\vskip 0.1cm \normalsize \today}
\lfoot[]{}
\cfoot[]{}
\rfoot[]{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\mbox{}\vskip 0.0cm
\section{The population total, population mean, and population variance of a population characteristic}
\setcounter{theorem}{0}

Let $n, N \in \N$, with $n \leq N$.  Let $\mathcal{U} = \{\,1,2,\ldots,N\,\}$, which represents the finite population, or universe, of $N$ elements.  

\begin{definition}\quad
A \emph{population characteristic} is an $\Re$-valued function $y : \mathcal{U} \longrightarrow \Re$ defined on the population $\mathcal{U}$.  We denote the value of $y$ evaluated at $i \in \mathcal{U}$ by $y_{i}$.
The {population total}, denoted by $t$, of $y$ is defined:
\begin{equation*}
    t \; := \; \sum^{N}_{i=1} y_{i} \in \Re\,.
\end{equation*}
The \emph{population mean}, denoted by $\overline{y}$, of $y$ is defined by:
\begin{equation*}
    \overline{y} \; := \; \dfrac{1}{N}\sum^{N}_{i=1} y_{i} \in \Re\,.
\end{equation*}
The \emph{population variance}, denoted by $S^{2}$, of $y$ is defined by:
\begin{equation*}
    S^{2} \; := \; \dfrac{1}{N-1}\sum^{N}_{i=1} \left(y_{i}-\overline{y}\right)^{2}
             \; = \; \dfrac{1}{N-1}\left\{\;\left(\sum^{N}_{i=1}y_{i}^{2}\right) - N\cdot\overline{y}^{2} \;\right\} \in \Re\,.
\end{equation*}
\end{definition}

In survey sampling, we seek to estimate population total $t$ and population mean $\overline{y}$ of a population characteristic $y : \mathcal{U} \longrightarrow \Re$ by making observations of values of $y$ on only a (usually proper) subset of $\mathcal{U}$, and extrapolate from these observations.  The subset on which observations of values of $y$ are made is called a \emph{sample}.

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Simple Random Sampling (SRS)}
\setcounter{theorem}{0}

\begin{definition}\quad
Let $\mathcal{U}$ be a nonempty finite set, $N:=\#(\mathcal{U})\in\N$, and let $n \in \{\,1,2\ldots,N\,\}$ be given.  We define the probability space $\Omega_{\textnormal{SRS}}(\mathcal{U},n)$ as follows:  Let $\Omega(\mathcal{U},n)$ be the set of all subsets of $\mathcal{U}$ with $n$ elements, i.e.
\begin{equation*}
     \Omega(\mathcal{U},n)\; := \; \left\{ \; \omega\subset\mathcal{U} \;\vert\; \#(\omega)=n \; \right\}.
\end{equation*}
Note that $\#\!\left(\Omega(\mathcal{U},n)\right)$ $=$ {\small$\left(\!\begin{array}{c} N \\ n \end{array}\!\right)$}.
Let $\mathcal{P}(\Omega(\mathcal{U},n))$ be the power set of $\Omega(\mathcal{U},n)$.
Define $\mu : \Omega \longrightarrow \Re$ to be the ``uniform'' probability measure on the (finite) $\sigma$-algebra $\mathcal{P}(\Omega(\mathcal{U},n))$ determined by:
\begin{equation*}
   \mu(\omega) \; = \; \dfrac{1}{\textnormal{\small$\left(\!\begin{array}{c} N \\ n \end{array}\!\right)$}} \; = \; \dfrac{n!(N-n)!}{N!}\;,
   \quad\textnormal{for each}\;\; \omega \in \Omega(\mathcal{U},n).
\end{equation*}
Then, $\Omega_{\textnormal{SRS}}(\mathcal{U},n)$ is defined to be the probability space $\left(\;\Omega(\mathcal{U},n)\,,\,\mathcal{P}(\Omega(\mathcal{U},n))\,,\,\mu\;\right)$.
\end{definition}


\begin{definition}\quad
The \emph{simple-random-sampling sample total} $\widehat{t}_{\textnormal{SRS}}$ of the population characteristic $y$ is, by definition, the random variable $\widehat{t}_{\textnormal{SRS}} : \Omega_{\textnormal{SRS}}(\mathcal{U},n) \longrightarrow \Re$ defined by 
\begin{equation*}
    \widehat{t}_{\textnormal{SRS}}(\omega) \; := \; \dfrac{N}{n}\sum_{i\in\omega}y_{i}\,, \quad\textnormal{for each}\;\; \omega\in\Omega.
\end{equation*}
The \emph{simple-random-sampling sample mean} $\widehat{\overline{y}}_{\textnormal{SRS}}$ of the population characteristic $y$ is, by definition, the random variable $\widehat{\overline{y}}_{\textnormal{SRS}} : \Omega_{\textnormal{SRS}}(\mathcal{U},n) \longrightarrow \Re$ defined by
\begin{equation*}
    \widehat{\overline{y}}_{\textnormal{SRS}}(\omega) \; := \; \dfrac{1}{n}\sum_{i\in\omega} y_{i}\,, \quad\textnormal{for each}\;\; \omega\in\Omega.
\end{equation*}
The \emph{simple-random-sampling sample variance} $\widehat{s^{2}}_{\textnormal{SRS}}$ of the population characteristic $y$ is, by definition, the random variable $\widehat{s^{2}}_{\textnormal{SRS}} : \Omega_{\textnormal{SRS}}(\mathcal{U},n) \longrightarrow \Re$ defined by
\begin{equation*}
    \widehat{s^{2}}_{\textnormal{SRS}}(\omega) \; := \; \dfrac{1}{n-1}\sum_{i\in\omega} \left(y_{i}-\widehat{\overline{y}}_{\textnormal{SRS}}(\omega)\right)^{2}\,, \quad\textnormal{for each}\;\; \omega\in\Omega.
\end{equation*}
\end{definition}

\begin{proposition}\label{SRS:unbiased:estimators}\quad
\begin{enumerate}
\item  $\widehat{\overline{y}}_{\textnormal{SRS}}$ is an unbiased estimator of the population mean $\overline{y}$, and
          $\Var\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]$ $=$ $\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n}$.
\item  $\widehat{t}_{\textnormal{SRS}}$ is an unbiased estimator of the population total $t$, and
          $\Var\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]$ $=$ $N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n}$.
\item  $\widehat{s^{2}}_{\textnormal{SRS}}$ is an unbiased estimator of the population variance $S^{2}$.
\item  $\widehat{\Var}\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]$
          $:=$ $\left(1-\dfrac{n}{N}\right)\dfrac{\widehat{s^{2}}_{\textnormal{SRS}}}{n}$
          is an unbiased estimator of $\Var\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]$.
\item  $\widehat{\Var}\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]$
          $:=$ $N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{\widehat{s^{2}}_{\textnormal{SRS}}}{n}$
          is an unbiased estimator of $\Var\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]$.
\end{enumerate}
\end{proposition}

\noindent
A quote from Lohr \cite{Lohr1999}, p.37:
\emph{
H\'{a}jek \cite{Hajek1960} proves a central limit theorem for simple random sampling without replacement.  In practical terms, H\'{a}jek's theorem says that if certain technical conditions hold, and if $n$, $N$, and $N-n$ are all ``sufficiently large," then the sampling distribution of
\begin{equation*}
    \dfrac{\widehat{\overline{y}}_{\textnormal{SRS}}-\overline{y}}{\sqrt{\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n}}}
\end{equation*}
is ``approximately" normal (Gaussian) with mean $0$ and variance $1$.
}

\begin{corollary}[to H\'{a}jek's theorem]\quad
For a simple random sampling procedure, an approximate $(1-\alpha)$-confidence interval, $0 < \alpha < 1$, for the population mean $\overline{y}$ is given by:
\begin{equation*}
\widehat{\overline{y}}_{\textnormal{SRS}} \, \pm \, z_{\alpha/2} \cdot \sqrt{\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n}}
\end{equation*}
For sufficiently large samples, the above approximate confidence interval can itself be estimated from observations by:
\begin{equation*}
\widehat{\overline{y}}_{\textnormal{SRS}} \, \pm \, \textnormal{SE}\!\left[\; \widehat{\overline{y}}_{\textnormal{SRS}} \;\right]
\;\; = \;\;
\widehat{\overline{y}}_{\textnormal{SRS}} \, \pm \, \sqrt{ \left(1-\dfrac{n}{N}\right)\dfrac{\widehat{s^{2}}_{\textnormal{SRS}}}{n} }
\end{equation*}
where
\begin{equation*}
\textnormal{SE}\!\left[\; \widehat{\overline{y}}_{\textnormal{SRS}} \;\right]
\;\; := \;\;
\sqrt{ \widehat{\Var}\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right] }
\;\; = \;\;
\sqrt{ \left(1-\dfrac{n}{N}\right)\dfrac{\widehat{s^{2}}_{\textnormal{SRS}}}{n} }
\end{equation*}
\end{corollary}

\noindent
In order to prove Proposition \ref{SRS:unbiased:estimators}, we introduce some auxiliary random variables:

\begin{definition}\quad
Let $n, N \in \N$, with $n<N$, $\mathcal{U} := \{\;1,2,\ldots,N\;\}$, and $\Omega := \left\{\;\omega\subset\mathcal{U}\;\vert\;\#(\omega)=n\;\right\}$.  For each $i \in \mathcal{U} = \{\;1,2,\ldots,N\;\}$, we define the random variable $Z_{i} : \Omega \longrightarrow \{0,1\}$ as follows:
\begin{equation*}
    Z_{i}(\omega) \; = \;
    \left\{
    \begin{array}{cl}
        1, & \textnormal{if}\;\; i \in \omega\,, \\
        0, & \textnormal{if}\;\; i \notin \omega
    \end{array}
    \right..
\end{equation*}
\end{definition}

\noindent\textbf{Immediate observations:}
\begin{itemize}
\item  $\widehat{t}_{\textnormal{SRS}}$ $=$ $\dfrac{N}{n}\sum^{N}_{i=1} Z_{i}\,y_{i}$,, as random variables on $(\Omega,P)$, i.e.
          \begin{equation*}
              \widehat{t}_{\textnormal{SRS}}(\omega) \; = \; \dfrac{N}{n}\sum_{i=1}^{N} Z_{i}(\omega)\,y_{i},
               \quad\textnormal{for each} \;\; \omega \in \Omega.
          \end{equation*}
\item  $\widehat{\overline{y}}_{\textnormal{SRS}}$ $=$ $\dfrac{1}{n}\sum_{i=1}^{N} Z_{i}\,y_{i}$, as random variables on $(\Omega,P)$, i.e.
          \begin{equation*}
              \widehat{\overline{y}}_{\textnormal{SRS}}(\omega) \; = \; \dfrac{1}{n}\sum_{i=1}^{N} Z_{i}(\omega)\,y_{i},
               \quad\textnormal{for each} \;\; \omega \in \Omega.
          \end{equation*}
\item  $E[\;Z_{i}\;] = \dfrac{n}{N}$.  \; Indeed,
          \begin{equation*}
              E[\;Z_{i}\;] = 1 \cdot P(Z_{i}=1) + 0 \cdot P(Z_{i}=0) = P(Z_{i}=1)
              = \dfrac{\textnormal{\small number of samples containing $i$}}{\textnormal{\small number of all possible samples}}
              = \dfrac{\left(\begin{array}{c} N-1 \\ n-1 \end{array}\right)}{\left(\begin{array}{c} N \\ n \end{array}\right)}
              = \dfrac{n}{N}
          \end{equation*}
\item  $Z_{i}^{2} = Z_{i}$, since $\textnormal{range}(Z_{i}) = \{\,0,1\,\}$.  Consequently,
          \begin{equation*}
              E[\;Z_{i}^{2}\;] \; = \; E[\;Z_{i}\;] \; = \; \dfrac{n}{N}\,.
          \end{equation*}
\item  $\Var[\;Z_{i}\;] = \dfrac{n}{N}\left(1 - \dfrac{n}{N}\right)$. \; Indeed,
          \begin{eqnarray*}
              \Var[\;Z_{i}\;] & := & E\left[\;\left(Z_{i} - E[\;Z_{i}\;]\right)^{2}\;\right]
                                                      \;\; = \;\; E\left[Z_{i}^{2}\right] - \left(E[\;Z_{i}\;]\right)^{2} \\
                                                      & = &  E[\;Z_{i}\;] - \left(\dfrac{n}{N}\right)^{2}
                                                      \;\; = \;\; \dfrac{n}{N} - \left(\dfrac{n}{N}\right)^{2} \\
                                                      & = & \dfrac{n}{N}\left(1 - \dfrac{n}{N}\right).
          \end{eqnarray*}
\item  For $i \neq j$, we have $E[\;Z_{i} \cdot Z_{j}\;] = \left(\!\dfrac{n-1}{N-1}\!\right)\cdot\left(\!\dfrac{n}{N}\!\right)$. \; Indeed,
          \begin{eqnarray*}
              E[\;Z_{i}\cdot Z_{j}\;] & = & 1 \cdot P(Z_{i}=1 \textnormal{\;\;and\;\;} Z_{j}=1) + 0\cdot P(Z_{i}=0 \textnormal{\;\;or\;\;} Z_{j}=0) \\
                                               & = & P(Z_{i}=1 \textnormal{\;\;and\;\;} Z_{j}=1)
                                               \;\; = \;\; P(Z_{j}=1\vert Z_{i}=1) \cdot P(Z_{i}=1) \\
                                               & = & \left(\!\dfrac{n-1}{N-1}\!\right)\cdot\left(\!\dfrac{n}{N}\!\right)
          \end{eqnarray*}
\item  For $i \neq j$, we have
          $\textnormal{Cov}\left(Z_{i},Z_{j}\right) = - \dfrac{1}{N-1}\left(1-\dfrac{n}{N}\right)\left(\dfrac{n}{N}\right) \leq 0$.
          \; Indeed,
          \begin{eqnarray*}
                         \textnormal{Cov}\left(Z_{i},Z_{j}\right)
              & := &  E\left[\;\left(Z_{i}-E[\;Z_{i}\;]\right)\cdot\left(Z_{j}-E[\;Z_{j}\;]\right)\;\right]
              \;\;=\;\;  E\left[\;Z_{i}\,Z_{j}\;\right] - E[\;Z_{i}\;]\cdot E[\;Z_{j}\;] \\
              &  = &  \left(\!\dfrac{n-1}{N-1}\!\right)\cdot\left(\!\dfrac{n}{N}\!\right) - \left(\dfrac{n}{N}\right)^{2}
              \;\;=\;\;  \dfrac{n}{N}\left(\dfrac{nN - N - nN + n}{N(N-1)}\right) \\
              & = &  - \dfrac{1}{N-1}\left(1-\dfrac{n}{N}\right)\left(\dfrac{n}{N}\right)
          \end{eqnarray*}
\end{itemize}

\proofof Proposition \ref{SRS:unbiased:estimators}
\begin{enumerate}
\item  \begin{equation*}
                        E\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]
             \;\;=\;\;   E\!\left[\; \dfrac{1}{n}\sum_{i=1}^{N} Z_{i} \, y_{i} \;\right]
             \;\;=\;\;  \dfrac{1}{n} \sum^{N}_{i=1}E\!\left[\; Z_{i} \;\right]\cdot y_{i}
             \;\;=\;\;  \dfrac{1}{n} \sum^{N}_{i=1}\left(\dfrac{n}{N}\right)\cdot y_{i} 
             \;\;=\;\;  \dfrac{1}{N} \sum^{N}_{i=1} y_{i}
             \;\;=:\;\; \overline{y}.
          \end{equation*}
          \begin{eqnarray*}
                         \Var\!\left[\; \widehat{\overline{y}}_{\textnormal{SRS}} \;\right]
              & = &  \Var\!\left[\; \dfrac{1}{n}\sum_{i=1}^{N} Z_{i} \, y_{i} \;\right]
              \;\;=\;\; \dfrac{1}{n^{2}}\,\Var\!\left[\; \sum_{i=1}^{N} Z_{i} \, y_{i} \;\right]
              \;\;=\;\; \dfrac{1}{n^{2}}\,\textnormal{Cov}\!\left[\; \sum_{i=1}^{N} Z_{i} \, y_{i} \;,\; \sum_{j=1}^{N} Z_{j} \, y_{j} \;\right] \\
              & = &  \dfrac{1}{n^{2}}\left\{\;\sum^{N}_{i=1}y_{i}^{2}\,\Var(Z_{i})
                                                        + \sum^{N}_{i=1}\sum_{i\neq j=1}^{N} y_{i}y_{j}\,\textnormal{Cov}(Z_{i},Z_{j})\;\right\} \\
              & = &  \dfrac{1}{n^{2}}\left\{\;\sum^{N}_{i=1}y_{i}^{2}\,\dfrac{n}{N}\left(1-\dfrac{n}{N}\right)
                        - \sum^{N}_{i=1}\sum_{i\neq j=1}^{N} y_{i}y_{j}\,\dfrac{1}{N-1}\left(1-\dfrac{n}{N}\right)\left(\dfrac{n}{N}\right)\;\right\} \\
              & = &  \dfrac{1}{n^{2}}\dfrac{n}{N}\left(1-\dfrac{n}{N}\right)\left\{\;\sum^{N}_{i=1}y_{i}^{2}
                        - \dfrac{1}{N-1}\sum^{N}_{i=1}\sum_{i\neq j=1}^{N} y_{i}y_{j}\;\right\} \\
              & = &  \dfrac{1}{n}\left(1-\dfrac{n}{N}\right)\dfrac{1}{N(N-1)}\left\{\;(N-1)\sum^{N}_{i=1}y_{i}^{2}
                        - \sum^{N}_{i=1}\sum_{i\neq j=1}^{N} y_{i}y_{j}\;\right\} \\
              & = &  \dfrac{1}{n}\left(1-\dfrac{n}{N}\right)\dfrac{1}{N(N-1)}\left\{\;(N-1)\sum^{N}_{i=1}y_{i}^{2}
                        - \sum^{N}_{i=1}\sum_{j=1}^{N} y_{i}y_{j} + \sum^{N}_{i=1}y_{i}^{2}\;\right\} \\
              & = &  \dfrac{1}{n}\left(1-\dfrac{n}{N}\right)\dfrac{1}{N(N-1)}\left\{\;N\sum^{N}_{i=1}y_{i}^{2}
                        - \left(\sum^{N}_{i=1} y_{i}\right)\left(\sum_{j=1}^{N}y_{j}\right)\;\right\} \\
              & = &  \dfrac{1}{n}\left(1-\dfrac{n}{N}\right)\dfrac{1}{N-1}\left\{\;\sum^{N}_{i=1}y_{i}^{2}
                        - N\left(\dfrac{1}{N}\sum^{N}_{i=1} y_{i}\right)^{2}\;\right\} \\
              & = &  \dfrac{1}{n}\left(1-\dfrac{n}{N}\right)\dfrac{1}{N-1}\left\{\;\sum^{N}_{i=1}y_{i}^{2}
                        - N\cdot\overline{y}^{2}\;\right\} \\
              & = &  \left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n}
          \end{eqnarray*}
\item  \begin{equation*}
                        E\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]
             \;\; = \;\; E\!\left[\;N\cdot\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]
             \;\; = \;\; N\cdot E\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]
             \;\; = \;\; N\cdot\overline{y}
             \;\; = \;\; N\cdot\left(\dfrac{1}{N}\sum^{N}_{i=1}y_{i}\right)
             \;\; = \;\; \sum^{N}_{i=1}y_{i}
             \;\; =:\;\; t.
          \end{equation*}
          \begin{equation*}
                        \Var\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]
             \;\; = \;\; \Var\!\left[\;N\cdot\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]
             \;\; = \;\; N^{2} \cdot \Var\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right]
             \;\; = \;\; N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n}
          \end{equation*}
\item  \begin{eqnarray*}
                        E\!\left[\;\widehat{s^{2}}_{\textnormal{SRS}}\;\right]
              & = &  E\!\left[\; \dfrac{1}{n-1}\sum_{i\in\omega} \left(y_{i}-\widehat{\overline{y}}_{\textnormal{SRS}}\right)^{2} \;\right]      
              \;\;=\;\;  \dfrac{1}{n-1}\,E\!\left[\; \sum_{i\in\omega}
                          \left((y_{i}-\overline{y})-(\,\widehat{\overline{y}}_{\textnormal{SRS}}-\overline{y})\right)^{2} \;\right] \\
              & = &  \dfrac{1}{n-1}\,E\!\left[\; \left(\sum_{i\in\omega}(y_{i}-\overline{y})^{2} \right)
                          -n\left(\widehat{\overline{y}}_{\textnormal{SRS}}-\overline{y}\right)^{2} \;\right] \\
              & = &  \dfrac{1}{n-1}\left\{\;E\!\left[\; \sum_{i=1}^{N}Z_{i}(y_{i}-\overline{y})^{2} \;\right]
                          - n\,\Var\!\left[\;\widehat{\overline{y}}_{\textnormal{SRS}}\;\right] \right\} \\
              & = &  \dfrac{1}{n-1}\left\{\; \sum_{i=1}^{N} E\!\left[\; Z_{i} \;\right] (y_{i}-\overline{y})^{2}
                          - n\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}}{n} \; \right\} \\
              & = &  \dfrac{1}{n-1}\left\{\;\sum_{i=1}^{N} \dfrac{n}{N} (y_{i}-\overline{y})^{2}
                          - \left(1-\dfrac{n}{N}\right)S^{2}\;\right\} \\
              & = &  \dfrac{1}{n-1}\left\{\;\dfrac{n(N-1)}{N}\dfrac{1}{N-1}\sum_{i=1}^{N} (y_{i}-\overline{y})^{2}
                          - \left(1-\dfrac{n}{N}\right)S^{2}\;\right\} \\
              & = &  \dfrac{1}{n-1}\left\{\;\dfrac{n(N-1)}{N} - \left(1-\dfrac{n}{N}\right)\;\right\} S^{2} \\
              & = &  \dfrac{1}{n-1}\left\{\;\dfrac{nN-n-N+n}{N}\;\right\} S^{2} \;\; = \;\; S^{2}
          \end{eqnarray*}
\item  Immediate from preceding statements.
\item  Immediate from preceding statements. \qed
\end{enumerate}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Stratified Simple Random Sampling}
\setcounter{theorem}{0}

Let $\mathcal{U} = \{\,1,2,\ldots,N\,\}$ be the population, as before.  Let
\begin{equation*}
    \mathcal{U} \; = \; \bigsqcup_{h=1}^{H}\,\mathcal{U}_{h}
\end{equation*}
be a partition of $\mathcal{U}$.  Such a partition is called a \emph{stratification} of the population $\mathcal{U}$.
Each of $\mathcal{U}_{1}, \mathcal{U}_{2}, \ldots, \mathcal{U}_{H}$ is called a \emph{stratum}.
Let $N_{h} := \#\!\left(\mathcal{U}_{h}\right)$, for $h=1,2,\ldots,H$.  Note that $N_{1}+N_{2}+\cdots+N_{H} = N$.

In \emph{stratified simple random sampling}, an SRS is taken within each stratum $\mathcal{U}_{h}$, $h=1,2,\ldots,H$.
Let $n_{h}$, $h = 1,2,\ldots,H$, be the number elements in the simple random sample taken in the stratum $\mathcal{U}_{h}$.
In other words, a stratified simple random sample $\omega$ of the stratified population $\mathcal{U} = \bigsqcup_{h=1}^{H}\,\mathcal{U}_{h}$ has the form:
\begin{equation*}
     \omega \; = \; \bigsqcup_{h=1}^{H}\,\omega_{h}\,,
     \quad\textnormal{where}\;\; \omega_{h} \in \Omega_{\textnormal{SRS}}(\mathcal{U}_{h},n_{h})\,,
     \quad\textnormal{for each}\;\; h = 1,2,\ldots,h\,.
\end{equation*}
Note that $n_{1} + n_{2} + \cdots + n_{H} =: n = \#\!(\omega)$.

We now give unbiased estimators, and their variances, of the population total and population mean of a population characteristic under stratified simple random sampling.  Let $y : \mathcal{U} \longrightarrow \Re$ be a population characteristic.  Define:
\begin{equation*}
    \widehat{t}_{\textnormal{Str}} \; := \; 
    \sum^{H}_{h=1}\, N_{h}\cdot\widehat{\overline{y}}_{h,\textnormal{SRS}}
\end{equation*}
\begin{equation*}
              \widehat{\overline{y}}_{\textnormal{Str}}
    \; := \; \dfrac{1}{N}\cdot\widehat{t}_{\textnormal{Str}}
    \;  = \; \sum^{H}_{h=1}\, \dfrac{N_{h}}{N}\cdot\widehat{\overline{y}}_{h,\textnormal{SRS}}
\end{equation*}
Here,
\begin{equation*}
    \widehat{\overline{y}}_{h,\textnormal{SRS}} \; : \; \Omega_{\textnormal{SRS}}(\mathcal{U}_{h},n_{h}) \longrightarrow \Re
    \; : \: \omega_{h} \; \longmapsto \; \dfrac{1}{n_{h}}\,\sum_{i\in\omega_{h}}\,y_{i}
\end{equation*}
is the SRS estimator of
\begin{equation*}
    \overline{y}_{h} \; := \; \overline{y|_{\mathcal{U}_{h}}} \; = \; \dfrac{1}{N_{h}} \sum_{i\in\mathcal{U}_{h}} y_{i} \in \Re,
\end{equation*}
the ``stratum mean" of the ``stratum characteristic" $y|_{\mathcal{U}_{h}} : \mathcal{U}_{h} \longrightarrow \Re$, the restriction of the population characteristic $y : \mathcal{U} \longrightarrow \Re$ to the stratum $\mathcal{U}_{h}$.  Then,
\begin{equation*}
    E\!\left[\;\widehat{t}_{\textnormal{Str}}\;\right] \; = \; t \; := \; \sum^{N}_{i=1}\,y_{i},
    \quad\textnormal{and}\quad
    E\!\left[\;\widehat{\overline{y}}_{\textnormal{Str}}\;\right] \; = \; \overline{y} \; := \; \dfrac{1}{N}\,\sum^{N}_{i=1}\,y_{i}.
\end{equation*}
In other words, $\widehat{t}_{\textnormal{Str}}$ and $\widehat{\overline{y}}_{\textnormal{Str}}$ are unbiased estimators of the population total $t$ and population mean $\overline{y}$ of the population characteristic $y : \mathcal{U} \longrightarrow \Re$, respectively.  Indeed,
\begin{eqnarray*}
            E\!\left[\;\widehat{t}_{\textnormal{Str}}\;\right]
    &=&  E\!\left[\; \sum^{H}_{h=1}\, N_{h}\cdot\widehat{\overline{y}}_{h,\textnormal{SRS}} \;\right]
    \;\;=\;\;  \sum^{H}_{h=1}\, N_{h}\,E\!\left[\; \widehat{\overline{y}}_{h,\textnormal{SRS}} \;\right]
    \;\;=\;\;  \sum^{H}_{h=1}\, N_{h}\,\overline{y}_{h} \\
    &=& \sum^{H}_{h=1}\, N_{h}\left(\dfrac{1}{N_{h}}\sum_{i\in\mathcal{U}_{h}}\,y_{i}\right)
    \;\;=\;\;  \sum^{H}_{h=1}   \left(\sum_{i\in\mathcal{U}_{h}}\,y_{i}\right)
    \;\;=\;\;  \sum^{N}_{i=1}\,y_{i}
    \;\;=:\;\; t\,.
\end{eqnarray*}
And,
\begin{equation*}
           E\!\left[\;\widehat{\overline{y}}_{\textnormal{Str}}\;\right]
   \;=\;  E\!\left[\;\dfrac{1}{N}\cdot\widehat{t}_{\textnormal{Str}}\;\right]
   \;=\;  \dfrac{1}{N}\,E\!\left[\;\widehat{t}_{\textnormal{Str}}\;\right]
   \;=\;  \dfrac{1}{N}\,\sum^{N}_{i=1}\,y_{i}
   \;=:\; \overline{y}\,.
\end{equation*}
Furthermore,
\begin{equation*}
             \Var\!\left[\;\widehat{t}_{\textnormal{Str}}\;\right]
    \;=\;  \Var\!\left[\; \sum^{H}_{h=1}\, N_{h}\cdot\widehat{\overline{y}}_{h,\textnormal{SRS}} \;\right]
    \;=\;  \sum^{H}_{h=1}\, N_{h}^{2}\cdot\Var\!\left[\; \widehat{\overline{y}}_{h,\textnormal{SRS}} \;\right]
    \;=\;  \sum^{H}_{h=1}\, N_{h}^{2}\left(1-\dfrac{n_{h}}{N_{h}}\right)\dfrac{S^{2}_{h}}{n_{h}}.
\end{equation*}
\begin{equation*}
             \Var\!\left[\;\widehat{\overline{y}}_{\textnormal{Str}}\;\right]
    \;=\;  \Var\!\left[\; \dfrac{1}{N}\cdot\widehat{t}_{\textnormal{Str}} \;\right]
    \;=\;  \dfrac{1}{N^{2}}\cdot\Var\!\left[\; \widehat{t}_{\textnormal{Str}} \;\right]
    \;=\;  \sum^{H}_{h=1}\, \left(\dfrac{N_{h}}{N}\right)^{2}\left(1-\dfrac{n_{h}}{N_{h}}\right)\dfrac{S^{2}_{h}}{n_{h}}.
\end{equation*}

\noindent
\textbf{Comparing variances of SRS and stratified simple random sampling with proportional allocation via ANOVA (analysis of variance):}

By definition, in stratified simple random sampling with proportional allocation, the stratum sample size $n_{h}$, for each $h=1,2,\ldots,H$, is chosen such that $n_{h}/N_{h} = n/N$.  Consequently,
\begin{eqnarray*}
            \Var\!\left[\;\widehat{t}_{\textnormal{PropStr}}\;\right]
    &=&  \sum^{H}_{h=1}\, N_{h}^{2}\left(1-\dfrac{n_{h}}{N_{h}}\right)\dfrac{S^{2}_{h}}{n_{h}}
    \;\;=\;\;  \dfrac{N}{n}\left(1-\dfrac{n}{N}\right) \sum^{H}_{h=1}\, N_{h}\,S^{2}_{h} \\
    &=&  \dfrac{N}{n}\left(1-\dfrac{n}{N}\right) \left\{\;\sum^{H}_{h=1}\, (N_{h}-1)\,S^{2}_{h} + \sum^{H}_{h=1}\,S^{2}_{h}\;\right\} \\
    &=&  \dfrac{N}{n}\left(1-\dfrac{n}{N}\right) \left\{\; \textnormal{SSW} + \sum^{H}_{h=1}\,S^{2}_{h}\;\right\},
\end{eqnarray*}
where
\begin{equation*}
   \textnormal{SSW} \; := \;
   \sum^{H}_{h=1}\sum_{i\in\mathcal{U}_{h}}\left(y_{i}-\overline{y}_{\mathcal{U}_{h}}\right)^{2}
   \; = \;
   \sum^{H}_{h=1}\,(N_{h}-1)\,S^{2}_{h}.
\end{equation*}
is called the \emph{inter-strata squared deviation} (or \emph{within-strata squared deviation}), and
\begin{equation*}
    S^{2}_{h} \; := \; \dfrac{1}{N_{h}-1}\,\sum_{i\in\mathcal{U}_{h}}\left(y_{i}-\overline{y}_{\mathcal{U}_{h}}\right)^{2}
\end{equation*}
is the stratum variance of the population characteristic $y : \mathcal{U} \longrightarrow \Re$ over the stratum $\mathcal{U}_{h}$.  The following relation between $\Var\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]$ and $\Var\!\left[\;\widehat{t}_{\textnormal{PropStr}}\;\right]$ always holds (see \cite{Lohr1999}, p.106):
\begin{equation*}
\Var\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]
\;=\;
\Var\!\left[\;\widehat{t}_{\textnormal{PropStr}}\;\right] + 
\left(1-\dfrac{n}{N}\right)\dfrac{N}{n}\dfrac{N}{N-1}
\left\{\;\textnormal{SSB} - \sum_{h=1}^{H}\left(1-\dfrac{N_{h}}{N}\right)\!S^{2}_{h}\;\right\},
\end{equation*}
where
\begin{equation*}
    \textnormal{SSB} \; := \; \sum^{H}_{h=1}\,N_{h}\!\left(\overline{y}_{\mathcal{U}_{h}} - \overline{y}_{\mathcal{U}}\right)^{2}
    \; = \; \sum^{H}_{h=1}\,\sum_{i\in\mathcal{U}_{h}}\left(\overline{y}_{\mathcal{U}_{h}} - \overline{y}_{\mathcal{U}}\right)^{2}
\end{equation*}
is the \emph{inter-strata squared deviation} (or \emph{between-strata squared deviation}).  It is also an easily established fact that the sum of the inter-strata squared deviation $\textnormal{SSB}$ and the intra-strata squared deviation $\textnormal{SSW}$ is always the total population squared deviation $\textnormal{SSTO}$:
\begin{equation*}
   \textnormal{SSTO} \; := \; \sum^{N}_{i=1}\left(y_{i}-\overline{y}_{\mathcal{U}}\right)^{2}
   \; = \; \sum^{H}_{h=1}\sum_{i\in\mathcal{U}_{h}}\left(y_{i}-\overline{y}_{\mathcal{U}}\right)^{2}\,.
\end{equation*}
Most importantly, we see from above that, for stratified simple random sampling with proportional allocation, the following implication holds:
\begin{equation*}
\sum_{h=1}^{H}\left(1-\dfrac{N_{h}}{N}\right)\!S^{2}_{h} \,\leq\, \textnormal{SSB}
\quad\Longrightarrow\quad
\Var\!\left[\;\widehat{t}_{\textnormal{PropStr}}\;\right]
\leq
\Var\!\left[\;\widehat{t}_{\textnormal{SRS}}\;\right]\,.
\end{equation*}
\emph{In heuristic terms, in proportional-allocation stratification for which each stratum is relatively homogeneous and the strata are relatively dissimilar to each other (intra-strata variation being smaller than inter-strata variation), then the unbiased estimator for the population total from the proportional-allocation stratified simple random sampling is more precise than that from SRS.
}

%\cite{KrewskiRao1981}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Two-stage Cluster Sampling}
\setcounter{theorem}{0}

The universe $\mathcal{U} = \displaystyle{\bigsqcup_{i=1}^{N}}\,\mathcal{C}_{i}$ of observation units is partitioned into $N$ \emph{clusters} (or \emph{primary sampling units}, psu's) $\mathcal{C}_{i}$.  In two-stage cluster sampling, the \emph{secondary sampling units} (or ssu's) are the observation units.  Let $M_{i}$ be the number of ssu's in the $i$th psu; in other words, $M_{i} := \#(\mathcal{C}_{i})$. \\

\noindent
\textbf{First Stage:} \; Select a simple random sample (SRS) $\omega_{1} = \left\{\,\mathcal{C}_{i_{1}},\mathcal{C}_{i_{2}},\ldots,\mathcal{C}_{i_{n}}\,\right\}$ of $n$ psu's from the collection of $N$ psu's. \\

\noindent
\textbf{Second Stage:} \; From each psu $\mathcal{C} \in \omega_{1}$ selected in the First Stage, select a simple random sample (SRS) $\omega_{\mathcal{C}}$ of $m_{i}$ secondary sampling units (ssu's) from the collection of $M_{i}$ ssu's in $\mathcal{C}$. \\

\noindent
The sample is then $\omega := \displaystyle{\bigsqcup_{\mathcal{C}\in\omega_{1}}}\,\omega_{\mathcal{C}}$. \,In other words, the sample $\omega$ consists of all the secondary sampling units (or observation units) selected (during the Second Stage) from all the primary sampling units selected in the First Stage. \\

\noindent
The Horvitz-Thompson estimator $\widehat{t}_{\textnormal{HT}}$, as defined below, is an unbiased estimator for the total of an $\Re$-valued population characteristic $y : \mathcal{U} \longrightarrow \Re$.

\begin{eqnarray*}
\widehat{t}_{\textnormal{HT}}
& := & \sum_{k\in\omega}\left(\dfrac{N}{n}\dfrac{M_{y_{k}}}{m_{y_{k}}}\right)y_{k}
\;\;=\;\; \sum_{k\in\omega}\left(\dfrac{1}{\pi_{k}}\right)y_{k}
\;\;=\;\; \sum_{\mathcal{C}\in\omega_{1}} \sum_{k\in\omega_{\mathcal{C}}}\left(\dfrac{N}{n}\dfrac{M_{y_{k}}}{m_{y_{k}}}\right)y_{k},
\end{eqnarray*}
where $M_{y_{k}} := M_{i} := \#(\mathcal{C}_{i})$ and $m_{y_{k}} := m_{i} := \#(\omega_{\mathcal{C}_{i}})$ such that $\mathcal{C}_{i}$ is the unique psu containing the ssu $k \in \mathcal{U} = \displaystyle{\bigsqcup_{i}^{N}}\,\mathcal{C}_{i}$.  The variance of the Horvitz-Thompson estimator $\widehat{t}_{\textnormal{HT}}$ is given by:
\begin{eqnarray*}
%\widehat{t}_{\textnormal{HT}}
%& := & \sum_{k\in\omega}\left(\dfrac{N}{n}\dfrac{M_{y_{k}}}{m_{y_{k}}}\right)y_{k}
%\;\;=\;\; \sum_{k\in\omega}\left(\dfrac{1}{\pi_{k}}\right)y_{k}
%\;\;=\;\; \sum_{\mathcal{C}\in\omega_{1}} \sum_{k\in\mathcal{C}}\left(\dfrac{N}{n}\dfrac{M_{y_{k}}}{m_{y_{k}}}\right)y_{k}
%\\
\Var\!\left(\,\widehat{t}_{\textnormal{HT}}\,\right)
& = & N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}_{t}}{n} \; + \; 
\sum^{N}_{i=1}\dfrac{N}{n}\cdot M_{i}^{2}\left(1-\dfrac{m_{i}}{M_{i}}\right)\dfrac{S^{2}_{i}}{m_{i}},
\end{eqnarray*}
where
\begin{equation*}
S_{t}^{2} \; := \; \dfrac{1}{N-1}\sum^{N}_{i=1}\left(t_{i}-\dfrac{t}{N}\right)^{2}
\,,\quad
S_{i}^{2} \; := \; \dfrac{1}{M_{i}-1}\sum^{M_{i}}_{j=1}\left(y_{j}-\dfrac{t_{i}}{M_{i}}\right)^{2}
\,,\quad
t \; := \; \sum_{k\in\mathcal{U}}\,y_{k}
\,,\quad\textnormal{and}\quad
t_{i} \; := \; \sum_{k\in\mathcal{C}_{i}}\,y_{k}
\end{equation*}

\noindent
\textbf{IMPORTANT OBSERVATION:}\;\;
The first summand in the expression of $\Var\!\left(\,\widehat{t}_{\textnormal{HT}}\,\right)$ is due to variability in the First-Stage sampling, whereas the second summand is due to variability in the Second-Stage sampling.

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{One-stage Cluster Sampling}
\setcounter{theorem}{0}

One-stage cluster sampling is a special form of two-stage cluster sampling in which all second-stage samples are censuses.  In other words, following the notation introduced for two-stage cluster sampling, in one-stage cluster sampling, we have $\omega_{\mathcal{C}} = \mathcal{C}$, for each first-stage-selected $\mathcal{C} \in \omega_{1}$.  This also implies $m_{i} = M_{i}$ for each $i = 1, 2, \ldots, N$. \\

Then, the Horvitz-Thompson estimator $\widehat{t}_{\textnormal{HT}}$ and its variance reduces to:
\begin{eqnarray*}
\widehat{t}_{\textnormal{HT}}
& := & \sum_{\mathcal{C}\in\omega_{1}} \sum_{k\in\mathcal{C}}\left(\dfrac{N}{n}\dfrac{M_{y_{k}}}{m_{y_{k}}}\right)y_{k}
\;\;=\;\; \dfrac{N}{n}\cdot\sum_{\mathcal{C}\in\omega_{1}} \sum_{k\in\mathcal{C}}\,y_{k} 
\;\;=\;\; \dfrac{N}{n}\cdot\sum_{\mathcal{C}\in\omega_{1}} t_{\mathcal{C}}\,,
\quad\textnormal{where}\;\; t_{\mathcal{C}} := \sum_{k\in\mathcal{C}}y_{k} \\
\\
\Var\!\left(\,\widehat{t}_{\textnormal{HT}}\,\right)
& = &
N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}_{t}}{n} \; + \; 
\sum^{N}_{i=1}\dfrac{N}{n}\cdot M_{i}^{2}\left(1-\dfrac{m_{i}}{M_{i}}\right)\dfrac{S^{2}_{i}}{m_{i}} \\
& = &
N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}_{t}}{n} \; + \; 
\sum^{N}_{i=1}\dfrac{N}{n}\cdot M_{i}^{2}\left(1-1\right)\dfrac{S^{2}_{i}}{m_{i}}
\;\;=\;\;
N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}_{t}}{n}
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Stratified Simple Random Sampling as a special case of Two-stage Cluster Sampling}
\setcounter{theorem}{0}

Stratified simple random sampling is a special case of two-stage cluster sampling in which the first-stage sampling is a census.  In other words, if $\mathcal{U} = \displaystyle{\bigsqcup_{i=1}^{N}}\,\mathcal{C}_{i}$, then $\omega_{1} = \left\{\,\mathcal{C}_{1},\mathcal{C}_{2},\ldots,\mathcal{C}_{N}\,\right\}$.  In particular, $n = N$.

Then, the Horvitz-Thompson estimator $\widehat{t}_{\textnormal{HT}}$ and its variance reduces to:
\begin{eqnarray*}
\widehat{t}_{\textnormal{HT}}
& := & \sum_{\mathcal{C}\in\omega_{1}} \sum_{k\in\omega_{\mathcal{C}}}\left(\dfrac{N}{n}\dfrac{M_{y_{k}}}{m_{y_{k}}}\right)y_{k}
\;\;=\;\;  \sum_{i=1}^{N} M_{i}\left(\dfrac{1}{m_{i}} \sum_{k\in\omega_{\mathcal{C}_{i}}}y_{k}\right) 
\;\;=\;\;  \sum_{i=1}^{N} M_{i}\,\overline{y}_{\omega_{\mathcal{C}_{i}}}
\\
\Var\!\left(\,\widehat{t}_{\textnormal{HT}}\,\right)
& = &
N^{2}\left(1-\dfrac{n}{N}\right)\dfrac{S^{2}_{t}}{n} \; + \; 
\sum^{N}_{i=1}\dfrac{N}{n}\cdot M_{i}^{2}\left(1-\dfrac{m_{i}}{M_{i}}\right)\dfrac{S^{2}_{i}}{m_{i}} \\
& = &
N^{2}\left(1-1\right)\dfrac{S^{2}_{t}}{n} \; + \; 
\sum^{N}_{i=1}1\cdot M_{i}^{2}\left(1-\dfrac{m_{i}}{M_{i}}\right)\dfrac{S^{2}_{i}}{m_{i}}
\;\;=\;\;
\sum^{N}_{i=1}\,M_{i}^{2}\left(1-\dfrac{m_{i}}{M_{i}}\right)\dfrac{S^{2}_{i}}{m_{i}}
\end{eqnarray*}
The above formula agree exactly with those derived earlier for stratified simple random sampling (apart from obvious notational changes).

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{General linear estimators for (multivariate) population totals}
\setcounter{theorem}{0}

Let $U = \{1,2,\ldots,N\}$ be a finite population.
Let $\mathbf{y} : U \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued function defined on $U$
(commonly called a ``population parameter").
We will use the common notation $\mathbf{y}_{k}$ for $\mathbf{y}(k)$.
We wish to estimate
$\mathbf{T}_{\mathbf{y}} := \underset{k \in U}{\sum}\,\mathbf{y}_{k} \in \Re^{m}$
via survey sampling.
Let $p:\mathcal{S} \longrightarrow (0,1]$ be our chosen sampling design,
where $\mathcal{S} \subseteq \mathcal{P}(U)$ is the set of all possible
samples in the design, and $\mathcal{P}(U)$ is the power set of $U$.

\begin{definition}
\mbox{}
\vskip 0.1cm
\noindent
A random variable $\widehat{\mathbf{T}}_{\mathbf{y}} : \mathcal{S} \longrightarrow \Re^{m}$
is said to be \underline{\emph{linear} in the population parameter $\mathbf{y} : U \longrightarrow \Re$}
if it has the following form:
\begin{equation*}
\begin{array}{cccl}
\widehat{\mathbf{T}}_{\mathbf{y}} : & \mathcal{S} & \longrightarrow & \Re^{m} \\
     & s &\longmapsto & \underset{k\in s}{\sum}\,w_{k}(s)\,\mathbf{y}_{k} \;=\; \underset{k\in U}{\sum}\,I_{k}(s)\,w_{k}(s)\,\mathbf{y}_{k},
\end{array}
\end{equation*}
where, for each $k \in U$, $w_{k} : \mathcal{S} \longrightarrow \Re$ is itself
an $\Re$-valued random variable, and $I_{k} : \mathcal{S} \longrightarrow \{0,1\}$
is the indicator random variable defined by:
\begin{equation*}
I_{k}(s)
\;=\;
\left\{
\begin{array}{cl}
1, & \textnormal{if $k \in s$}, \\
0, & \textnormal{otherwise}
\end{array}
\right.
\end{equation*}
We call the $w_{k}$'s the \underline{\emph{weights}} of $\widehat{\mathbf{T}}_{\mathbf{y}}$,
and we use the notation $\widehat{\mathbf{T}}_{\mathbf{y};w}$ to indicate that the random
variable depends on the weights $w_{k}$.
\end{definition}

\noindent
\textbf{Nomenclature}
\;
In the context of finite-population probability sampling, under a design
$p : \mathcal{S} \longrightarrow (0,1]$,
an ``estimator" is precisely just a random variable defined on the space
$\mathcal{S}$ of all admissible samples in the design.
%When we speak of a random variable $\widehat{\theta} : \mathcal{S} \longrightarrow \Re$
%as an estimator for a certain population characteristic $\theta \in \Re$, we are merely
%conveying the connotation that $\widehat{\theta}$ is supposed to have an expected
%value equal or close to $\theta$ and that its variance is supposed to be sufficiently small
%that we have some degree of confidence that an actual observed value of $\widehat{\theta}$,
%i.e. $\widehat{\theta}(s)$ where $s \in \mathcal{S}$ is the actual obtained sample, should be
%reasonably close to the actual value of $\theta$.

\begin{proposition}
\label{proposition:Unbiasedness}
\mbox{}
\vskip 0.2cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re^{m}$, with
$\widehat{\mathbf{T}}_{\mathbf{y};w}(s)$
$=$ $\underset{k \in U}{\sum}\,I_{k}(s)\,w_{k}(s)\,\mathbf{y}_{k}$
$=$ $\underset{k \in s}{\sum}\,w_{k}(s)\,\mathbf{y}_{k}$,
be a random variable linear in the population parameter
$\mathbf{y} : U \longrightarrow \Re$.
Then,
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right] \; = \; \mathbf{T}_{\mathbf{y}},\;\textnormal{for arbitrary $\mathbf{y}$}
\quad\Longleftrightarrow\quad
E\!\left[\;I_{k}\,w_{k}\;\right] \; = \; 1,\;\textnormal{for each $k \in U$}.
\end{equation*}
\end{proposition}
\proof
Note:
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;=\; E\!\left[\;\underset{k \in s}{\sum}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; E\!\left[\;\underset{k \in U}{\sum}\,I_{k}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; \underset{k \in U}{\sum}\,E\!\left[\,I_{k}\,w_{k}\,\right]\,\mathbf{y}_{k}
\end{equation*}
Hence, since $\mathbf{y} : U \longrightarrow \Re$ is arbitrary,
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right] \;=\; \mathbf{T}_{\mathbf{y}} \; := \; \underset{k \in U}{\sum}\,\mathbf{y}_{k}
\quad\Longleftrightarrow\quad
\sum_{k \in U}\left(E\!\left[\,I_{k}\,w_{k}\,\right]-1\right)\cdot\mathbf{y}_{k} \; = \; \mathbf{0}
\quad\Longleftrightarrow\quad
E\!\left[\;I_{k}\,w_{k}\;\right] \; = \; 1,\;\textnormal{for each $k \in U$}.
\end{equation*}
The proof of the Proposition is now complete. \qed

\begin{corollary}
\mbox{}
\vskip 0.2cm
\noindent
Let $U = \{1,2,\ldots,N\}$ be a finite population.
For any fixed but arbitrary population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$ and
for any sampling design $p : \mathcal{S} \longrightarrow (0,1]$ such that each of
its first-order inclusion probabilities is strictly positive,
the Horvitz-Thompson estimator $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$ is well-defined
and it is the unique unbiased estimator for $\mathbf{T}_{\mathbf{y}}$, which is linear in $\mathbf{y}$ and
whose weights are constant in $s$.
\end{corollary}

\proof
Recall that the Horvitz-Thompson estimator is defined as:
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}(s)
\;:=\; \sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\mathbf{y}_{k}
\;:=\; \sum_{k \in U}\,I_{k}(s)\,\dfrac{1}{\pi_{k}}\,\mathbf{y}_{k},
\end{equation*}
where $\pi_{k} := E\!\left[\,I_{k}\,\right] = \underset{k \in U}{\sum}\,p(s)\,I_{k}(s) = \underset{s \ni k}{\sum}\,p(s)$
is the inclusion probability of $k \in U$ under the sampling design $p : \mathcal{S} \longrightarrow (0,1]$.
Clearly, $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$ is linear in $\mathbf{y}$ with weights constant in $s$.
Next, note that:
\begin{equation*}
E\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\mathbf{y}_{k}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in U}\,I_{k}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}\;\right]
\;\;=\;\; \sum_{k \in U}\,E\!\left[\;I_{k}\;\right]\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\;\;=\;\; \sum_{k \in U}\,\pi_{k}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\;\;=\;\; \sum_{k \in U}\,\mathbf{y}_{k} \;\; = \;\; \mathbf{T}_{y}
\end{equation*}
Hence, $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$ is an unbiased estimator for $\mathbf{T}_{\mathbf{y}}$.
Conversely, let
\begin{equation*}
\widehat{\mathbf{T}}_{y;w}(s) \; = \; \sum_{k \in s}\,w_{k}\,\mathbf{y}_{k}
\end{equation*}
be any unbiased estimator for $\mathbf{T}_{\mathbf{y}}$ which linear in $\mathbf{y}$ with weights $w_{k}$ constant in $s$.
Thus,
\begin{equation*}
\sum_{k \in U}\,\mathbf{y}_{k}
\;=\; \mathbf{T}_{\mathbf{y}}
\;=\; E\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;=\; E\!\left[\;\sum_{k \in s}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; E\!\left[\;\sum_{k \in U}\,I_{k}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;=\; \sum_{k \in U}\,E\!\left[\;I_{k}\;\right]\,w_{k}\,\mathbf{y}_{k}
\;=\; \sum_{k \in U}\,\pi_{k}\,w_{k}\,\mathbf{y}_{k}.
\end{equation*}
Since $\mathbf{y}$ is arbitrary, the above equation immediately implies that
\begin{equation*}
\pi_{k}w_{k} - 1 = 0,
\end{equation*}
or equivalently, $w_{k} = \dfrac{1}{\pi_{k}}$; in other words,
$\widehat{\mathbf{T}}_{\mathbf{y};w}$ is in fact equal to the Horvitz-Thompson estimator.
The proof of the Corollary is now complete.
\qed

\begin{lemma}
\label{lemma:technical}
\mbox{}
\vskip 0.1cm
\noindent
Let $(\Omega,\mathcal{A},p)$ be a probability space,
$X, Y : \Omega \longrightarrow \Re$ be two $\Re$-valued random variables defined on $\Omega$,
and $\mathbf{u}, \mathbf{v} \in \Re^{m}$ be two fixed vectors in $\Re^{m}$.
Then,
\begin{equation*}
\Cov\!\left(\,X\cdot\mathbf{u}\,,\,Y\cdot\mathbf{v}\,\right)
\;\;=\;\;
\Cov(X,Y)\cdot\mathbf{u}\cdot\mathbf{v}^{T}
\;\;\in\;\; \Re^{m \times m}
\end{equation*}
\end{lemma}

\proof
Note:
\begin{eqnarray*}
\Cov\!\left(\;X\cdot\mathbf{u}\;,\;Y\cdot\mathbf{v}\;\right)
&:=& E\!\left[\;\left(X\,\mathbf{u} - \mu_{X}\mathbf{u}\right)\cdot\left(Y\,\mathbf{v} - \mu_{Y}\mathbf{v}\right)^{T}\;\right]
\;\;=\;\; E\!\left[\;\left(X - \mu_{X}\right)\mathbf{u}\cdot\left(Y - \mu_{Y}\right)\mathbf{v}^{T}\;\right] \\
&=& E\!\left[\;\left(X - \mu_{X}\right)\left(Y - \mu_{Y}\right)\cdot\mathbf{u}\cdot\mathbf{v}^{T}\;\right] 
\;\;=\;\; E\!\left[\;\left(X - \mu_{X}\right)\left(Y - \mu_{Y}\right)\;\right]\cdot\mathbf{u}\cdot\mathbf{v}^{T} \\
&=& \Cov(X,Y)\cdot\mathbf{u}\cdot\mathbf{v}^{T},
\end{eqnarray*}
as required.
\qed

\begin{proposition}
\label{proposition:generalLinear:Var}
\mbox{}
\vskip 0.2cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re$,
with $\widehat{\mathbf{T}}_{\mathbf{y};w}(s)$
$=$ $\underset{k \in s}{\sum}\,w_{k}(s)\,\mathbf{y}_{k}$
$=$ $\underset{k \in U}{\sum}\,I_{k}(s)\,w_{k}(s)\,\mathbf{y}_{k}$,
be a random variable linear in the population parameter $\mathbf{y} : U \longrightarrow \Re$.
Then, the covariance matrix of $\widehat{\mathbf{T}}_{\mathbf{y};w}$ is given by:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;\;=\;\;\sum_{i\in U}\,\sum_{k\in U}\,\Cov\!\left[\;I_{i}\,w_{i}\,,\,I_{k}\,w_{k}\;\right]\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\; \in \Re^{m \times m}
\end{equation*}
%where
%\begin{equation*}
%a_{i}\; := \;\Var\left[\;I_{i}\,w_{i}\;\right],
%\quad
%a_{ij} \; := \; \textnormal{Cov}\left[\;I_{i}w_{i}\,,\,I_{j}w_{j}\;\right].
%\quad\textnormal{and}\quad
%I_{i}(s)
%\;=\;
%\left\{
%\begin{array}{cl}
%1\,, & \textnormal{if $i \in s$} \\
%0\,, & \textnormal{otherwise}
%\end{array}.
%\right.
%\end{equation*}
%Furthermore, if $\widehat{T}_{y;w}$ is an unbiased estimator for $T_{y} := \underset{k \in U}{\sum}y_{k}$
%for arbitrary $y$, then
%\begin{equation*}
%a_{i} \; = \; \Var\!\left[\;I_{i}w_{i}\;\right]
%\; = \; \left(\sum_{s\in\mathcal{S}}\,p(s)\,I_{i}(s)\,w_{i}^{2}(s)\right) - 1
%\end{equation*}
Furthermore, if the first-order and second-order inclusion probabilities of the sampling design
$p : \mathcal{S} \longrightarrow (0,1]$ are all strictly positive,
i.e. $\pi_{k} = \pi_{kk} := \underset{s \ni k}{\sum}\,p(s) > 0$, for each $k \in U$, and
$\pi_{ik} := \underset{s \ni i,k}{\sum}\,p(s) > 0$, for any distinct $i,k \in U$,
then an unbiased estimator for $\Var\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]$ is given by:
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]\!(s)
\;:=\; \sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;=\; \sum_{k \in s}\dfrac{\Var(I_{k}w_{k})}{\pi_{k}}\,\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}
       + \underset{i \neq k}{\sum_{i,k \in s}}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T},
\;\;\textnormal{for each $s \in \mathcal{S}$}.
\end{equation*}
\end{proposition}

\proof
First, note that Lemma \ref{lemma:technical} implies:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]
\;\;=\;\; \Cov\!\left[\;\sum_{i\in U}\,I_{i}\,w_{i}\,\mathbf{y}_{i}\;,\;\sum_{k\in U}\,I_{k}\,w_{k}\,\mathbf{y}_{k}\;\right]
\;\;=\;\; \sum_{i\in U}\,\sum_{k\in U}\,\Cov\!\left[\;I_{i}\,w_{i}\,,\,I_{k}\,w_{k}\;\right]\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;\in\;\;\Re^{m \times m}
\end{equation*}
Next,
\begin{eqnarray*}
E\!\left(\;\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]\;\right)
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w}\;\right]\!(s)
\;\;=\;\; \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(\sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}\right) \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot
\left(\sum_{i,k \in U}I_{i}(s)I_{k}(s)\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}\right) \\
&=& \sum_{i,k \in U}\left(\sum_{s\in\mathcal{S}}\,p(s)I_{i}(s)I_{k}(s)\right)\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{i,k \in U}\left(\sum_{s\ni i, k}\,p(s)\right)\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{i,k \in U}\,\pi_{ik}\cdot\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{i,k \in U}\,\Cov(I_{i}w_{i},I_{k}w_{k})\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \Var\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]
\end{eqnarray*}
Lastly, recall that $\pi_{kk} = \pi_{k}$ and $\Cov(I_{k}w_{k},I_{k}w_{k}) = \Var[\,I_{k}w_{k}\,]$,
and the validity of the following identity is thus trivial:
\begin{equation*}
\sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{k \in s}\dfrac{\Var(I_{k}w_{k})}{\pi_{k}}\cdot\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}
       + \underset{i \neq k}{\sum_{i,k \in s}}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\end{equation*}
The proof of the Proposition is complete.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Unbiased variance estimators for the Horvitz-Thompson Estimator}
\setcounter{theorem}{0}

Let $U = \{1,2,\ldots,N\}$ be a finite population.
Let $\mathbf{y} = (y_{1},y_{2},\ldots,y_{m}) : U \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued function defined on $U$
(commonly called a ``population parameter").
We will use the common notation $\mathbf{y}_{k}$ for $\mathbf{y}(k)$.
We wish to estimate
$\mathbf{T}_{\mathbf{y}} := \underset{k \in U}{\sum}\,\mathbf{y}_{k} \in \Re^{m}$
via survey sampling.
Let $p:\mathcal{S} \longrightarrow (0,1]$ be our chosen sampling design,
where $\mathcal{S} \subseteq \mathcal{P}(U)$ is the set of all possible
samples in the design, and $\mathcal{P}(U)$ is the power set of $U$.

\begin{proposition}
\mbox{}
\vskip 0.1cm
\noindent
Suppose the first-order and second-order inclusion probabilities of $p:\mathcal{S}\longrightarrow(0,1]$
are all strictly positive, i.e.
\begin{equation*}
\pi_{k} := \sum_{s \ni k}\,p(s) = \sum_{k \in U}\,I_{k}(s)p(s) > 0
\quad\textnormal{and}\quad
\pi_{ik} := \sum_{s \ni i,k}\,p(s) = \sum_{i,k \in U}\,I_{i}(s)I_{k}(s)p(s) > 0,
\end{equation*}
for any $i,k \in U$.
Then, an unbiased estimator for the covariance matrix of the Horvitz-Thompson estimator
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}(s)
\;\;:=\;\;
\sum_{k \in s} \dfrac{1}{\pi_{k}}\,\mathbf{y}_{k}
\end{equation*}
is given by:
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\;\right]\!(s)
\;\;=\;\;
\sum_{i,k \in s}\left(\dfrac{\pi_{ik} - \pi_{i}\pi_{k}}{\pi_{ik}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T},
\;\;\textnormal{for each $s \in \mathcal{S}$}.
\end{equation*}
\end{proposition}

\proof
By Proposition \ref{proposition:generalLinear:Var},
for any random variable (a.k.a. estimator) $\widehat{\mathbf{T}}_{\mathbf{y};w}$
linear in the population parameter $\mathbf{y} : \mathcal{S} \longrightarrow \Re^{m}$
with weights $w_{k} : \mathcal{S} \longrightarrow \Re$, $k \in U$, the following
\begin{equation}
\label{eqn:generalLinear:VarEstimator}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{y;w}\;\right]\!(s)
\;:=\; \sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\end{equation}
always gives an unbiased estimator for the covariance matrix of $\widehat{\mathbf{T}}_{y;w}$.
For the Horvitz-Thompson estimator, the weights are $w_{k} = 1/\pi_{k}$, for each $k \in U$,
and the weights are independent of the sample $s \in \mathcal{S}$.
Thus, for the Horvitz-Thompson estimator, the right-hand side of equation
\eqref{eqn:generalLinear:VarEstimator} becomes:
\begin{eqnarray*}
\sum_{i,k \in s}\dfrac{\Cov(I_{i}w_{i},I_{k}w_{k})}{\pi_{ik}}\,\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
&=&\sum_{i,k \in s}\dfrac{\Cov(I_{i},I_{k})}{\pi_{ik}}\,\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T} \\
&=&\sum_{i,k \in s}\dfrac{E(I_{i}I_{k})-E(I_{i})E(I_{k})}{\pi_{ik}}\,\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T} \\
&=&\sum_{i,k \in s}\dfrac{\pi_{ik}-\pi_{i}\pi_{k}}{\pi_{ik}}\,\left(\dfrac{\mathbf{y}_{i}}{\pi_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{\pi_{k}}\right)^{T},
\end{eqnarray*}
which coincides with the right-hand side of the equation of the conclusion of the present Proposition.
Thus this present Proposition is but a special case of Proposition \ref{proposition:generalLinear:Var},
specialized to the Horvitz-Thompson estimator, and the proof is now complete. \qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Estimation of Domain Totals}
\setcounter{theorem}{0}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Calibrated linear estimators for (multivariate) population totals}
\setcounter{theorem}{0}

\begin{definition}
\mbox{}
\vskip 0.1cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re^{m}$
be an $\Re^{m}$-valued random variable which is linear in the $\Re^{m}$-valued
population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$, i.e.
\begin{equation*}
\begin{array}{cccl}
\widehat{\mathbf{T}}_{\mathbf{y};w} : & \mathcal{S} & \longrightarrow & \Re^{m} \\
     & s &\longmapsto & \underset{k\in s}{\sum}\,w_{k}(s)\cdot\mathbf{y}_{k} \;\; = \;\; \underset{k\in U}{\sum}\,I_{k}(s)\,w_{k}(s)\cdot\mathbf{y}_{k},
\end{array}
\end{equation*}
where, for each $k \in U$, $w_{k} : \mathcal{S} \longrightarrow \Re$ is itself an $\Re$-valued random variable,
and $I_{k} : \mathcal{S} \longrightarrow \{0,1\}$ is the indicator random variable defined by:
\begin{equation*}
I_{k}(s)
\;\;=\;\;
\left\{
\begin{array}{cl}
1, & \textnormal{if $k \in s$}, \\
0, & \textnormal{otherwise}
\end{array}
\right.
\end{equation*}
Let $x : U \longrightarrow \Re$ be an $\Re$-valued population parameter
and  $T_{x} := \underset{k\in U}{\sum}\,x_{k}$.\\
Then, $\widehat{\mathbf{T}}_{\mathbf{y};w}$ is said to be
\underline{\emph{calibrated with respect to $x$}} if
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k} \; = \; T_{x},
\;\;\textnormal{for {\color{red}each} $s \in \mathcal{S}$}.
\end{equation*}
\end{definition}

\begin{example}
\mbox{}
\vskip 0.1cm
\noindent
If the sampling design has fixed sample size and each of its first-order inclusion probabilities is strictly positive, then Horvitz-Thompson estimator is calibrated with respect to the first-order inclusion probabilities.
\vskip 0.2cm
\noindent
To see this, let $U = \{1,2,\ldots,N\}$ be a finite population,
$\mathbf{y} : U \longrightarrow \Re^{m}$ a population parameter, and
$p : \mathcal{S}\subset\mathcal{P}(U) \longrightarrow (0,1]$ a sampling design
such that $\pi_{k} := \sum_{s \ni k}p(s) > 0$, for each $k \in U$.
The Horvitz-Thompson estimator
$\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}} : \mathcal{S} \longrightarrow \Re$
is then well-defined and is given by:
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\!(s)
\;\; := \;\; \sum_{k \in s}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\end{equation*}
Let $x : U \longrightarrow \Re$ be defined by
\begin{equation*}
x_{k} \; = \; \pi_{k},
\;\;\textnormal{for each $k \in U$},
\end{equation*}
i.e. $x_{k}$ is simply the inclusion probability of $k \in U$
under the sampling design $p : \mathcal{S} \longrightarrow (0,1]$.

Now, suppose that the sampling design has a fixed sample size $n$,
and we shall show that $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$
is consequently calibrated with respect to $x : U \longrightarrow \Re$.
Indeed, recall that the weights of the Horvitz-Thompson estimator are simply
$w_{k}(s) = 1 / \pi_{k}$, for each $k \in U$ and each $s \in \mathcal{S}$.
Hence,
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k}
\;\;=\;\; \sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\pi_{k}
\;\;=\;\; \sum_{k \in s}\,1
\;\;=\;\; \left(\begin{array}{c} \textnormal{sample} \\ \textnormal{size of $s$} \end{array}\right)
\;\;=\;\; n,
\end{equation*}
since the sampling design has fixed size $n$.
On the other hand,
\begin{equation*}
T_{x}
\;\;=\;\;\sum_{k\in U}x_{k}
\;\;=\;\; \sum_{k \in U}\pi_{k}
\;\;=\;\; \sum_{k \in U}E\!\left[\;I_{k}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in U}I_{k}\;\right]
\;\;=\;\; E\!\left[\begin{array}{c} \textnormal{sample} \\ \textnormal{size} \end{array}\right]
\;\;=\;\; n,
\end{equation*}
again since the sample size is fixed and equals $n$.
Therefore, we have, for any $s \in \mathcal{S}$,
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k}
\;\;=\;\; n
\;\;=\;\; T_{x}
\end{equation*}
Therefore, the Horvitz-Thompson estimator, under the assumption of fixed sample size,
is indeed calibrated with respect to the inclusion probabilities $x : U \longrightarrow \Re$,
$x _{k} = \pi_{k} := \sum_{s \ni k}\,p(s)$, for each $k \in U$. \qed
\end{example}

\begin{proposition}
\label{proposition:calibratedLinear:MSE}
\mbox{}
\vskip 0.1cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w,x} : \mathcal{S} \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued random variable 
which is linear in the $\Re^{m}$-valued population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$
and calibrated with respect to the population parameter $x : U \longrightarrow \Re$, with $x_{k} \neq 0$ for each $k \in U$.\\
Then, the mean squared error matrix of $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ as an estimator of $\mathbf{T}_{\mathbf{y}}$ is given by:
\begin{equation*}
\MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
\;=\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\;\in\;\Re^{m \times m},
\;\;\textnormal{where}\;\;
a_{ik} \;:=\; E\!\left[\,\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\,\right].
\end{equation*}
\end{proposition}

\proof
\begin{eqnarray*}
\MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
&=& E\!\left[\;\left(\widehat{\mathbf{T}}_{\mathbf{y};w,x} - \mathbf{T}_{\mathbf{y}}\right)\cdot\left(\widehat{\mathbf{T}}_{\mathbf{y};w,x} - \mathbf{T}_{\mathbf{y}}\right)^{T}\;\right]
\;\;=\;\; E\!\left[\;\left(\sum_{i\in U}(I_{i}w_{i}-1)\,\mathbf{y}_{i}\right)\cdot\left(\sum_{k\in U}(I_{k}w_{k}-1)\,\mathbf{y}_{k}\right)^{T}\;\right] \\
&=& \sum_{i\in U}\,\sum_{k\in U}\,E\!\left[\;\left(I_{i}w_{i}-1\right)\left(I_{k}w_{k}-1\right)\;\right]\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{k \in U}\,a_{kk}\cdot\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T} + \underset{i\neq k}{\sum_{i,k\in U}}\,a_{ik}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{k \in U}\,a_{kk}\left(\dfrac{\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}}{x_{k}^{2}}\right)x_{k}^{2}
+ \underset{i\neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\,x_{i}\,x_{k}
\end{eqnarray*}
On the other hand,
\begin{eqnarray*}
&&   - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\,x_{i}\,x_{k} \\
&=& - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left[\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
- \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
- \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\right]\,x_{i}\,x_{k} \\
&=& - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left[
    \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
+ \underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\,x_{i}\,x_{k}
\end{eqnarray*}
Thus, the proof of the present Proposition will be complete once we show:
\begin{equation*}
\underset{
\dfrac{1}{2}\,\underset{i {\color{red}=} k}{\underset{i,k\in U}{\textnormal{\large$\sum$}}}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
}{
\underbrace{\sum_{k \in U}\,a_{kk}\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}x_{k}^{2}}
}
\;\;=\;\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k},
\end{equation*}
which is equivalent to:
\begin{equation}\label{MSE}
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
\;\;=\;\; 0.
\end{equation}
Observe that
\begin{eqnarray*}
\textnormal{LHS\eqref{MSE}}
&=&
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}x_{i}\,x_{k}
+
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}x_{i}\,x_{k} \\
&=&
2\,\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}x_{i}\,x_{k}
\;\;=\;\; 2\,\sum_{i\in U}\,x_{i}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}\left(\sum_{k\in U}\,a_{ik}x_{k}\right).
\end{eqnarray*}
Hence, \eqref{MSE} follows once we show
\begin{equation}\label{MSE2}
\sum_{k\in U}\,a_{ik}x_{k} = 0,
\quad\textnormal{for each $i \in U$.}
\end{equation}
Lastly, we now claim that \eqref{MSE2} follows from the hypothesis that
$\widehat{T}_{y;w;x}$ is calibrated with respect to $x$.
Indeed,
\begin{eqnarray*}
\sum_{k \in U}\,a_{ik}x_{k}
&=& \sum_{k\in U}\,E\!\left[\;(I_{i}w_{i}-1)(I_{k}w_{k}-1)\;\right] x_{k}
\;\;=\;\; \sum_{k\in U}\,\left[\;\sum_{s\in\mathcal{S}}p(s)(I_{i}(s)w_{i}(s)-1)(I_{k}(s)w_{k}(s)-1)\;\right] x_{k} \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(I_{i}(s)w_{i}(s)-1\right)\cdot\left[\;\sum_{k\in U}\left(I_{k}(s)w_{k}(s)-1\right)\cdot x_{k}\;\right] \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(I_{i}(s)w_{i}(s)-1\right)\cdot\underset{0}{\left[\underbrace{\;\left(\sum_{k\in s}w_{k}(s)\,x_{k}\right) - T_{x}}\;\right]}\\
&=& 0
\end{eqnarray*}
The proof of the present Proposition is now complete. \qed

\begin{proposition}[The Yates-Grundy-Sen Variance Estimator for calibrated linear population total estimators]
\mbox{}
\vskip 0.1cm
\noindent
Let $p : \mathcal{S} \longrightarrow (0,1]$ be a sampling design each of whose
first-order and second-order inclusion probabilities is strictly positively.
Let $\widehat{\mathbf{T}}_{\mathbf{y};w,x} : \mathcal{S} \longrightarrow \Re^{m}$
be a random variable  which is linear in the population parameter
$\mathbf{y} : U \longrightarrow \Re^{m}$
and calibrated with respect to the population parameter $x : U \longrightarrow \Re$,
with $x_{k} \neq 0$ for each $k \in U$.
Suppose that $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ is an unbiased estimator for
$\mathbf{T}_{\mathbf{y}} := \underset{k \in U}{\sum}\mathbf{y}_{k}$, for arbitrary $\mathbf{y}$.
Then, the following is an unbiased estimator of the variance
$\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]$ of $\widehat{\mathbf{T}}_{\mathbf{y};w,x} $:
For each $s \in \mathcal{S}$ admissible in the sampling design $p : \mathcal{S} \longrightarrow (0,1]$,
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]\!(s)
\;\;:=\;\;
-\dfrac{1}{2}\underset{i\neq k}{\sum_{i,k\in s}}\left(w_{i}(s)w_{k}(s) - \dfrac{1}{\pi_{ik}}\right)
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\end{equation*}
\end{proposition}

\noindent
\textbf{Terminology:}\quad
$\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]$ is called the Yates-Grundy-Sen Variance Estimator.

\vskip 0.5cm
\proof
Since $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ is an unbiased estimator for $\mathbf{T}_{\mathbf{y}}$ by hypothesis, we have
$\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right] = \MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]$.
By Proposition \ref{proposition:calibratedLinear:MSE}, we thus have:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
\;\;=\;\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{2}\,x_{i}\,x_{k},
\quad\textnormal{where}\;\;
a_{ik} \;:=\; E\!\left[\;\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\;\right].
\end{equation*}
On the other hand,
\begin{eqnarray*}
E\!\left(\;\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]\;\right)
&=& 
-\dfrac{1}{2}\underset{i \neq k}{\sum_{i,k\in U}}
E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right]
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\end{eqnarray*}
Thus, it remains only to show:
\begin{equation*}
a_{ik} \;=\; E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right].
\end{equation*}
Now,
\begin{equation*}
E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right]
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - \dfrac{1}{\pi_{ik}}E\!\left[\;I_{i}I_{k}\;\right]
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - \dfrac{1}{\pi_{ik}}\,\pi_{ik}
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - 1,
\end{equation*}
and
\begin{eqnarray*}
a_{ik}
&=& E\!\left[\;\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\;\right]
\;\;=\;\; E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - E\!\left[\;I_{i}\,w_{i}\;\right] - E\!\left[\;I_{k}\,w_{k}\;\right] + 1 \\
&=&E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - 1 - 1 + 1
\;\;=\;\; E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - 1 \\
&=& E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right],
\end{eqnarray*}
where third last equality follows from Proposition \ref{proposition:Unbiasedness} and
the unbiasedness hypothesis on $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ as an estimator for $\mathbf{T}_{\mathbf{y}}$.
The proof of the present Proposition is now complete.  \qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Conditional inference in finite-population sampling}
\setcounter{theorem}{0}

In this section, we give a justification for making inference conditional on the observed sample size
for sampling designs with random sample size.

\vskip 0.5cm
\noindent
\textbf{Observation (``mixture" of experiments) [see \cite{Valliant2000}, p.15.]}\vskip 0.1cm
\noindent
Consider a population $\mathcal{U}$ of 1000 units.
We wish to estimate the total $T_{y}$ of a certain population characteristic $\mathbf{y} = (y_{1},y_{2},\ldots,y_{1000})$.
Suppose we use the following two-step sampling scheme:
\begin{itemize}
\item	Step 1: We first flip a fair coin.\\
		Define the random variable $X$ by letting $X = 1$ if the coin lands heads, and $X = 0$ if it lands tails.
\item	Step 2: If $X = 1$, we select an SRS from $\mathcal{U}$ of size 100. If $X = 0$, we take a census on all of $\mathcal{U}$.
\end{itemize}
Let $\mathcal{S} \subset \mathcal{P}(\mathcal{U})$ denote the probability space of all
possible samples induced by the (two-step) sampling design above.
Note that $\mathcal{S} = \mathcal{S}_{0} \sqcup \mathcal{S}_{1}$,
where $\mathcal{S}_{0} = \left\{\,\,\mathcal{U}\,\right\}$ and
$\mathcal{S}_{1}$ is the set of all subsets of $\mathcal{U}$ of size 100.
The sampling design is determined by the following probability distribution on $\mathcal{S}$:
\begin{equation*}
P\!\left(\,\mathcal{U}\,\right) = \dfrac{1}{2}
\quad\textnormal{and}\quad
P\!\left(\,s\,\right) = \frac{1}{2\left(\begin{array}{c}1000\\100\end{array}\right)},
\;\;\textnormal{for each $s \in \mathcal{S}_{1}$}.
\end{equation*}
Let $\widehat{T}_{y}:\mathcal{S} \longrightarrow \Re$ denote our chosen estimator for $T_{y}$.
Then the (unconditional) probability distribution of $\widehat{T}_{y}$ can be ``decomposed" as follows:
\begin{eqnarray*}
P\left(\,\left.\widehat{T}_{y} = t\;\right\vert\,\mathbf{y}\,\right)
&=& P\left(\,\left.\widehat{T}_{y} = t,\,X = 0\;\right\vert\,\mathbf{y}\,\right) + P\left(\,\left.\widehat{T}_{y} = t,\,X = 1\,\right\vert\,\mathbf{y}\,\right) \\
&=& P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 0,\,\mathbf{y}\,\right)\cdot P\left(\,\left.X = 0\;\right\vert\,\mathbf{y}\,\right)
	+ P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 1,\,\mathbf{y}\,\right)\cdot P\left(\,\left.X = 1\;\right\vert\,\mathbf{y}\,\right) \\
&=& P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 0,\,\mathbf{y}\,\right)\cdot P\left(\,X = 0\,\right)
	+ P\left(\,\left.\widehat{T}_{y} = t\;\right\vert X = 1,\,\mathbf{y}\,\right)\cdot P\left(\,X = 1\,\right),
\end{eqnarray*}
where the last equality follows because the distribution of $X$ is independent of $\mathbf{y}$.
Suppose the observation we make consists of $\left(\,\widehat{T}_{y}\,,\,X\,\right)$.
The unconditional probability distribution of $\widehat{T}_{y}$,
given by $P\left(\,\left.\widehat{T}_{y} = t\;\right\vert\,\mathbf{y}\,\right)$ above,
describes of course the randomness of the estimator $\widehat{T}_{y}$ as induced
by both the randomness of the sample
$s \in \mathcal{S} = \mathcal{S}_{0} \sqcup \mathcal{S}_{1}$
as well as that of $X$ (the outcome of the coin flip in Step 1).
Now, suppose we have indeed carried out the sampling procedure and have obtained
an observation of $\left(\,\widehat{T}_{y}\,,\,X\,\right)$.
Suppose it happened that $X = 1$.
Hence, we know that the estimate $\widehat{T}_{y}(s)$ we actually obtained
was generated from an SRS of size 100 (rather than a census).
Note also that the probability distribution of $X$ is independent of $\mathbf{y}$
and the observation of $X$ gives no information about $\mathbf{y}$.
{\color{red}One school of thought therefore argues that downstream inferences
about $\mathbf{y}$ should be carried out using the conditional probability
$P\!\left(\,\left.\widehat{T}_{y} = t\;\right\vert\;X = 1\,,\,\mathbf{y}\,\right)$,
rather than the unconditional probability
$P\left(\,\left.\widehat{T}_{y} = t\;\right\vert\,\mathbf{y}\,\right)$.}
In other words, in the present example, as far as making inferences about $\mathbf{y}$
is concerned, only the randomness in Step 2 is relevant, and the randomness in Step 1
(i.e. the randomness of $X$, the outcome of the coin flip) is irrelevant to any inference
about $\mathbf{y}$. Consequently randomness of $X$ ``should" be removed in any
inference procedure for $\mathbf{y}$, and this is achieved by conditioning on the
observed value of $X$. \qed

\vskip 0.5cm
\noindent
\textbf{Conditioning on obtained sample size for sample designs with random sample size}\vskip 0.1cm
\noindent
Suppose $\mathcal{U}$ is a finite population.
We wish to estimate the total $T_{y} = \sum_{i\in\mathcal{U}}y_{i}$ of a
population characteristic $\mathbf{y} : \mathcal{U} \longrightarrow \Re$,
using a sample design $p: \mathcal{S} \longrightarrow [\,0\,,1\,]$
and a estimator $\widehat{T} : \mathcal{S} \longrightarrow \Re$.
{\color{red}We make the assumption that the sampling design $p$ is independent of $\mathbf{y}$.}
Let $N : \mathcal{S} \longrightarrow \N\cup\{\,0\,\}$ be the random variable
of sample size, i.e. $N(s)$ $=$ number of elements in $s$,
for each possible sample $s \in \mathcal{S}$. Then,
\begin{eqnarray*}
P\left(\,\left.\widehat{T} = t \,\right\vert\,\mathbf{y}\,\right)
&=&  \sum_{n}\,P\left(\,\left.\widehat{T} = t,\, N = n \,\right\vert\,\mathbf{y}\,\right) \\
&=&  \sum_{n}\,P\left(\,\left.\widehat{T} = t\,\right\vert\,\,N = n,\, \mathbf{y}\,\right)\cdot P\left(\left.\,N = n\,\right\vert\,\mathbf{y}\,\right)\\
&=&  \sum_{n}\,P\left(\,\left.\widehat{T} = t\,\right\vert\,\,N = n,\, \mathbf{y}\,\right)\cdot P\left(\,N = n\,\right),
\end{eqnarray*}
where the last equality follows from the assumed independence of the probability distribution
$p : \mathcal{S} \longrightarrow [\,0\,,\,1\,]$ (hence that of $N$) from $\mathbf{y}$.
The key observation to make now is that:
{\color{red}Although the actual sampling procedure operationally may or may not
have been a two-step procedure, the independence of $p$ from $\mathbf{y}$ makes it probabilistically
equivalent to a two-step procedure, as shown by the above decomposition of
$P\!\left(\,\left.\widehat{T} = t\;\right\vert\,\mathbf{y}\right)$}
--- Step (1): randomly select a sample size $N = n$ according to the distribution $P(N = n)$, and then
Step (2): randomly select a sample $s$ of size $n$ chosen in Step (1) according to the distribution
$P\!\left(\,s\;\vert\,N = n\,\right)$.
By the statistical reasoning explained in the preceding observation, it follows that
post-sampling inference about $\mathbf{y}$ should be made based on the conditional
distribution $P\!\left(\,\left.\widehat{T} = t\;\right\vert\,N = n\,,\,\mathbf{y}\,\right)$,
rather than the unconditional distribution $P\!\left(\,\left.\widehat{T} = t\;\right\vert\,\mathbf{y}\,\right)$.
This is because the sampling scheme is probabilistically equivalent to a two-step procedure,
with the probability distribution of the first step (choosing a sample size) independent of the parameters
of interest ($T_{y}$), and thus only the probability distribution of the second step (choosing a sample
of the size chosen in first step) should be used to make inference about $T_{y}$.
\qed

\vskip 0.5cm
\noindent
\textbf{Caution}\vskip 0.1cm
\noindent
In more formal parlance, the random variable $N : \mathcal{S} \longrightarrow \N\cup\{\,0\,\}$
is \underline{ancillary} to the parameter $\mathbf{y}$.
Thus, conditioning on sample size, for finite-population sampling schemes with random
sample size, \emph{partially} conforms to the \textbf{Conditionality Principle},
which states that statistical inference about a parameter should be made conditioned on
observed values of statistics ancillary to that parameter.
The conformance is only partial due to the (obvious) fact that it is the sample $s$ itself
which is ancillary to the parameter of interest $\mathbf{y}$, not just its sample size $N(s)$.
Thus, full conformance to the Conditionality Principle would require inference about
$\mathbf{y}$ be made conditioned on the observed sample $s$ itself (rather than its
size $N(s)$).
However, if we did condition on the obtained sample $s$ itself, the domain of the estimator
$\widehat{T}$ would be restricted to the singleton $\{\,s\,\}$, and $\widehat{T}$ could then
attain only one value under conditioning on $s$, and no randomization-based
(i.e. design-based) inference --- apart from the observed value of $\widehat{T}(s)$ --- could
be made any longer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{alpha}
%\bibliographystyle{plain}
%\bibliographystyle{amsplain}
\bibliographystyle{acm}
\bibliography{KenChuStatistics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

