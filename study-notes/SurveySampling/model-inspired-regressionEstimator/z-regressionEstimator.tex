
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Model-inspired Regression Estimator -- Definition}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Model-inspired regression estimator]
\mbox{}
\vskip 0.15cm
\noindent
Suppose:
\begin{itemize}
\item
	$N \in \N$ and $U = \{1,2,\ldots,N\}$.
	\vskip 0.05cm
	$y : U \longrightarrow \Re$ is an $\Re$-valued population characteristic defined on $U$.
\item
	$\mathcal{S} \subset \mathcal{P}(U)$ is a collection of subsets of $U$.
	$p : \mathcal{S} \longrightarrow (0,1]$ is a sampling design on $\mathcal{S}$,
	i.e. $\underset{s\in\mathcal{S}}{\sum}\;p(s) = 1$.
	\vskip 0.05cm
	$\mathcal{S}$ can therefore be considered the collection of admissible samples under the sampling design $p$.
\item
	For each \,$k \in \left\{\,1,2,\ldots,N\,\right\} = U$,\, we have
	$\pi_{k} \,:=\, \underset{s \ni k}{\sum}\,p(s) \,>\, 0$.
\end{itemize}
If
\begin{enumerate}
\item
	$\mathbf{x} : U \longrightarrow \Re^{1 \times m}$ is an $\Re^{1 \times m}$-valued
	population characteristic defined on $U$, and
\item
	$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$,
\end{enumerate}
then the \,\underline{\textbf{model-inspired regression estimator}}\; is the function
\,$\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}} : \mathcal{S} \longrightarrow \Re$\,
defined by:
\begin{equation*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
\;\; := \;\;
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)\,,
	\quad\textnormal{for each \,$s \in \mathcal{S}$},
\end{equation*}
where
\begin{equation*}
\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)
\;\; := \;\;
	\underset{k \in s}{\sum}\;\dfrac{\mathbf{x}_{k}}{\pi_{k}}
	\;\; \in \;\;
	\Re^{1 \times m}
\quad\quad\textnormal{and}\quad\quad
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\;\; := \;\;
	\left(\,\underset{k \in s}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot \mathbf{x}_{k}}{\pi_{k}\,\sigma_{k}^{2}} \,\right)^{-1}
	\cdot
	\left(\,\underset{k \in s}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot y_{k}}{\pi_{k}\,\sigma_{k}^{2}} \,\right)
	\;\; \in \;\;
	\Re^{m \times 1}
\end{equation*}
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{remark}[Motivating observations for model-inspired regression estimator]
\mbox{}
\vskip 0.15cm
\noindent
Suppose
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
	$N \in \N$.
\item
	$Y : (\Omega,\mathcal{A},\mu) \longrightarrow \Re^{N}$ is an $\Re^{N}$-valued random variable.
\item
	$X \in \Re^{N \times m}$.\;
	$\beta \in \Re^{m \times 1}$.\;
	$\Sigma \in \Re^{N \times N}$ is symmetric and positive-definite.
\item
	$E\!\left[\,Y\,\right] \; = \; X \cdot \beta$.
\item
	$\Var\!\left[\,Y\,\right]$
	\,$=$\, $\Sigma$
	\,$=$\, $\diag\!\left(\,\sigma_{1}^{2}\,,\,\sigma_{2}^{2}\,,\,\ldots,\sigma_{N}^{2}\,\right) \in \Re^{N \times N}$,\,
	where $\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$.
\end{itemize}
Then, by Gauss-Markov theory, the BLUE (Best Linear Unbiased Estimator) for $\beta$ is
\begin{equation*}
\widehat{\beta}
\;\; := \;\;
	\underset{\beta\in\Re^{m \times 1}}{\textnormal{argmin}}\,
	\left\{\,
		\left\Vert\;Y - X \cdot \beta\;\right\Vert^{2}
	\,\right\}
\;\; = \;\;
	\left(\,X^{T} \overset{{\color{white}!}}{\cdot} \Sigma^{-1} \cdot X\,\right)^{-1}
	\cdot
	\left(\,X^{T} \overset{{\color{white}!}}{\cdot} \Sigma^{-1} \cdot Y \,\right)
\end{equation*}
(Recall that BLUE here means an unbiased estimator linear in $Y$
having the smallest variance among unbiased estimators linear in $Y$.)
Hence, we have
\begin{equation}\label{TyEqTxBetaHat}
T_{y}
\;\; = \;\; J \cdot Y
\;\; \approx \;\; J \cdot X \cdot \widehat{\beta}
\;\; = \;\; T_{\mathbf{x}} \cdot \widehat{\beta}\,,
\end{equation}
where $J = (1,1,\ldots,1) \in \Re^{1 \times N}$.

\vskip 0.3cm
\noindent
Now, since $\Sigma$
\,$=$\, $\diag\!\left(\,\sigma_{1}^{2}\,,\,\sigma_{2}^{2}\,,\,\ldots,\sigma_{N}^{2}\,\right) \in \Re^{N \times N}$,\,
we see that $\widehat{\beta}$ simplifies to:
\begin{equation*}
\widehat{\beta}
\;\; = \;\;
	\left(\,\underset{k \in U}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot \mathbf{x}_{k}}{\sigma_{k}^{2}} \,\right)^{-1}
	\cdot
	\left(\,\underset{k \in U}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot y_{k}}{\sigma_{k}^{2}} \,\right)
	\;\; \in \;\;
	\Re^{m \times 1}
\end{equation*}
Lastly, replacing $T_{y}$ and $T_{\mathbf{x}}$ in \eqref{TyEqTxBetaHat}
with their respective Horvitz-Thompson estimators,
as well as the numerator and denominator of $\widehat{\beta}$
with their respective Horvitz-Thompson estimators yields:
\begin{equation}\label{THTyEqTHTxBetaHat}
\widehat{T}_{y}^{\,\textnormal{HT}}(s)
\;\; \approx \;\; \widehat{T}_{\mathbf{x}}^{\,\textnormal{HT}}(s) \cdot \widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\end{equation}
Substracting \eqref{THTyEqTHTxBetaHat} from \eqref{TyEqTxBetaHat} yields:
\begin{equation*}
T_{y}
	\,-\,
	\widehat{T}_{y}^{\,\textnormal{HT}}(s)
\;\; \approx \;\;
	T_{\mathbf{x}} \cdot \widehat{\beta}
	\,-\,
	\widehat{T}_{\mathbf{x}}^{\,\textnormal{HT}}(s) \cdot \widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\end{equation*}
Replacing $\widehat{\beta}$ with $\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)$ in the above equation
and rearranging then finally yield:
\begin{equation*}
T_{y}
\;\; \approx \;\;
	\widehat{T}_{y}^{\,\textnormal{HT}}(s)
	\,+\,
	\left(\,T_{\mathbf{x}} \,-\, \widehat{T}_{\mathbf{x}}^{\,\textnormal{HT}}(s) \right)
	\cdot \widehat{\mathbf{B}}_{y,\mathbf{x}}(s)\,,
\end{equation*}
where right-hand-side above is precisely the definition of the model-inspired regression estimator.
\end{remark}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{example}[Group mean model yields poststratified estimator]
\mbox{}
\vskip 0.05cm
\noindent
%\textnormal{\textbf{(model-inspired regression estimator specializes to poststratified estimator under group mean model)}}
%\vskip 0.15cm
%\noindent
Suppose:
\begin{itemize}
\item
	$N \in \N$ and $U = \{1,2,\ldots,N\}$.
	\vskip 0.05cm
	$y : U \longrightarrow \Re$ is an $\Re$-valued population characteristic defined on $U$.
\item
	$\mathcal{S} \subset \mathcal{P}(U)$ is a collection of subsets of $U$.
	$p : \mathcal{S} \longrightarrow (0,1]$ is a sampling design on $\mathcal{S}$,
	i.e. $\underset{s\in\mathcal{S}}{\sum}\;p(s) = 1$.
	\vskip 0.05cm
	$\mathcal{S}$ can therefore be considered the collection of admissible samples under the sampling design $p$.
\item
	For each \,$k \in \left\{\,1,2,\ldots,N\,\right\} = U$,\, we have
	$\pi_{k} \,:=\, \underset{s \ni k}{\sum}\,p(s) \,>\, 0$.
\end{itemize}
Suppose furthermore that
$\mathbf{x} : U \longrightarrow \Re^{1 \times G}$ and
$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$
follow the \,\underline{\textbf{group mean model}}.
\vskip 0.05cm
\noindent
Equivalently, but more precisely, suppose:
\begin{enumerate}
\item
	The population $U$ can be partitioned into $m$ disjoint subsets (post-strata), i.e.
	$U \,=\, \overset{G}{\underset{g=1}{\bigsqcup}}\, U_{g}$, and
	the population characteristic (auxiliary variable)
	$\mathbf{x} : U \longrightarrow \Re^{1 \times G}$ is given by:
	for each $k \in U$ and $g = 1,2,\ldots,G$,
	\begin{equation*}
	\mathbf{x}_{k}
	\; = \; (\eta_{k1},\eta_{k2},\ldots,\eta_{kg},\ldots,\eta_{kG})
	\; = \; (0,0,\ldots,0,1,0,\ldots,0)\,,
	\end{equation*}
	where
	\begin{equation*}
	\eta_{kg}
	\;\; = \;\;
		\left\{\begin{array}{cl}
		1, & \textnormal{if $k \in U_{g}$}
		\\
		\overset{{\color{white}-}}{0}, & \textnormal{if $k \notin U_{g}$}
		\end{array}\right.
	\end{equation*}
\item
	There exist \,$\sigma_{0,1} \,,\, \sigma_{0,2} \,,\, \ldots \,,\, \sigma_{0,G} \,>\, 0$\,
	such that
	$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$ are given by:
	\begin{equation*}
	\sigma_{k} \; = \; \eta_{kg}\cdot\sigma_{0,g}\,,
	\quad\textnormal{for each \,$k \in U$\, and \,$g = 1,2,\ldots,G$}\,.
	\end{equation*}
\end{enumerate}
Then, the \,\textbf{model-inspired regression estimator}\,
specializes to the \,\underline{\textbf{poststratified estimator}}, i.e.
\begin{eqnarray*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
& := &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)\,,
	\quad\textnormal{for each \,$s \in \mathcal{S}$},
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\overset{{\color{white}.}}{\widehat{N}_{g}(s)}
		%\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\;\; = \;\;
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\end{eqnarray*}
\end{example}
\proof
First, note that \,$\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)$\, can be expressed as
\begin{equation*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\;\; = \;\;
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
	\cdot
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)\,,
\end{equation*}
where $X(s) \in \Re^{n(s) \times G}$, $\Sigma(s)^{-1} \in \Re^{n(s) \times n(s)}$, and $Y(s) \in \Re^{n(s)}$
are given by
\begin{eqnarray*}
X(s)_{ig}
& = &
	\eta_{ig}
\;\; = \;\;
	\left\{\begin{array}{cl}
		1, &  \textnormal{if \,$i \in s \cap U_{g}$}
		\\
		0, & \textnormal{if \,$i \in s\,\backslash\,U_{g}$}
	\end{array}\right.
\\
\Sigma(s)^{-1}_{ij}
& = &
	\dfrac{\delta_{ij}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}\,,
	\quad
	\textnormal{where \,$i = j \in s \cap U_{g(i)}$}
\\
Y(s)_{i}
& = &
	y_{i}
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\left(\,\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} X(s)\right)_{ig}{\color{white}111}
& = &
	\underset{k \in s}{\sum}\;\,
	\dfrac{\delta_{ik}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}
	\cdot
	\eta_{kg}
\;\; = \;\;
	\dfrac{\eta_{ig}}{\pi_{i}\,\sigma^{2}_{0,g}}
\\
\left(\,X(s)^{T}\cdot\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} X(s)\right)_{gh}
& = &
	\underset{k \in s}{\sum}\;\,
	\eta_{kg} \cdot \dfrac{\eta_{kh}}{\pi_{k}\,\sigma^{2}_{0,h}}
\;\; = \;\;
	\dfrac{\delta_{gh}}{\sigma^{2}_{0,g}}
	\cdot
	\left(\,\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\,\dfrac{1}{\pi_{k}}\,\right)
\;\; = \;\;
	\dfrac{\delta_{gh}}{\sigma^{2}_{0,g}} \cdot \widehat{N}_{g}(s)
\\
\left(\,\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} Y(s)\right)_{i}{\color{white}1111}
& = &
	\underset{k \in s}{\sum}\;\,
	\dfrac{\delta_{ik}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}
	\cdot
	y_{k}
\;\; = \;\;
	\dfrac{y_{i}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}
\\
\left(\,X(s)^{T}\cdot\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} Y(s)\right)_{g}{\color{white}1}
& = &
	\underset{k \in s}{\sum}\;\,
	\eta_{kg} \cdot \dfrac{y_{k}}{\pi_{k}\,\sigma^{2}_{0,g(k)}}
\;\; = \;\;
	\dfrac{1}{\sigma^{2}_{0,g}}
	\cdot
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\end{eqnarray*}
which together imply
\begin{eqnarray*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)_{g}
& = &
	\left[\,
		\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
		\cdot
		\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)
	\,\right]_{g}
\\
& = &
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\dfrac{\delta_{gh}\,\sigma^{2}_{0,g}}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\cdot
	\left(\,
		\dfrac{1}{\sigma^{2}_{0,h}}
		\cdot
		\underset{k\,\in\,s\,\cap\,U_{h}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
	\,\right)
\;\; = \;\;
	\dfrac{\sigma^{2}_{0,g}}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\cdot
	\dfrac{1}{\sigma^{2}_{0,g}}
	\cdot
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\\
& = &
	\dfrac{1}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\cdot
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\end{eqnarray*}
On the other hand, it is clear that
\begin{equation*}
T_{x} \,-\, \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)
\;\; = \;\;
	\left(\, N_{1} - \widehat{N}_{1}(s) \,,\, N_{2} - \widehat{N}_{2}(s) \,,\, \ldots \,,\, N_{G} - \widehat{N}_{G}(s) \,\right)
\end{equation*}
Consequently,
\begin{eqnarray*}
\left(\,T_{x} \,-\, \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)\right)
\cdot
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	\left(\,N_{g} - \widehat{N}_{g}(s) \,\right)
	\cdot
	\left(\,
		\dfrac{1}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
		\cdot
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
	\,\right)
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	N_{g} \cdot \dfrac{ \underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, y_{k}/\pi_{k} }{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\; - \;
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\;
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	N_{g} \cdot \dfrac{ \underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, y_{k}/\pi_{k} }{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\; - \;
	\underset{k\,\in\,s}{\sum}\;\;\dfrac{y_{k}}{\pi_{k}}
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	N_{g} \cdot \dfrac{ \underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, y_{k}/\pi_{k} }{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\; - \;
	\widehat{T}^{\textnormal{HT}}_{y}(s)
\end{eqnarray*}
We now see that
\begin{eqnarray*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
& := &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\overset{{\color{white}.}}{\widehat{N}_{g}(s)}
		%\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\;\; = \;\;
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\end{eqnarray*}
This completes the proof of the Example.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
