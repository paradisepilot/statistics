
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Model-inspired Regression Estimator -- Definition}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Model-inspired regression estimator]
\mbox{}
\vskip 0.15cm
\noindent
Suppose:
\begin{itemize}
\item
	$N \in \N$ and $U = \{1,2,\ldots,N\}$.
	\vskip 0.05cm
	$y : U \longrightarrow \Re$ is an $\Re$-valued population characteristic defined on $U$.
\item
	$\mathcal{S} \subset \mathcal{P}(U)$ is a collection of subsets of $U$.
	$p : \mathcal{S} \longrightarrow (0,1]$ is a sampling design on $\mathcal{S}$,
	i.e. $\underset{s\in\mathcal{S}}{\sum}\;p(s) = 1$.
	\vskip 0.05cm
	$\mathcal{S}$ can therefore be considered the collection of admissible samples under the sampling design $p$.
\item
	For each \,$k \in \left\{\,1,2,\ldots,N\,\right\} = U$,\, we have
	$\pi_{k} \,:=\, \underset{s \ni k}{\sum}\,p(s) \,>\, 0$.
\end{itemize}
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
If
\begin{enumerate}
\item
	$\mathbf{x} : U \longrightarrow \Re^{1 \times m}$ is an $\Re^{1 \times m}$-valued
	population characteristic defined on $U$, and
\item
	$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$,
\end{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
then the \,\underline{\textbf{model-inspired regression estimator}}\; is the function
\,$\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}} : \mathcal{S} \longrightarrow \Re$\,
defined by:
\begin{equation*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
\;\; := \;\;
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)\,,
	\quad\textnormal{for each \,$s \in \mathcal{S}$},
\end{equation*}
where
\begin{equation*}
\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)
\;\; := \;\;
	\underset{k \in s}{\sum}\;\dfrac{\mathbf{x}_{k}}{\pi_{k}}
	\;\; \in \;\;
	\Re^{1 \times m}\,,
\end{equation*}
and
\begin{eqnarray*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
& := &
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
	\cdot
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)
\\
& = &
	\left(\,\underset{k \in s}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot \mathbf{x}_{k}}{\pi_{k}\,\sigma_{k}^{2}} \,\right)^{-1}
	\cdot
	\left(\,\underset{k \in s}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot y_{k}}{\pi_{k}\,\sigma_{k}^{2}} \,\right)
	\;\; \in \;\;
	\Re^{m \times 1}\,,
\end{eqnarray*}
where \,$X(s) \in \Re^{n(s) \times m}$,\, $\Sigma(s)^{-1} \in \Re^{n(s) \times n(s)}$,\, and \,$Y(s) \in \Re^{n(s)}$
are respectively given by
\begin{equation*}
	X(s)_{ij} \; = \; (\,\mathbf{x}_{i}\,)_{j}\,,
\quad\quad
	\Sigma(s)^{-1}_{ij} \; = \; \dfrac{\delta_{ij}}{\pi_{i}\,\sigma^{2}_{i}}\,,
\quad\;\;\textnormal{and}\quad\;\;
	Y(s)_{i} \; = \; y_{i}\,.
\end{equation*}
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\clearpage
\begin{remark}[Motivating observations for model-inspired regression estimator]
\mbox{}
\vskip 0.15cm
\noindent
Suppose
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
	$N \in \N$.
\item
	$Y : (\Omega,\mathcal{A},\mu) \longrightarrow \Re^{N}$ is an $\Re^{N}$-valued random variable.
\item
	$X \in \Re^{N \times m}$.\;
	$\beta \in \Re^{m \times 1}$.\;
	$\Sigma \in \Re^{N \times N}$ is symmetric and positive-definite.
\item
	$E\!\left[\,Y\,\right] \; = \; X \cdot \beta$.
\item
	$\Var\!\left[\,Y\,\right]$
	\,$=$\, $\Sigma$
	\,$=$\, $\diag\!\left(\,\sigma_{1}^{2}\,,\,\sigma_{2}^{2}\,,\,\ldots,\sigma_{N}^{2}\,\right) \in \Re^{N \times N}$,\,
	where $\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$.
\end{itemize}
Then, by Gauss-Markov theory, the
BLUE (Best Linear Unbiased Estimator)\footnote{Recall that BLUE means an unbiased estimator linear in $Y$
having the smallest variance among all unbiased estimators linear in $Y$.} for $\beta$ is
\begin{equation*}
\widehat{\beta}
\;\; := \;\;
	\underset{\beta\in\Re^{m \times 1}}{\textnormal{argmin}}\,
	\left\{\,
		\left\Vert\;Y - X \cdot \beta\;\right\Vert^{2}
	\,\right\}
\;\; = \;\;
	\left(\,X^{T} \overset{{\color{white}!}}{\cdot} \Sigma^{-1} \cdot X\,\right)^{-1}
	\cdot
	\left(\,X^{T} \overset{{\color{white}!}}{\cdot} \Sigma^{-1} \cdot Y \,\right)
\end{equation*}
Hence, we have
\begin{equation}\label{TyEqTxBetaHat}
T_{y}
\;\; = \;\; J \cdot Y
\;\; \approx \;\; J \cdot X \cdot \widehat{\beta}
\;\; = \;\; T_{\mathbf{x}} \cdot \widehat{\beta}\,,
\end{equation}
where $J = (1,1,\ldots,1) \in \Re^{1 \times N}$.

\vskip 0.3cm
\noindent
Now, since $\Sigma$
\,$=$\, $\diag\!\left(\,\sigma_{1}^{2}\,,\,\sigma_{2}^{2}\,,\,\ldots,\sigma_{N}^{2}\,\right) \in \Re^{N \times N}$,\,
we see that $\widehat{\beta}$ simplifies to:
\begin{equation*}
\widehat{\beta}
\;\; = \;\;
	\left(\,\underset{k \in U}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot \mathbf{x}_{k}}{\sigma_{k}^{2}} \,\right)^{-1}
	\cdot
	\left(\,\underset{k \in U}{\sum}\;\dfrac{\mathbf{x}_{k}^{T} \cdot y_{k}}{\sigma_{k}^{2}} \,\right)
	\;\; \in \;\;
	\Re^{m \times 1}
\end{equation*}
Lastly, replacing $T_{y}$ and $T_{\mathbf{x}}$ in \eqref{TyEqTxBetaHat}
with their respective Horvitz-Thompson estimators,
as well as the numerator and denominator of $\widehat{\beta}$
with their respective Horvitz-Thompson estimators yields:
\begin{equation}\label{THTyEqTHTxBetaHat}
\widehat{T}_{y}^{\,\textnormal{HT}}(s)
\;\; \approx \;\; \widehat{T}_{\mathbf{x}}^{\,\textnormal{HT}}(s) \cdot \widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\end{equation}
Substracting \eqref{THTyEqTHTxBetaHat} from \eqref{TyEqTxBetaHat} yields:
\begin{equation*}
T_{y}
	\,-\,
	\widehat{T}_{y}^{\,\textnormal{HT}}(s)
\;\; \approx \;\;
	T_{\mathbf{x}} \cdot \widehat{\beta}
	\,-\,
	\widehat{T}_{\mathbf{x}}^{\,\textnormal{HT}}(s) \cdot \widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\end{equation*}
Replacing $\widehat{\beta}$ with $\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)$ in the above equation
and rearranging then finally yield:
\begin{equation*}
T_{y}
\;\; \approx \;\;
	\widehat{T}_{y}^{\,\textnormal{HT}}(s)
	\,+\,
	\left(\,T_{\mathbf{x}} \,-\, \widehat{T}_{\mathbf{x}}^{\,\textnormal{HT}}(s) \right)
	\cdot \widehat{\mathbf{B}}_{y,\mathbf{x}}(s)\,,
\end{equation*}
where right-hand-side above is precisely the definition of the model-inspired regression estimator.
\end{remark}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
