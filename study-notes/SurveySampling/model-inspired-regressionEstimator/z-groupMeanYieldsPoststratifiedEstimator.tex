
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Group mean model yields the poststratified estimator}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

The following example shows that the model-inspired regression estimator specializes to the poststratified estimator under the group mean model.
\begin{example}
\mbox{}
\vskip 0.05cm
\noindent
%\textnormal{\textbf{(model-inspired regression estimator specializes to poststratified estimator under group mean model)}}
%\vskip 0.15cm
%\noindent
Suppose:
\begin{itemize}
\item
	$N \in \N$ and $U = \{1,2,\ldots,N\}$.
	\vskip 0.05cm
	$y : U \longrightarrow \Re$ is an $\Re$-valued population characteristic defined on $U$.
\item
	$\mathcal{S} \subset \mathcal{P}(U)$ is a collection of subsets of $U$.
	$p : \mathcal{S} \longrightarrow (0,1]$ is a sampling design on $\mathcal{S}$,
	i.e. $\underset{s\in\mathcal{S}}{\sum}\;p(s) = 1$.
	\vskip 0.05cm
	$\mathcal{S}$ can therefore be considered the collection of admissible samples under the sampling design $p$.
\item
	For each \,$k \in \left\{\,1,2,\ldots,N\,\right\} = U$,\, we have
	$\pi_{k} \,:=\, \underset{s \ni k}{\sum}\,p(s) \,>\, 0$.
\end{itemize}
Suppose furthermore that
$\mathbf{x} : U \longrightarrow \Re^{1 \times G}$ and
$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$
follow the \,\underline{\textbf{group mean model}}.
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\vskip 0.1cm
\noindent
Equivalently, but more precisely, suppose:
\begin{enumerate}
\item \vskip -0.25cm
	The population $U$ can be partitioned into $m$ disjoint subsets (post-strata), i.e.
	$U \,=\, \overset{G}{\underset{g=1}{\bigsqcup}}\, U_{g}$, and
	the population characteristic (auxiliary variable)
	$\mathbf{x} : U \longrightarrow \Re^{1 \times G}$ is given by:
	for each $k \in U$ and $g = 1,2,\ldots,G$,
	\begin{equation*}
	\mathbf{x}_{k}
	\; = \; (\eta_{k1},\eta_{k2},\ldots,\eta_{kg},\ldots,\eta_{kG})
	\; = \; (0,0,\ldots,0,1,0,\ldots,0)\,,
	\end{equation*}
	where
	\begin{equation*}
	\eta_{kg}
	\;\; = \;\;
		\left\{\begin{array}{cl}
		1, & \textnormal{if $k \in U_{g}$}
		\\
		\overset{{\color{white}-}}{0}, & \textnormal{if $k \notin U_{g}$}
		\end{array}\right.
	\end{equation*}
\item
	There exist \,$\sigma_{0,1} \,,\, \sigma_{0,2} \,,\, \ldots \,,\, \sigma_{0,G} \,>\, 0$\,
	such that
	$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$ are given by:
	\begin{equation*}
	\sigma_{k} \; = \; \eta_{kg}\cdot\sigma_{0,g}\,,
	\quad\textnormal{for each \,$k \in U$\, and \,$g = 1,2,\ldots,G$}\,.
	\end{equation*}
\end{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
Then, the \,\textbf{model-inspired regression estimator}\,
specializes to the \,\underline{\textbf{poststratified estimator}}, i.e.
\begin{eqnarray*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
& := &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)\,,
	\quad\textnormal{for each \,$s \in \mathcal{S}$},
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\overset{{\color{white}.}}{\widehat{N}_{g}(s)}
		%\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\;\; = \;\;
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\end{eqnarray*}
\end{example}
\proof
First, recall that \,$\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)$\, is given by
\begin{equation*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\;\; = \;\;
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
	\cdot
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)\,,
\end{equation*}
where $X(s) \in \Re^{n(s) \times G}$, $\Sigma(s)^{-1} \in \Re^{n(s) \times n(s)}$, and $Y(s) \in \Re^{n(s)}$
are, in the present scenario, given by
\begin{eqnarray*}
X(s)_{ig}
& = &
	\eta_{ig}
\;\; = \;\;
	\left\{\begin{array}{cl}
		1, &  \textnormal{if \,$i \in s \cap U_{g}$}
		\\
		0, & \textnormal{if \,$i \in s\,\backslash\,U_{g}$}
	\end{array}\right.
\\
\Sigma(s)^{-1}_{ij}
& = &
	\dfrac{\delta_{ij}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}\,,
	\quad
	\textnormal{where \,$i = j \in s \cap U_{g(i)}$}
\\
Y(s)_{i}
& = &
	y_{i}
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\left(\,\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} X(s)\right)_{ig}{\color{white}111}
& = &
	\underset{k \in s}{\sum}\;\,
	\dfrac{\delta_{ik}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}
	\cdot
	\eta_{kg}
\;\; = \;\;
	\dfrac{\eta_{ig}}{\pi_{i}\,\sigma^{2}_{0,g}}
\\
\left(\,X(s)^{T}\cdot\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} X(s)\right)_{gh}
& = &
	\underset{k \in s}{\sum}\;\,
	\eta_{kg} \cdot \dfrac{\eta_{kh}}{\pi_{k}\,\sigma^{2}_{0,h}}
\;\; = \;\;
	\dfrac{\delta_{gh}}{\sigma^{2}_{0,g}}
	\cdot
	\left(\,\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\,\dfrac{1}{\pi_{k}}\,\right)
\;\; = \;\;
	\dfrac{\delta_{gh}}{\sigma^{2}_{0,g}} \cdot \widehat{N}_{g}(s)
\\
\left(\,\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} Y(s)\right)_{i}{\color{white}1111}
& = &
	\underset{k \in s}{\sum}\;\,
	\dfrac{\delta_{ik}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}
	\cdot
	y_{k}
\;\; = \;\;
	\dfrac{y_{i}}{\pi_{i}\,\sigma^{2}_{0,g(i)}}
\\
\left(\,X(s)^{T}\cdot\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} Y(s)\right)_{g}{\color{white}1}
& = &
	\underset{k \in s}{\sum}\;\,
	\eta_{kg} \cdot \dfrac{y_{k}}{\pi_{k}\,\sigma^{2}_{0,g(k)}}
\;\; = \;\;
	\dfrac{1}{\sigma^{2}_{0,g}}
	\cdot
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\end{eqnarray*}
which together imply
\begin{eqnarray*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)_{g}
& = &
	\left[\,
		\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
		\cdot
		\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)
	\,\right]_{g}
\\
& = &
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\dfrac{\delta_{gh}\,\sigma^{2}_{0,g}}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\cdot
	\left(\,
		\dfrac{1}{\sigma^{2}_{0,h}}
		\cdot
		\underset{k\,\in\,s\,\cap\,U_{h}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
	\,\right)
\;\; = \;\;
	\dfrac{\sigma^{2}_{0,g}}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\cdot
	\dfrac{1}{\sigma^{2}_{0,g}}
	\cdot
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\\
& = &
	\dfrac{1}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\cdot
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\end{eqnarray*}
On the other hand, it is clear that
\begin{equation*}
T_{x} \,-\, \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)
\;\; = \;\;
	\left(\, N_{1} - \widehat{N}_{1}(s) \,,\, N_{2} - \widehat{N}_{2}(s) \,,\, \ldots \,,\, N_{G} - \widehat{N}_{G}(s) \,\right)
\end{equation*}
Consequently,
\begin{eqnarray*}
\left(\,T_{x} \,-\, \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)\right)
\cdot
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	\left(\,N_{g} - \widehat{N}_{g}(s) \,\right)
	\cdot
	\left(\,
		\dfrac{1}{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
		\cdot
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
	\,\right)
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	N_{g} \cdot \dfrac{ \underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, y_{k}/\pi_{k} }{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\; - \;
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\;
	\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	N_{g} \cdot \dfrac{ \underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, y_{k}/\pi_{k} }{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\; - \;
	\underset{k\,\in\,s}{\sum}\;\;\dfrac{y_{k}}{\pi_{k}}
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;
	N_{g} \cdot \dfrac{ \underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\, y_{k}/\pi_{k} }{\overset{{\color{white}.}}{\widehat{N}_{g}(s)}}
	\; - \;
	\widehat{T}^{\textnormal{HT}}_{y}(s)
\end{eqnarray*}
We now see that
\begin{eqnarray*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
& := &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\\
& = &
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\overset{{\color{white}.}}{\widehat{N}_{g}(s)}
		%\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\;\; = \;\;
	\overset{G}{\underset{g\,=\,1}{\sum}}\;\, N_{g}
	\cdot
	\dfrac{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\sum}\;y_{k}/\pi_{k}
		}{
		\underset{k\,\in\,s\,\cap\,U_{g}}{\overset{{\color{white}.}}{\sum}}\;\;1/\pi_{k}
		}
\end{eqnarray*}
This completes the proof of the Example.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
