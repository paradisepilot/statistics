
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Common ratio model yields the ratio estimator}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

The following proposition shows that the model-inspired regression estimator specializes to
the ratio estimator under the common ratio model.
\begin{proposition}
\mbox{}
\vskip 0.05cm
\noindent
%\textnormal{\textbf{(model-inspired regression estimator specializes to poststratified estimator under group mean model)}}
%\vskip 0.15cm
%\noindent
Suppose:
\begin{itemize}
\item
	$N \in \N$ and $U = \{1,2,\ldots,N\}$.
	\vskip 0.05cm
	$y : U \longrightarrow \Re$ is an $\Re$-valued population characteristic defined on $U$.
\item
	$\mathcal{S} \subset \mathcal{P}(U)$ is a collection of subsets of $U$.
	$p : \mathcal{S} \longrightarrow (0,1]$ is a sampling design on $\mathcal{S}$,
	i.e. $\underset{s\in\mathcal{S}}{\sum}\;p(s) = 1$.
	\vskip 0.05cm
	$\mathcal{S}$ can therefore be considered the collection of admissible samples under the sampling design $p$.
\item
	For each \,$k \in \left\{\,1,2,\ldots,N\,\right\} = U$,\, we have
	$\pi_{k} \,:=\, \underset{s \ni k}{\sum}\,p(s) \,>\, 0$.
\end{itemize}
Suppose furthermore that
$\mathbf{x} : U \longrightarrow \Re^{1 \times 1}$ and
$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$
follow the \,\underline{\textbf{{\color{red}common ratio}{\color{white}j}model}}.
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\vskip 0.1cm
\noindent
Equivalently, but more precisely, suppose:
\begin{enumerate}
\item \vskip -0.10cm
	The population characteristic (auxiliary variable)
	$\mathbf{x} : U \longrightarrow \Re^{1 \times 1}$ is non-zero but otherwise arbitrary.
	\begin{equation*}
	\mathbf{x}_{k}
		\; = \; x_{k}
		\; \neq 0\,,
	\quad\textnormal{for each \,$k \in U$}
	\end{equation*}
\item
	There exist \,$\sigma \,>\, 0$\, such that
	$\sigma_{1}$, $\sigma_{2}$, $\ldots$\,, $\sigma_{N}$ $>$ $0$ are given by:
	\begin{equation*}
	\sigma_{k}^{2} \; = \; \sigma^{2} \cdot x_{k}\,,
	\quad\textnormal{for each \,$k \in U$}\,.
	\end{equation*}
\end{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
Then, the \,\textbf{model-inspired regression estimator}\,
specializes to the \,\underline{\textbf{{\color{red}ratio}{\color{white}j}estimator}}, i.e.
\begin{eqnarray*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
& := &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\\
& = &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\cdot
	\dfrac{
		T_{\mathbf{x}}
		}{
		\overset{{\color{white}.}}{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}
		}
\;\; = \;\;
	\left(\,\underset{k \in s}{\sum}\;\dfrac{y_{k}}{\pi_{k}}\,\right)
	\cdot
 	\dfrac{
		T_{\mathbf{x}}
		}{
		\underset{k \in s}{\sum}\;x_{k}/\pi_{k}
		}
	\,,\quad\quad\textnormal{for each \,$s \in \mathcal{S}$}.
\end{eqnarray*}
\end{proposition}
\proof
Recall that \,$\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)$\, is given by
\begin{eqnarray*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
& = &
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
	\cdot
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)
\end{eqnarray*}
where $X(s) \in \Re^{n(s) \times 1}$, $\Sigma(s)^{-1} \in \Re^{n(s) \times n(s)}$, and $Y(s) \in \Re^{n(s)}$
are, in the present scenario, given by
\begin{eqnarray*}
X(s)_{i}
& = &
	x_{i}
\\
\Sigma(s)^{-1}_{ij}
& = &
	\dfrac{\delta_{ij}}{\pi_{i}\,\sigma^{2}\,x_{i}}
\\
Y(s)_{i}
& = &
	y_{i}
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\left(\,\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} X(s)\right)_{i}{\color{white}!}
& = &
	\underset{k \in s}{\sum}\;\,
	\dfrac{\delta_{ik}}{\pi_{i}\,\sigma^{2}\,x_{i}} \cdot x_{k}
\;\; = \;\;
	\dfrac{1}{\pi_{i}\,\sigma^{2}}
\\
X(s)^{T}\cdot\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} X(s)
& = &
	\underset{k \in s}{\sum}\;\,
	x_{k} \cdot \dfrac{1}{\pi_{k}\,\sigma^{2}}
\;\; = \;\;
	\dfrac{1}{\sigma^{2}}
	\cdot
	\left(\,\underset{k \in s}{\sum}\,\dfrac{x_{k}}{\pi_{k}}\,\right)
\;\; = \;\;
	\dfrac{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}{\sigma^{2}}
\\
\left(\,\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} Y(s)\right)_{i}{\color{white}1}
& = &
	\underset{k \in s}{\sum}\;\,
	\dfrac{\delta_{ik}}{\pi_{i}\,\sigma^{2}\,x_{i}}
	\cdot
	y_{k}
\;\; = \;\;
	\dfrac{y_{i}}{\pi_{i}\,\sigma^{2}\,x_{i}}
\\
X(s)^{T}\cdot\Sigma(s)^{-1} \overset{{\color{white}!}}{\cdot} Y(s){\color{white}!}
& = &
	\underset{k \in s}{\sum}\;\,
	x_{k} \cdot \dfrac{y_{k}}{\pi_{k}\,\sigma^{2}\,x_{k}}
\;\; = \;\;
	\dfrac{1}{\sigma^{2}}
	\cdot
	\underset{k \in s}{\sum}\, \dfrac{y_{k}}{\pi_{k}}
\;\; = \;\;
	\dfrac{\widehat{T}^{\,\textnormal{HT}}_{y}(s)}{\sigma^{2}}
\end{eqnarray*}
which together imply
\begin{eqnarray*}
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
& = &
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot X(s)\,\right)^{-1}
	\cdot
	\left(\,X(s)^{T} \overset{{\color{white}!}}{\cdot} \Sigma(s)^{-1} \cdot Y(s) \,\right)
\\
& = &
	\left(\, \dfrac{ \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) }{\sigma^{2}} \,\right)^{-1}
	\cdot
	\left(\,
		\dfrac{\widehat{T}^{\,\textnormal{HT}}_{y}(s)}{\sigma^{2}}
	\,\right)
\;\; = \;\;
	\dfrac{\sigma^{2}}{\overset{{\color{white}.}}{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}}
	\cdot
	\dfrac{\widehat{T}^{\,\textnormal{HT}}_{y}(s)}{\sigma^{2}}
\;\; = \;\;
	\dfrac{
		\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	}{
		\overset{{\color{white}.}}{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}
	}
\end{eqnarray*}
Consequently,
\begin{eqnarray*}
\left(\,T_{\mathbf{x}} \,-\, \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)\right)
\cdot
\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
& = &
	\left(\,T_{\mathbf{x}} \,-\, \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)\right)
	\cdot
	\left(\;
		\dfrac{
			\widehat{T}^{\,\textnormal{HT}}_{y}(s)
		}{
			\overset{{\color{white}.}}{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}
		}
	\;\right)
\\
& = &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\cdot
	\left(\;
		\dfrac{
			T_{\mathbf{x}}
		}{
			\overset{{\color{white}.}}{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}
		}
	\;\right)
	\; - \;
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
\end{eqnarray*}
We now see that
\begin{eqnarray*}
\widehat{T}^{\,\textnormal{RGR}}_{y,\mathbf{x}}(s)
& := &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\; + \;
	\left(\,T_{\mathbf{x}} - \widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s) \,\right)
	\cdot
	\widehat{\mathbf{B}}_{y,\mathbf{x}}(s)
\\
& = &
	\widehat{T}^{\,\textnormal{HT}}_{y}(s)
	\cdot
	\dfrac{
		T_{\mathbf{x}}
		}{
		\overset{{\color{white}.}}{\widehat{T}^{\,\textnormal{HT}}_{\mathbf{x}}(s)}
		}
\;\; = \;\;
	\left(\,\underset{k \in s}{\sum}\;\dfrac{y_{k}}{\pi_{k}}\,\right)
	\cdot
 	\dfrac{
		T_{\mathbf{x}}
		}{
		\underset{k \in s}{\sum}\;x_{k}/\pi_{k}
		}
	\,,\quad\quad\textnormal{for each \,$s \in \mathcal{S}$}.
\end{eqnarray*}
This completes the proof of the Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
