
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Calibrated linear estimators for (multivariate) population totals}
\setcounter{theorem}{0}

\begin{definition}
\mbox{}
\vskip 0.1cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w} : \mathcal{S} \longrightarrow \Re^{m}$
be an $\Re^{m}$-valued random variable which is linear in the $\Re^{m}$-valued
population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$, i.e.
\begin{equation*}
\begin{array}{cccl}
\widehat{\mathbf{T}}_{\mathbf{y};w} : & \mathcal{S} & \longrightarrow & \Re^{m} \\
     & s &\longmapsto & \underset{k\in s}{\sum}\,w_{k}(s)\cdot\mathbf{y}_{k} \;\; = \;\; \underset{k\in U}{\sum}\,I_{k}(s)\,w_{k}(s)\cdot\mathbf{y}_{k},
\end{array}
\end{equation*}
where, for each $k \in U$, $w_{k} : \mathcal{S} \longrightarrow \Re$ is itself an $\Re$-valued random variable,
and $I_{k} : \mathcal{S} \longrightarrow \{0,1\}$ is the indicator random variable defined by:
\begin{equation*}
I_{k}(s)
\;\;=\;\;
\left\{
\begin{array}{cl}
1, & \textnormal{if $k \in s$}, \\
0, & \textnormal{otherwise}
\end{array}
\right.
\end{equation*}
Let $x : U \longrightarrow \Re$ be an $\Re$-valued population parameter
and  $T_{x} := \underset{k\in U}{\sum}\,x_{k}$.\\
Then, $\widehat{\mathbf{T}}_{\mathbf{y};w}$ is said to be
\underline{\emph{calibrated with respect to $x$}} if
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k} \; = \; T_{x},
\;\;\textnormal{for {\color{red}each} $s \in \mathcal{S}$}.
\end{equation*}
\end{definition}

\begin{example}
\mbox{}
\vskip 0.1cm
\noindent
If the sampling design has fixed sample size and each of its first-order inclusion probabilities is strictly positive, then Horvitz-Thompson estimator is calibrated with respect to the first-order inclusion probabilities.
\vskip 0.2cm
\noindent
To see this, let $U = \{1,2,\ldots,N\}$ be a finite population,
$\mathbf{y} : U \longrightarrow \Re^{m}$ a population parameter, and
$p : \mathcal{S}\subset\mathcal{P}(U) \longrightarrow (0,1]$ a sampling design
such that $\pi_{k} := \sum_{s \ni k}p(s) > 0$, for each $k \in U$.
The Horvitz-Thompson estimator
$\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}} : \mathcal{S} \longrightarrow \Re$
is then well-defined and is given by:
\begin{equation*}
\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}\!(s)
\;\; := \;\; \sum_{k \in s}\,\dfrac{\mathbf{y}_{k}}{\pi_{k}}
\end{equation*}
Let $x : U \longrightarrow \Re$ be defined by
\begin{equation*}
x_{k} \; = \; \pi_{k},
\;\;\textnormal{for each $k \in U$},
\end{equation*}
i.e. $x_{k}$ is simply the inclusion probability of $k \in U$
under the sampling design $p : \mathcal{S} \longrightarrow (0,1]$.

Now, suppose that the sampling design has a fixed sample size $n$,
and we shall show that $\widehat{\mathbf{T}}^{\textnormal{HT}}_{\mathbf{y}}$
is consequently calibrated with respect to $x : U \longrightarrow \Re$.
Indeed, recall that the weights of the Horvitz-Thompson estimator are simply
$w_{k}(s) = 1 / \pi_{k}$, for each $k \in U$ and each $s \in \mathcal{S}$.
Hence,
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k}
\;\;=\;\; \sum_{k \in s}\,\dfrac{1}{\pi_{k}}\,\pi_{k}
\;\;=\;\; \sum_{k \in s}\,1
\;\;=\;\; \left(\begin{array}{c} \textnormal{sample} \\ \textnormal{size of $s$} \end{array}\right)
\;\;=\;\; n,
\end{equation*}
since the sampling design has fixed size $n$.
On the other hand,
\begin{equation*}
T_{x}
\;\;=\;\;\sum_{k\in U}x_{k}
\;\;=\;\; \sum_{k \in U}\pi_{k}
\;\;=\;\; \sum_{k \in U}E\!\left[\;I_{k}\;\right]
\;\;=\;\; E\!\left[\;\sum_{k \in U}I_{k}\;\right]
\;\;=\;\; E\!\left[\begin{array}{c} \textnormal{sample} \\ \textnormal{size} \end{array}\right]
\;\;=\;\; n,
\end{equation*}
again since the sample size is fixed and equals $n$.
Therefore, we have, for any $s \in \mathcal{S}$,
\begin{equation*}
\sum_{k \in s}\,w_{k}(s)\,x_{k}
\;\;=\;\; n
\;\;=\;\; T_{x}
\end{equation*}
Therefore, the Horvitz-Thompson estimator, under the assumption of fixed sample size,
is indeed calibrated with respect to the inclusion probabilities $x : U \longrightarrow \Re$,
$x _{k} = \pi_{k} := \sum_{s \ni k}\,p(s)$, for each $k \in U$. \qed
\end{example}

\begin{proposition}
\label{proposition:calibratedLinear:MSE}
\mbox{}
\vskip 0.1cm
\noindent
Let $\widehat{\mathbf{T}}_{\mathbf{y};w,x} : \mathcal{S} \longrightarrow \Re^{m}$ be an $\Re^{m}$-valued random variable 
which is linear in the $\Re^{m}$-valued population parameter $\mathbf{y} : U \longrightarrow \Re^{m}$
and calibrated with respect to the population parameter $x : U \longrightarrow \Re$, with $x_{k} \neq 0$ for each $k \in U$.\\
Then, the mean squared error matrix of $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ as an estimator of $\mathbf{T}_{\mathbf{y}}$ is given by:
\begin{equation*}
\MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
\;=\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\;\in\;\Re^{m \times m},
\;\;\textnormal{where}\;\;
a_{ik} \;:=\; E\!\left[\,\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\,\right].
\end{equation*}
\end{proposition}

\proof
\begin{eqnarray*}
\MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
&=& E\!\left[\;\left(\widehat{\mathbf{T}}_{\mathbf{y};w,x} - \mathbf{T}_{\mathbf{y}}\right)\cdot\left(\widehat{\mathbf{T}}_{\mathbf{y};w,x} - \mathbf{T}_{\mathbf{y}}\right)^{T}\;\right]
\;\;=\;\; E\!\left[\;\left(\sum_{i\in U}(I_{i}w_{i}-1)\,\mathbf{y}_{i}\right)\cdot\left(\sum_{k\in U}(I_{k}w_{k}-1)\,\mathbf{y}_{k}\right)^{T}\;\right] \\
&=& \sum_{i\in U}\,\sum_{k\in U}\,E\!\left[\;\left(I_{i}w_{i}-1\right)\left(I_{k}w_{k}-1\right)\;\right]\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T}
\;\;=\;\; \sum_{k \in U}\,a_{kk}\cdot\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T} + \underset{i\neq k}{\sum_{i,k\in U}}\,a_{ik}\cdot\mathbf{y}_{i}\cdot\mathbf{y}_{k}^{T} \\
&=& \sum_{k \in U}\,a_{kk}\left(\dfrac{\mathbf{y}_{k}\cdot\mathbf{y}_{k}^{T}}{x_{k}^{2}}\right)x_{k}^{2}
+ \underset{i\neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\cdot\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\,x_{i}\,x_{k}
\end{eqnarray*}
On the other hand,
\begin{eqnarray*}
&&   - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\,x_{i}\,x_{k} \\
&=& - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}
\left[\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
- \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
- \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\right]\,x_{i}\,x_{k} \\
&=& - \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left[
    \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
+ \underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}\,x_{i}\,x_{k}
\end{eqnarray*}
Thus, the proof of the present Proposition will be complete once we show:
\begin{equation*}
\underset{
\dfrac{1}{2}\,\underset{i {\color{red}=} k}{\underset{i,k\in U}{\textnormal{\large$\sum$}}}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
}{
\underbrace{\sum_{k \in U}\,a_{kk}\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}x_{k}^{2}}
}
\;\;=\;\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k},
\end{equation*}
which is equivalent to:
\begin{equation}\label{MSE}
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left[
   \left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}
+ \left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
\right]\,x_{i}\,x_{k}
\;\;=\;\; 0.
\end{equation}
Observe that
\begin{eqnarray*}
\textnormal{LHS\eqref{MSE}}
&=&
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}x_{i}\,x_{k}
+
\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)\left(\dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}x_{i}\,x_{k} \\
&=&
2\,\sum_{i\in U}\,\sum_{k\in U}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}x_{i}\,x_{k}
\;\;=\;\; 2\,\sum_{i\in U}\,x_{i}\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)\left(\dfrac{\mathbf{y}_{i}}{x_{i}}\right)^{T}\left(\sum_{k\in U}\,a_{ik}x_{k}\right).
\end{eqnarray*}
Hence, \eqref{MSE} follows once we show
\begin{equation}\label{MSE2}
\sum_{k\in U}\,a_{ik}x_{k} = 0,
\quad\textnormal{for each $i \in U$.}
\end{equation}
Lastly, we now claim that \eqref{MSE2} follows from the hypothesis that
$\widehat{T}_{y;w;x}$ is calibrated with respect to $x$.
Indeed,
\begin{eqnarray*}
\sum_{k \in U}\,a_{ik}x_{k}
&=& \sum_{k\in U}\,E\!\left[\;(I_{i}w_{i}-1)(I_{k}w_{k}-1)\;\right] x_{k}
\;\;=\;\; \sum_{k\in U}\,\left[\;\sum_{s\in\mathcal{S}}p(s)(I_{i}(s)w_{i}(s)-1)(I_{k}(s)w_{k}(s)-1)\;\right] x_{k} \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(I_{i}(s)w_{i}(s)-1\right)\cdot\left[\;\sum_{k\in U}\left(I_{k}(s)w_{k}(s)-1\right)\cdot x_{k}\;\right] \\
&=& \sum_{s\in\mathcal{S}}\,p(s)\cdot\left(I_{i}(s)w_{i}(s)-1\right)\cdot\underset{0}{\left[\underbrace{\;\left(\sum_{k\in s}w_{k}(s)\,x_{k}\right) - T_{x}}\;\right]}\\
&=& 0
\end{eqnarray*}
The proof of the present Proposition is now complete. \qed

\begin{proposition}[The Yates-Grundy-Sen Variance Estimator for calibrated linear population total estimators]
\mbox{}
\vskip 0.1cm
\noindent
Let $p : \mathcal{S} \longrightarrow (0,1]$ be a sampling design each of whose
first-order and second-order inclusion probabilities is strictly positively.
Let $\widehat{\mathbf{T}}_{\mathbf{y};w,x} : \mathcal{S} \longrightarrow \Re^{m}$
be a random variable  which is linear in the population parameter
$\mathbf{y} : U \longrightarrow \Re^{m}$
and calibrated with respect to the population parameter $x : U \longrightarrow \Re$,
with $x_{k} \neq 0$ for each $k \in U$.
Suppose that $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ is an unbiased estimator for
$\mathbf{T}_{\mathbf{y}} := \underset{k \in U}{\sum}\mathbf{y}_{k}$, for arbitrary $\mathbf{y}$.
Then, the following is an unbiased estimator of the variance
$\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]$ of $\widehat{\mathbf{T}}_{\mathbf{y};w,x} $:
For each $s \in \mathcal{S}$ admissible in the sampling design $p : \mathcal{S} \longrightarrow (0,1]$,
\begin{equation*}
\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]\!(s)
\;\;:=\;\;
-\dfrac{1}{2}\underset{i\neq k}{\sum_{i,k\in s}}\left(w_{i}(s)w_{k}(s) - \dfrac{1}{\pi_{ik}}\right)
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\end{equation*}
\end{proposition}

\noindent
\textbf{Terminology:}\quad
$\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]$ is called the Yates-Grundy-Sen Variance Estimator.

\vskip 0.5cm
\proof
Since $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ is an unbiased estimator for $\mathbf{T}_{\mathbf{y}}$ by hypothesis, we have
$\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right] = \MSE\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]$.
By Proposition \ref{proposition:calibratedLinear:MSE}, we thus have:
\begin{equation*}
\Var\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x}\;\right]
\;\;=\;\;
- \dfrac{1}{2}\,\underset{i \neq k}{\sum_{i,k\in U}}\,a_{ik}\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{2}\,x_{i}\,x_{k},
\quad\textnormal{where}\;\;
a_{ik} \;:=\; E\!\left[\;\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\;\right].
\end{equation*}
On the other hand,
\begin{eqnarray*}
E\!\left(\;\widehat{\Var}\!\left[\;\widehat{\mathbf{T}}_{\mathbf{y};w,x} \;\right]\;\right)
&=& 
-\dfrac{1}{2}\underset{i \neq k}{\sum_{i,k\in U}}
E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right]
\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)\cdot\left(\dfrac{\mathbf{y}_{i}}{x_{i}} - \dfrac{\mathbf{y}_{k}}{x_{k}}\right)^{T}
x_{i}\,x_{k}
\end{eqnarray*}
Thus, it remains only to show:
\begin{equation*}
a_{ik} \;=\; E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right].
\end{equation*}
Now,
\begin{equation*}
E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right]
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - \dfrac{1}{\pi_{ik}}E\!\left[\;I_{i}I_{k}\;\right]
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - \dfrac{1}{\pi_{ik}}\,\pi_{ik}
\;\;=\;\; E\!\left[\;I_{i}I_{k}w_{i}w_{k}\;\right] - 1,
\end{equation*}
and
\begin{eqnarray*}
a_{ik}
&=& E\!\left[\;\left(I_{i}\,w_{i}-1\right)\left(I_{k}\,w_{k}-1\right)\;\right]
\;\;=\;\; E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - E\!\left[\;I_{i}\,w_{i}\;\right] - E\!\left[\;I_{k}\,w_{k}\;\right] + 1 \\
&=&E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - 1 - 1 + 1
\;\;=\;\; E\!\left[\;I_{i}\,I_{k}\,w_{i}\,w_{k}\;\right] - 1 \\
&=& E\!\left[\;I_{i}I_{k}\left(w_{i}w_{k} - \dfrac{1}{\pi_{ik}}\right)\;\right],
\end{eqnarray*}
where third last equality follows from Proposition \ref{proposition:Unbiasedness} and
the unbiasedness hypothesis on $\widehat{\mathbf{T}}_{\mathbf{y};w,x}$ as an estimator for $\mathbf{T}_{\mathbf{y}}$.
The proof of the present Proposition is now complete.  \qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
