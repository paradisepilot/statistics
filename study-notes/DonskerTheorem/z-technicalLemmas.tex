 
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Technical Lemmas}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

Note that
\,$X_{n}\,\overset{d}{\longrightarrow}\,X$\,
and
\,$Y_{n}\,\overset{d}{\longrightarrow}\,Y$\,
does NOT in general imply
\,$X_{n} + Y_{n}\,\overset{d}{\longrightarrow}\,X + Y$.
But the implication does hold if $X_{n}$ and $Y_{n}$
are independent for each $n \in \N$, and 
both $X$ and $Y$ are Gaussian random variables,
as the following Proposition shows.

\begin{proposition}\label{GaussianDistributionLimit}
\quad
Let $k \in \N$ be fixed.
Suppose:
\begin{itemize}
\item	For each $n \in \N$,
		\begin{equation*}
		Y^{(n)}_{1},\, Y^{(n)}_{2},\, \ldots, Y^{(n)}_{k} \,:\,\Omega^{(n)}\,\longrightarrow\,\Re
		\end{equation*}
		are independent $\Re$-valued random variables defined on the probability space $\Omega^{(n)}$.
\item	For each $i = 1, 2, \ldots, k$,
		\begin{equation*}
		Y^{(n)}_{i} \; \overset{d}{\longrightarrow} \; N\!\left(\,\mu_{i},\,\sigma^{2}_{i}\,\right),
		\quad
		\textnormal{as \;$n \longrightarrow \infty$}.
		\end{equation*}
\end{itemize}
Then, for any $c_{1},\, c_{2},\, \ldots,\, c_{k} \,\in\, \Re$,
\begin{equation*}
\sum_{i\,=\,1}^{k}\,c_{i}\,Y^{(n)}_{i} \; \overset{d}{\longrightarrow} \;
N\!\left(\,\sum_{i\,=\,1}^{k}\,c_{i}\,\mu_{i} \;,\; \sum_{i\,=\,1}^{k}\,c_{i}^{2}\,\sigma_{i}^{2}\;\right),
\quad
\textnormal{as \;$n \longrightarrow \infty$}.
\end{equation*}
\end{proposition}
\proof
Let \,$Y^{(n)} \,:=\, \overset{k}{\underset{i\,=\,1}{\sum}}\,c_{i}\,Y^{(n)}_{i}$.\,
Let \,$\varphi_{X}$\, denote the characteristic function of a $\Re$-valued random variable $X$.
Then,
\begin{eqnarray*}
\varphi_{Y^{(n)}}(t)
&=& \varphi_{\sum_{i}^{k}c_{i}Y^{(n)}_{i}}(t)
\\
&=& \prod_{i\,=\,1}^{k}\,\varphi_{c_{i}Y^{(n)}_{i}}(t),
\quad\textnormal{since \,$Y^{(n)}_{1},\, \ldots,\, Y^{(n)}_{k}$\, are independent}
\\
&=& \prod_{i\,=\,1}^{k}\,\varphi_{Y^{(n)}_{i}}(\,c_{i}t\,)
\\
&\longrightarrow&
\prod_{i\,=\,1}^{k}\,\exp\!\left\{\;\sqrt{-1}\,\mu_{i}\,(c_{i}\,t) \;-\; \dfrac{1}{2}\,\sigma_{i}^{2}\,(\,c_{i}t\,)^{2}\;\right\}
\\
&=&
\exp\!\left\{\;
	\sqrt{-1}\left(\sum_{i\,=\,1}^{k}c_{i}\mu_{i}\right)t
	\;-\; \dfrac{1}{2}\left(\sum_{i\,=\,1}^{k}c_{i}^{2}\sigma_{i}^{2}\right)t^{2}
	\;\right\},
\quad\textnormal{as \,$n \longrightarrow \infty$},
\end{eqnarray*}
where the second and third equalities follow from the properties of
characteristic functions of random variables (see p.21, \cite{Ferguson1996}),
while the expression of the limit follows from the fact that the characteristic
function $\varphi_{Z}$ of a random variable $Z$ with distribution
$N\!\left(\,\mu,\,\sigma^{2}\,\right)$ is
\begin{equation*}
\varphi_{Z}
\;\;=\;\;
\exp\!\left\{\;\sqrt{-1}\,\mu t \;-\; \dfrac{1}{2}\,\sigma^{2}\,t^{2}\;\right\}.
\end{equation*}
The Proposition now follows immediately from
the L\'{e}vy-Cram\'{e}r Continuity Theorem (Theorem 1.9(ii), p.56, \cite{Shao2003}).
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
