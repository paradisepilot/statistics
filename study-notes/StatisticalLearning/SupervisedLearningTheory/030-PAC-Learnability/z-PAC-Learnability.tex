
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Uniform Convergence Property implies Agnostic PAC Learnability}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Agnostic PAC Learnability]
\mbox{}\vskip 0.1cm
\noindent
A hypothesis class
\,$\mathcal{H} = \left\{\; h : \mathcal{X} \overset{{\color{white}-}}{\longrightarrow} \{0,1\} \;\right\}$\,
is said to be \underline{\textbf{agnostically PAC{\color{white}q}learnable}} if
there exist
a learning algorithm
\begin{equation*}
\mathcal{F}
\;\; : = \;\;
	\left\{\;\,
		h_{n} : \left(\;\mathcal{X} \overset{{\color{white}.}}{\times} \{0,1\} \;\right)^{n}
		\; \longrightarrow \;\mathcal{H}
		\;\;\right\}_{n\in\N}
\end{equation*}
and a function
\begin{equation*}
n_{\mathcal{F}} : (0,1) \times (0,1) \longrightarrow \N
\end{equation*}
such that
\begin{itemize}
\item
	for each \,$(\varepsilon,\delta) \in (0,1) \times (0,1)$,\, and
\item
	for each $\left(\,\mathcal{X}\times\{0,1\}\right)$-valued random variable
	$(X,Y) : (\Omega,\mathcal{A},\mu) \longrightarrow \mathcal{X}\times\{0,1\}$\,,
%\item
%	for each probability space $(\Omega,\mathcal{A},\mu)$ and
%	random variables $X : \Omega \longrightarrow \Re^{d}$,
%	$Y : \Omega \longrightarrow \{0,1\}$\,,
\end{itemize}
we have:
\begin{equation*}
P_{{\color{red}D_{n}}}\!\!\left(\;
	P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{h}_{n}({\color{red}D_{n}})(X) \neq Y \,\right)
		\,-\,
		\underset{h\in\mathcal{H}}{\inf}\,P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\,\leq\,
		\varepsilon
	\;\right)
\;\; \geq \;\;
	1 - \delta\,,
\quad
	\textnormal{for every \,$n \,\geq\, n_{\mathcal{F}}(\varepsilon,\delta)$}\,,
\end{equation*}
where
\,$D_{n} = \left((X_{1},Y_{1}),(X_{2},Y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(X_{n},Y_{n})\right)
	: \Omega \longrightarrow
	\left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\,\right)^{n}$\,
is such that the
$(X_{i},Y_{i})$'s are I.I.D. copies of $(X,Y)$.
We will call the learning algorithm $\mathcal{F}$ a \underline{\textbf{viable learning algorithm}}
of the agnostically PAC learnable hypothesis class $\mathcal{H}$, and the function
\,$n_{\mathcal{F}}$\, a \underline{\textbf{minimum sample size function}} of the $\mathcal{F}$.
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{remark}\quad
PAC stands for ``probably approximately correct.''
\end{remark}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{remark}[Agnostic PAC Learnability in plain English]
\mbox{}\vskip 0.1cm
\noindent
A hypothesis class $\mathcal{H}$ is \,\textbf{agnostically PAC learnable}\, if
there exists a learning algorithm
\begin{equation*}
\mathcal{F} \; = \; \left\{\;
	h_{n} : \left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\right)^{n} \longrightarrow\mathcal{H}
	\;\right\}_{n\in\N}
\end{equation*}
such that, given any ``accuracy tolerance'' $\varepsilon > 0$ and
``confidence level'' $0 <1 - \delta < 1$, 
the event that
\begin{equation*}
\textnormal{$h_{n}$ has a probability of error within the $\varepsilon$-ball of
the best-in-class probability of error}
\end{equation*}
has probability exceeding $1 - \delta$, for all sufficiently large $n$.
\end{remark}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{remark}
\mbox{}\vskip 0.1cm
\noindent
Suppose the hypothesis class $\mathcal{H}$ is \,\textbf{agnostically PAC learnable}\,
with viable learning algorithm
\begin{equation*}
\mathcal{F} \; = \; \left\{\;
	h_{n} : \left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\right)^{n} \longrightarrow\mathcal{H}
	\;\right\}_{n\in\N}\,.
\end{equation*}
Then,
for each $\left(\,\mathcal{X}\times\{0,1\}\right)$-valued random variable
$(X,Y) : (\Omega,\mathcal{A},\mu) \longrightarrow \mathcal{X}\times\{0,1\}$\,,
and for each $\varepsilon > 0$, we have
\begin{equation*}
\underset{n\rightarrow\infty}{\lim}\;\,P_{{\color{red}D_{n}}}\!\!\left(\;
	P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{h}_{n}({\color{red}D_{n}})(X) \neq Y \,\right)
		\,-\,
		\underset{h\in\mathcal{H}}{\inf}\,P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\,\leq\,
		\varepsilon
	\;\right)
\;\; = \;\;
	1\,,
\end{equation*}
where
\,$D_{n} = \left((X_{1},Y_{1}),(X_{2},Y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(X_{n},Y_{n})\right)
	: \Omega \longrightarrow
	\left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\,\right)^{n}$\,
is such that the
$(X_{i},Y_{i})$'s are I.I.D. copies of $(X,Y)$.
\end{remark}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 1.0cm
\begin{remark}[Countrapositive of Agnostic PAC Learnability]
\mbox{}\vskip 0.1cm
\noindent
A hypothesis class
\,$\mathcal{H} = \left\{\; h : \mathcal{X} \overset{{\color{white}-}}{\longrightarrow} \{0,1\} \;\right\}$\,
is NOT agnostically PAC learnable if, for every learning algorithm
\begin{equation*}
\left\{\;\,
	h_{n} : \left(\;\mathcal{X} \overset{{\color{white}.}}{\times} \{0,1\} \;\right)^{n}
	\; \longrightarrow \;\mathcal{H}
	\;\;\right\}_{n\in\N}\,,
\end{equation*}
there exist
\begin{itemize}
\item
	$\varepsilon,\, \delta > 0$,\, and
\item
	a probability space $(\Omega,\mathcal{A},\mu)$ and
	random variables $X : \Omega \longrightarrow \Re^{d}$,
	$Y : \Omega \longrightarrow \{0,1\}$\,,
\end{itemize}
such that, given any $n_{0} \in \N$, we have
\begin{equation*}
P_{D_{n}}\!\!\left(\;
	P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{h}_{n}(D_{n})(X) \neq Y \,\right)
		\,-\,
		\underset{h\in\mathcal{H}}{\inf}\,P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\,\leq\,
		\varepsilon
	\;\right)
\;\; {\color{red}<} \;\;
	1 - \delta\,,
\quad
	\textnormal{for {\color{red}some} \,$n \,>\, n_{0}$}\,,
\end{equation*}
where
\,$D_{n} = \left((X_{1},Y_{1}),(X_{2},Y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(X_{n},Y_{n})\right)
	: \Omega \longrightarrow
	\left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\,\right)^{n}$\,
is such that the
$(X_{i},Y_{i})$'s are I.I.D. copies of $(X,Y)$.
\end{remark}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 1.0cm
\begin{definition}[Uniform Convergence Property]
\mbox{}\vskip 0.1cm
\noindent
A hypothesis class
\,$\mathcal{H} = \left\{\; h : \mathcal{X} \overset{{\color{white}-}}{\longrightarrow} \{0,1\} \;\right\}$\,
is said to possess the \underline{\textbf{uniform convergence property}} if
there exists a function
\begin{equation*}
n^{\textnormal{UC}}_{\mathcal{H}} : (0,1) \times (0,1) \longrightarrow \N
\end{equation*}
such that
\begin{itemize}
\item
	for each \,$(\varepsilon,\delta) \in (0,1) \times (0,1)$,\, and
\item
	for each $\left(\,\mathcal{X}\times\{0,1\}\right)$-valued random variable
	$(X,Y) : (\Omega,\mathcal{A},\mu) \longrightarrow \mathcal{X}\times\{0,1\}$\,,
\end{itemize}
we have:
\begin{equation*}
P_{{\color{red}D_{n}}}\!\!\left(\;\,
	\underset{h \in \mathcal{H}}{\sup}\;\left\vert\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
			I_{\left\{h(X_{i})\,\overset{{\color{white}.}}{\neq}\,Y_{i}\right\}}
			\,-\,
			P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
		\;\right\vert
	\;\leq\;
		\varepsilon
	\;\right)
\;\; \geq \;\;
	1 - \delta\,,
\quad
	\textnormal{for every \,$n \,\geq\, n^{\textnormal{UC}}_{\mathcal{H}}(\varepsilon,\delta)$}\,,
\end{equation*}
where
\,$D_{n} = \left((X_{1},Y_{1}),(X_{2},Y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(X_{n},Y_{n})\right)
	: \Omega \longrightarrow
	\left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\,\right)^{n}$\,
is such that the
$(X_{i},Y_{i})$'s are I.I.D. copies of $(X,Y)$.
We will all the function \,$n^{\textnormal{UC}}_{\mathcal{H}}$\,
a \underline{\textbf{minimum sample size function}} of $\mathcal{H}$.
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 1.0cm
\begin{theorem}[Uniform Convergence Property implies Agnostic PAC Learnability]
\label{Thm:UCPimpliesAPAC}
\mbox{}\vskip 0.1cm
\noindent
Let
\,$\mathcal{H} = \left\{\; h : \mathcal{X} \overset{{\color{white}-}}{\longrightarrow} \{0,1\} \;\right\}$\,
be a hypothesis class.
\vskip 0.2cm
\noindent
Suppose:
\begin{itemize}
\item
	$\mathcal{H}$\, possesses the \textbf{uniform convergence property}
	with \textbf{minimum sample size function}
	\begin{equation*}
	n^{\textnormal{UC}}_{\mathcal{H}} : (0,1) \times (0,1) \longrightarrow \N
	\end{equation*}
\item
	$\mathcal{H}$\, admits an Empirical Risk Minimization (ERM) learning algorithm
	\begin{equation*}
	\textnormal{ERM} \;=\; \left\{\;
		\overset{{\color{white}.}}{\textnormal{ERM}_{n}} :
		\left(\mathcal{X}\times\{0,1\}\right)^{n} \longrightarrow \, \mathcal{H}
		\;\right\}_{n\in\N}
	\end{equation*}
	i.e.
	\begin{equation*}
	\textnormal{ERM}_{n}({\color{red}d_{n}})
	\;\; \in \;\;
		\mathcal{H}
		\;\; \bigcap \;\;
		\underset{h\in\mathcal{H}}{\argmin}\;\left\{\;
			\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
			I_{\left\{h({\color{red}x_{i}})\,\overset{{\color{white}.}}{\neq}\,{\color{red}y_{i}}\right\}}
			\;\right\}
	\end{equation*}
	for each
	\,$d_{n} = \left((x_{1},y_{1}),(x_{2},y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(x_{n},y_{n})\right)
	\in \left(\,\mathcal{X} \times \{0,1\} \right)^{n}$\,.
\end{itemize}
Then, \,$\mathcal{H}$\, is \textbf{agnostically PAC learnable} with
\,$\textnormal{ERM} \; = \;
	\left\{\;
		\overset{{\color{white}.}}{\textnormal{ERM}_{n}} :
		\left(\mathcal{X}\times\{0,1\}\right)^{n} \longrightarrow \, \mathcal{H}
		\;\right\}
$\,
as a viable learning algorithm, and
\begin{equation*}
n_{\textnormal{ERM}}(\varepsilon,\delta) \;\; := \;\; n^{\textnormal{UC}}_{\mathcal{H}}(\varepsilon/2,\delta)\,,
\end{equation*}
as a minimum sample size function.
\end{theorem}
\proof

\vskip 0.3cm
\noindent
\textbf{Claim 1:}\quad
For each
\,$d_{n} = \left((x_{1},y_{1}),(x_{2},y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(x_{n},y_{n})\right)
\,\in\, \left(\,\mathcal{X} \times \{0,1\} \right)^{n}$\,,
we have
\begin{equation*}
	\underset{h \in \mathcal{H}}{\sup}\;\left\vert\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
			I_{\left\{h({\color{red}x_{i}})\,\overset{{\color{white}.}}{\neq}\,{\color{red}y_{i}}\right\}}
			\,-\,
			P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
		\;\right\vert
	\;\leq\;
		\dfrac{\varepsilon}{2}
\quad\Longrightarrow\;\;
	P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{\textnormal{ERM}}_{n}({\color{red}d_{n}})(X) \neq Y \,\right)
	\; \leq \;
		\underset{h\in\mathcal{H}}{\inf}\,P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
		\; + \; \varepsilon
\end{equation*}
\vskip 0.2cm
\noindent
Proof of Claim 1:\quad
Note the for each $h \in \mathcal{H}$, we have
\begin{eqnarray*}
P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{\textnormal{ERM}}_{n}({\color{red}d_{n}})(X) \neq Y \,\right)
& \leq &
	\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
	I_{\left\{\textnormal{ERM}({\color{red}d_{n}})(x_{i})\,\overset{{\color{white}.}}{\neq}\,y_{i}\right\}}	
	\; + \;
	\dfrac{\varepsilon}{2}\,,
	\quad\quad\quad
	\textnormal{by hypothesis on $D_{n}$}
\\
& \leq &
	\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
	I_{\left\{h(x_{i})\,\overset{{\color{white}.}}{\neq}\,y_{i}\right\}}	
	\; + \;
	\dfrac{\varepsilon}{2}\,,
	\quad\quad
	\textnormal{since $\textnormal{ERM}_{n}(d_{n})$ is a minimizer of empirical risk}
\\
& \leq &
	P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\; + \; \dfrac{\varepsilon}{2}
	\; + \; \dfrac{\varepsilon}{2}\,,
	\quad\quad\quad
	\textnormal{by hypothesis on $D_{n}$}
\\
& = &
	P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\; + \; \varepsilon
\end{eqnarray*}
Since the preceding inequality holds for each $h \in \mathcal{H}$, it follows immediately that
\begin{equation*}
P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{\textnormal{ERM}}_{n}({\color{red}d_{n}})(X) \neq Y \,\right)
\;\; \leq \;\;
	\underset{h\in\mathcal{H}}{\inf}\,P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\; + \; \varepsilon
\end{equation*}
This proves Claim 1.

\vskip 0.5cm
\noindent
We now continue the proof of the Theorem.
To this end, let $(\varepsilon,\delta) \in (0,1) \times (0,1)$ be given, and let
\begin{equation*}
D_{n} = \left((X_{1},Y_{1}),(X_{2},Y_{2}),\,\overset{{\color{white}\vert}}{\ldots}\,,(X_{n},Y_{n})\right)
	: \Omega \longrightarrow
	\left(\,\mathcal{X}\overset{{\color{white}.}}{\times}\{0,1\}\,\right)^{n}
\end{equation*}
be such that the
$(X_{i},Y_{i})$'s are I.I.D. copies of $(X,Y)$.
Then, by Claim 1,
for each $n > n^{\textnormal{UC}}_{\mathcal{H}}(\varepsilon/2,\delta)$, we have
\begin{eqnarray*}
&&
	\left\{\;\,
		{\color{red}\omega} \in \Omega
		\;\,\left\vert\;\;
		\underset{h \in \mathcal{H}}{\sup}\;\left\vert\,
			\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
				I_{\left\{h(X_{i}({\color{red}\omega}))\,\overset{{\color{white}.}}{\neq}\,Y_{i}({\color{red}\omega})\right\}}
				\,-\,
				P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
			\,\right\vert
		\;\leq\;
			\dfrac{\varepsilon}{2}
		\right.\;\right\}
\\
& \textnormal{\large$\subset$} &
	\left\{\;\,
		{\color{red}\omega} \in \Omega
		\;\,\left\vert\;\;
		P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{\textnormal{ERM}}_{n}(D_{n}({\color{red}\omega}))(X) \neq Y \,\right)
		\; \leq \;
			\underset{h\in\mathcal{H}}{\inf}\,P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
			\; + \; \varepsilon
		\right.\;\right\}\,,
\end{eqnarray*}
which trivially implies
\begin{eqnarray*}
&&
P_{{\color{red}D_{n}}}\!\!\left(\;
	P_{X,Y}\!\!\left(\,\overset{{\color{white}.}}{\textnormal{ERM}}_{n}({\color{red}D_{n}})(X) \neq Y \,\right)
		\,-\,
		\underset{h\in\mathcal{H}}{\inf}\,P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
	\,\leq\,
		\varepsilon
	\;\right)
\\
& \geq &
	P_{{\color{red}D_{n}}}\!\!\left(\;\,
		\underset{h \in \mathcal{H}}{\sup}\;\left\vert\;
			\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
				I_{\left\{h(X_{i})\,\overset{{\color{white}.}}{\neq}\,Y_{i}\right\}}
				\,-\,
				P\!\left(\,\overset{{\color{white}.}}{h}(X) \neq Y \,\right)
			\;\right\vert
		\;\leq\;
			\dfrac{\varepsilon}{2}
		\;\right)
\\
& \geq &
	1 - \delta\,,
	\quad
	\textnormal{for each $n > n^{\textnormal{UC}}_{\mathcal{H}}(\varepsilon/2,\delta)$,
	by the hypothesis of Uniform Convergence Property}
\end{eqnarray*}
This proves that $\mathcal{H}$ is indeed agnostically PAC learnable,
with viable learning algorithm \,$\textnormal{ERM}$:
\begin{equation*}
\textnormal{ERM}_{n}({\color{red}d_{n}})
\;\; \in \;\;
	\mathcal{H}
	\;\; \bigcap \;\;
	\underset{h\in\mathcal{H}}{\argmin}\;\left\{\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;
		I_{\left\{h({\color{red}x_{i}})\,\overset{{\color{white}.}}{\neq}\,{\color{red}y_{i}}\right\}}
		\;\right\}\,,
	\quad
	\textnormal{where \,$d_{n} = \left((x_{1},y_{1}),\,\overset{{\color{white}\vert}}{\ldots}\,,(x_{n},y_{n})\right)$}\,,
\end{equation*}
and minimum sample size function
\begin{equation*}
n_{\textnormal{ERM}}(\varepsilon,\delta) \;\; := \;\; n^{\textnormal{UC}}_{\mathcal{H}}(\varepsilon/2,\delta)\,.
\end{equation*}

\vskip 0.3cm
\noindent
This completes the proof of the Theorem.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
