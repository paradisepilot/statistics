
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Gradient tree boosting}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\renewcommand{\theenumi}{\arabic{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\labelenumii}{\textnormal{(\theenumii)}$\;\;$}

\begin{center}
\begin{minipage}{6in}
\begin{tcolorbox}[width=0.95\linewidth,colback=white,colframe=gray]
	\begin{center}
	{\color{white}.}\vskip 0.1cm
	\textbf{\large Algorithm: Gradient Tree Boosting}
	\end{center}
	\textbf{Input:}
	\begin{itemize}
	\item
		$\mathcal{X} = \Re^{p}$\, and \,$\mathcal{Y} = \Re$
	\item
		$(x_{1},y_{1}), \ldots, (x_{n},y_{n}) \in \mathcal{X} \times \mathcal{Y} = \Re^{p} \times \Re$.
	\item
		$\mathcal{B}$ is the collection of regression trees from $\mathcal{X} = \Re^{p}$ into $\mathcal{Y} = \Re$.
	\item
		$L : \mathcal{Y} \times \mathcal{Y} \longrightarrow [\,0,\infty)$, called the ``unit loss function''
	\item
		$K \in \N \,:=\, \{1,2,3,\ldots\}$.
	\end{itemize}
	\vskip 0.3cm
	\textbf{Algorithm:}
	\begin{enumerate}
	\item
		Initialization:
		Let $f_{0} : \mathcal{X} \longrightarrow \mathcal{Y}$ be a constant function from $\mathcal{X}$ into $\mathcal{Y}$
		defined by
		\begin{equation*}
		f_{0}
		\;\,\in\,
			\underset{\gamma \,\in\, \Re}{\argmin}\,
			\left\{\;
				\overset{n}{\underset{i=1}{\sum}}\;
				L\!\left(\,y_{i}\,,\,\overset{{\color{white}1}}{{\color{red}\gamma}}\,\right)
				\,\right\}
		\end{equation*}
	\item
		For each $k = 1, 2, ... , K$, 
		\begin{enumerate}
		\item
			compute
			\begin{equation*}
			g^{(k)}_{i}
			\;\; := \;\;
				-\,\dfrac{\partial L}{\partial f}\!\left(\,y_{i}\,,\,\overset{{{\color{white}.}}}{f_{k-1}(x_{i})}\,\right),
			\quad
			\textnormal{for \,$i = 1,2,\ldots,n$}
			\end{equation*}
		\item
			fit a regression tree to the data set:
			\begin{equation*}
			\mathscr{D}^{(k)}
			\;\; := \;\;
				\left\{\;
					(x_{1},g^{(k)}_{1})\,,
					(x_{2},g^{(k)}_{2})\,,
					\,\ldots\,,\,
					(x_{n},g^{(k)}_{n})
					\;\right\}
			\end{equation*}
			to obtain the terminal regions
			\begin{equation*}
			R^{(k)}_{1},\; R^{(k)}_{2},\; \ldots\,,\; R^{(k)}_{J^{(k)}}
			\end{equation*}
		\item
			for each $j = 1, 2, \ldots, J^{(k)}$, compute:
			\begin{equation*}
			\gamma^{(k)}_{j}
			\;\; := \;\;
				\underset{\gamma\,\in\,\Re}{\argmin}\,
				\left\{\;
					\underset{x_{i}\,\in\,R^{(k)}_{j}}{\sum}\;
					L\!\left(\,y_{i}\,,\,f_{k-1}(x_{i}) \overset{{\color{white}1}}{+} {\color{red}\gamma}\,\right)
					\,\right\}
			\end{equation*}
		\item
			update:
			\begin{equation*}
			f_{k}(x)
			\;\; := \;\;
				f_{k-1}(x)
				\; + \;
				\overset{J^{(k)}}{\underset{j\,=\,1}{\sum}}\;\,
				\gamma^{(k)}_{j} \cdot I\!\left(x \in R^{(k)}_{j}\right)
			\end{equation*}
		\end{enumerate}
	\end{enumerate}
	\textbf{Output:}
	\begin{equation*}
	f_{K}(x)
	\;\; := \;\;
		f_{0}
		\; + \;
		\overset{K}{\underset{k\,=\,1}{\sum}}\;\,
		\overset{J^{(k)}}{\underset{j\,=\,1}{\sum}}\;\,
		\gamma^{(k)}_{j} \cdot I\!\left(x \in R^{(k)}_{j}\right)
	\end{equation*}
\end{tcolorbox}
\end{minipage}
\end{center}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\noindent
Consider the \textbf{empirical loss functional}
\begin{equation*}
\mathscr{L}(\,f\,;\,x,y\,)
\;\; := \;\;
	\overset{n}{\underset{i\,=\,1}{\sum}}\;\,
	L\!\left(\,y_{i}\,,\,\overset{{\color{white}.}}{f(x_{i})}\right)
\end{equation*}
Assuming that $\mathscr{L}(\,f\,;\,x,y\,)$ possesses sufficient smoothness,
Taylor's theorem gives:
\begin{eqnarray*}
\mathscr{L}(\,f + t \cdot g\,;\,x,y\,)
\,-\,
\mathscr{L}(\,f \,;\,x,y\,)
& = &
	\overset{n}{\underset{i\,=\,1}{\sum}}\,
	\left\{\,
		\overset{{\color{white}1}}{L\!\left(\,y_{i}\,,\,\overset{{\color{white}.}}{f(x_{i}) + t \cdot g(x_{i})}\right)}
		\, - \,
		L\!\left(\,y_{i}\,,\,\overset{{\color{white}.}}{f(x_{i})}\right)
		\,\right\}
\\
& = &
	\overset{n}{\underset{i\,=\,1}{\sum}}\,
	\left\{\;
		\dfrac{\partial L}{\partial f}\!\left(\,y_{i}\,,\,\overset{{\color{white}.}}{f(x_{i})}\right) \cdot t \cdot g(x_{i})
		\, + \,
		\mathcal{O}\!\left(\,\vert\,t\,\vert^{2}\,\right)
		\,\right\}
\end{eqnarray*}
Hence, the directional derivative of $\mathscr{L}(\,f\,;\,x,y\,)$
with respect to $f$, in the direction of $g$, is given by:
\begin{eqnarray*}
\underset{t \rightarrow 0}{\lim}\;\,
\dfrac{1}{t}
\left\{\;
	\overset{{\color{white}1}}{\mathscr{L}(\,f + t \cdot g\,;\,x,y\,)}
	\,-\,
	\mathscr{L}(\,f \,;\,x,y\,)
	\,\right\}
& = &
	\overset{n}{\underset{i\,=\,1}{\sum}}\;
	\dfrac{\partial L}{\partial f}\!\left(\,y_{i}\,,\,\overset{{\color{white}.}}{f(x_{i})}\right) \cdot g(x_{i})
\end{eqnarray*}
Hence, the direction of maximal decrease of $\mathscr{L}(\,f\,;\,x,y\,)$ at $f$ is $g$, where $g$ satisfies:
\begin{equation*}
\left(%\!\!\!\!\!\!\!\!\!
\begin{array}{c}
	% {\color{white}\dfrac{\partial L}{\partial f}} g(x_{1})
	\overset{{\color{white}1}}{g(x_{1})}
	\\
	\underset{{\color{white}1}}{\overset{{\color{white}1}}{\vdots}}
	\\
	%{\color{white}\dfrac{\partial L}{\partial f}} g(x_{n})
	\underset{{\color{white}1}}{g(x_{n})}
	\end{array}\right)
\;\; \propto \;\;
	\left(\begin{array}{c}
		-\, \dfrac{\partial L}{\partial f}\!\left(\,y_{1}\,,\,\overset{{\color{white}.}}{f(x_{1})}\right)
		\\
		\vdots
		\\
		-\, \dfrac{\partial L}{\partial f}\!\left(\,y_{n}\,,\,\overset{{\color{white}.}}{f(x_{n})}\right)
		\end{array}\right)
\end{equation*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
