
\section{Support Vector Machines for Binary Classification}
\setcounter{theorem}{0}

\vskip 0.5cm
\noindent
\textbf{Summary}
\begin{itemize}
\item
	We formulate the training procedure for support vector machines for binary classification
	as a certain constrained optimization problem (the Primal Problem).
\item
	We formulate the corresponding Dual Problem (another constrained optimization problem).
\item
	Any solution to the Dual Problem gives rise to solution to the Primal Problem.
\item
	Via the Dual formulation, we see that the solution to the Primal Problem in fact
	depends only on the inner products between the predictor vectors
	(rather than explicitly on the predictor vectors themselves).
	This observation opens up the possibility of first mapping the predictor vectors
	into some \textit{reproducing kernel Hilbert space} (RKHS), and then
	apply SVM's on their images in that RKHS instead,
	thereby allowing for non-linear separation boundaries in the original predictor space.
\end{itemize}

\vskip 0.8cm
\noindent
\textbf{Reminder}
\begin{itemize}
\item  Let $\mathbf{n}, \mathbf{z} \in \Re^{d}$, with $\mathbf{n} \neq \mathbf{0}$, be given.
          The hyperplane $H_{\mathbf{n},\mathbf{z}} \subset \Re^{d}$ with normal vector $\mathbf{n}$ and
          containing the point $\mathbf{z}$ is given by: 
          \begin{eqnarray*}
          H_{\mathbf{n},\mathbf{z}}
          & := &
          \left\{\;
          	\left.
          	\mathbf{x}\overset{{\color{white}1}}{\in}\Re^{d}
		\;\;\right\vert\;
			\left\langle\,\mathbf{x}-\mathbf{z}\,,\,\mathbf{n}\,\right\rangle = 0
			\;\right\}
	\;\; = \;\;
	\left\{\;
		\mathbf{x}\in\Re^{d}
		\;\left\vert\;
			\left\langle\,\mathbf{x}-\mathbf{z}\,,\,\frac{\mathbf{n}}{\Vert\,\mathbf{n}\,\Vert}\,\right\rangle = 0
			\right.\;\right\}
	\\
	& = &
	\left\{\;
		\left.
		\mathbf{x}\overset{{\color{white}.}}{\in}\Re^{d}
		\;\;\right\vert\;
			\left\langle\,\mathbf{x}-\mathbf{z}\,,\,\widehat{\mathbf{n}}\,\right\rangle = 0
		\;\right\}
	\end{eqnarray*}
	
\item
	Note that $H_{\mathbf{n},\mathbf{z}} = H_{\alpha\mathbf{n},\mathbf{z}}$, for any $\alpha \neq 0$.
\item
	Let $\mathbf{x} \in \Re^{d}$.  The distance between $\mathbf{x}$ and the hyperplane $H_{\mathbf{n},\mathbf{z}}$
	is given by:
	\begin{equation*}
	\dist\!\left(\mathbf{x},H_{\mathbf{n},\mathbf{z}}\right)
	\;\; = \;\;
		\left\vert
		\left\langle\,\mathbf{x}-\mathbf{z}\,,\,\dfrac{\mathbf{n}}{\Vert\mathbf{n}\Vert}\,\right\rangle
		\right\vert
	\;\; = \;\;
		\dfrac{1}{{\color{white}.}\Vert\,\mathbf{n}\,\Vert{\color{white}.}}
		\cdot
		\left\vert
		\left\langle\,\mathbf{x}-\mathbf{z}\,,\,\overset{{\color{white}.}}{\mathbf{n}}\,\right\rangle
		\right\vert
	\end{equation*}
	%This implies:
	%\begin{equation*}
	%\Vert\,\mathbf{n}\,\Vert
	%\cdot\dist\!\left(\mathbf{x},H_{\mathbf{n},\mathbf{z}}\right) \;\; = \;\;
	%\left\vert\left\langle\,\mathbf{x}-\mathbf{z}\,,\,\mathbf{n}\,\right\rangle\right\vert
	%\end{equation*}
\item
	Note that
	$\dist\!\left(\mathbf{x},H_{\mathbf{n},\mathbf{z}}\right)$ is well-defined,
	i.e. it depends only on the point $\mathbf{x}$ and the hyperplane
	$H_{\mathbf{n},\mathbf{z}}$, and is indeed independent of the particular choice
	of the normal vector $\mathbf{n}$ and the point
	$\mathbf{z} \in H_{\mathbf{n},\mathbf{z}}$. 
\end{itemize}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.8cm
\noindent
\textbf{Linearly separable data}
\vskip 0.1cm
\noindent
A data set
\begin{equation*}
\mathcal{D}
\;\; = \;\;
	\left\{\,
		(\overset{{\color{white}-}}{\mathbf{x}}_{1},y_{1})\,,\,(\mathbf{x}_{2},y_{2})\,,\,\ldots\,,\,(\mathbf{x}_{m},y_{m})
		\,\right\},
\quad\quad
\mathbf{x}_{i} \in \Re^{d},
\quad\quad
y_{i} \in \{\,-1,\,1\,\}
\end{equation*}
is said to be \textit{linearly separable} if
\begin{equation*}
	\left\{\;\;
		(\mathbf{n},\mathbf{z}) \in \Re^{d} \times \Re^{d}
		\;\;\left\vert
		\begin{array}{c}
			\textnormal{$H_{\mathbf{n},\mathbf{z}}$ is a}
			\\
			\textnormal{separating hyperplane}
			\\
			\textnormal{for $(\mathbf{x}_{1},y_{1}),\ldots,(\mathbf{x}_{m},y_{m})$}
			\end{array}
			\right.
		\!\right\}
\;\; = \;\;
	\left\{\;
		(\mathbf{n},\mathbf{z}) \in \Re^{d} \times \Re^{d}
		\;\;\left\vert
		\begin{array}{c}
			y_{i}\cdot\langle\,\mathbf{x}_{i} - \mathbf{z}\,,\,\mathbf{n}\,\rangle > 0\,,
			\\
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\;\; \neq \;\;
	\varemptyset
\end{equation*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\noindent
\textbf{The Primal Problem of the linearly separable case}
\begin{eqnarray*}
&&
	\argmax
	\left\{\;
		{\color{white}........1}
		\underset{1\leq i \leq m}{\min}\left\{\,\dist(\,\mathbf{x}_{i}\overset{{\color{white}1}}{,}H_{\mathbf{n},\mathbf{z}}\,)\,\right\}
		{\color{white}........}
		\;\;\left\vert
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; \mathbf{z} \in \Re^{d}
			\\
			\textnormal{$H_{\mathbf{n},\mathbf{z}}$ is a separating hyperplane}
			\;
			\textnormal{for $(\mathbf{x}_{1},y_{1}),\ldots,(\mathbf{x}_{m},y_{m})$}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;
		{\color{white}.......1}\!
		\underset{1\leq i \leq m}{\min}\left\{\,
			\dfrac{
				\vert\,\langle\,\mathbf{x}_{i}-\mathbf{z}\,,\,\mathbf{n}\,\rangle\,\vert
				}{
				\Vert\,\mathbf{n}\,\Vert
				}
			\,\right\}
		{\color{white}.......}
		\;\;\left\vert
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; \mathbf{z} \in \Re^{d}
			\\
			y_{i}\cdot\langle\,\mathbf{x}_{i} - \mathbf{z}\,,\,\mathbf{n}\,\rangle > 0\,,
			\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;
		{\color{white}...}
		\dfrac{1}{\Vert\,\mathbf{n}\,\Vert}
		\cdot
		\underset{1\leq i \leq m}{\min}\left\{\,
			\left\vert\,\langle\,\mathbf{x}_{i}\overset{{\color{white}.}}{-}\mathbf{z}\,,\,\mathbf{n}\,\rangle\,\right\vert
			\,\right\}
		{\color{white}...}
		\;\;\left\vert
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; \mathbf{z} \in \Re^{d}
			\\
			y_{i}\cdot\left(\langle\,\mathbf{x}_{i}\,,\,\mathbf{n}\,\rangle - \langle\,\mathbf{z}\,,\,\mathbf{n}\,\rangle\right) > 0\,,
			\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;
		{\color{white}....}
		\dfrac{1}{\Vert\,\mathbf{n}\,\Vert}
		\cdot
		\underset{1\leq i \leq m}{\min}\left\{\,
			\left\vert\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle \overset{{\color{white}.}}{+} {\color{red}b}\,\right\vert
			\,\right\}
		{\color{white}...}
		\;\;\left\vert
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; {\color{red}b \in \Re}
			\\
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) > 0\,,
			\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;
		\dfrac{1}{\Vert\,\mathbf{n}\,\Vert}
		\cdot
		\underset{1\leq i \leq m}{\min}\left\{\,
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle \overset{{\color{white}.}}{+} b\,\right)
			\,\right\}
		\;\;\left\vert
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; b \in \Re
			\\
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) > 0\,,
			\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;
		\dfrac{1}{\Vert\,\mathbf{n}\,\Vert}
		\cdot
		\underset{1\leq i \leq m}{\min}\left\{\,
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle \overset{{\color{white}.}}{+} b\,\right)
			\,\right\}
		\;\;\left\vert
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; b \in \Re
			\\
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) > 0\,,\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\\
			{\color{red}\underset{1\leq i \leq m}{\min}\left\{\,
				y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle \overset{{\color{white}.}}{+} b\,\right)
				\,\right\}
				\; = \; 1}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;\;\,
		{\color{white}..}
		\dfrac{1}{\Vert\,\mathbf{n}\,\Vert}
		{\color{white}...}
		\;\;\left\vert\;
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; b \in \Re
			\\
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) > 0\,,\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\\
			y_{i}\overset{{\color{white}.}}{\cdot}\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right)\,{\color{red}\geq}\,1\,,\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\\
& = &
	\argmax
	\left\{\;\;\,
		{\color{white}...}
		\dfrac{1}{\Vert\,\mathbf{n}\,\Vert}
		{\color{white}...}\!
		\;\;\left\vert\;
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; b \in \Re
			\\
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) \geq 1\,,
			\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\\
& = &
	{\color{red}\argmin}\,
	\left\{\;\;\,
		\dfrac{1}{2}\cdot\Vert\,\mathbf{n}\,\Vert^{2}
		\;\,\left\vert\;
		\begin{array}{c}
			\mathbf{n} \in \Re^{d},\; b \in \Re
			\\
			y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) \geq 1\,,
			\;
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\noindent
\textbf{The Primal Problem of the linearly non-separable case}
\begin{eqnarray*}
	\argmin
	\left\{\;\;
		\dfrac{1}{2}\cdot\Vert\,\mathbf{n}\,\Vert^{2}
		+
		\lambda\cdot\overset{m}{\underset{i=1}{\sum}}\;\xi_{i}
		\;\;\,\left\vert\;
		\begin{array}{c}
			\mathbf{n} \in \Re^{d} \,,\;\, b \in \Re \,,\;\, \xi \in \Re^{m}
			\\
			\xi_{i} \geq 0,\;\,\textnormal{and}\;\, y_{i}\cdot\left(\,\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b\,\right) \geq 1 - \xi_{i}\,,
			\\
			\textnormal{for each \,$i = 1,2,\ldots,m$}
			\end{array}
			\right.
		\right\}
\end{eqnarray*}
Here, \,$\lambda \,>\, 0$\, is a user-prescribed hyperparameter, which stipulates how harshly ``slack'' is penalized.

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\noindent
\textbf{Karush-Kuhn-Tucker conditions for the Primal Problem of the linearly non-separable case}
\vskip 0.1cm
\noindent
The Lagrangian function of the Primal Problem is given by
\begin{equation*}
L(\,\mathbf{n},b,\xi\,;\,\alpha,\beta\,)
\;\; := \;\;
	\dfrac{1}{2}\cdot\Vert\,\mathbf{n}\,\Vert^{2}
	\; + \;
	\lambda\cdot\overset{m}{\underset{i=1}{\sum}}\;\xi_{i}
	\; + \;
	\overset{m}{\underset{i = 1}{\sum}}\;\,
		\alpha_{i}
		\cdot
		\left(\,
			1 - \xi_{i}
			-
			y_{i}\overset{{\color{white}1}}{\cdot}\left(\,
				\langle\,\mathbf{x}_{i}\,,\mathbf{n}\,\rangle + b
				\,\right)
			\,\right)
	\; - \;
	\overset{m}{\underset{i = 1}{\sum}}\;
		\beta_{i}
		\cdot
		\xi_{i}
\end{equation*}
As we will see below, {\color{red}once the Lagrange multipliers \,$\alpha_{i}^{*},\; \beta_{i}^{*}$\,
have been determined, the Karush-Kuhn-Tucker (KKT) necessary conditions
can be used to obtain the optimal \,$(\,\mathbf{n}^{*},b^{*},\xi^{*}\,)$}.\,
Now, the KKT necessary conditions are:
\begin{equation*}
\left\{\;\;
	\begin{array}{cccc}
	\left(\nabla_{(\mathbf{n},b,\xi)}\,L\right)\!(\,\mathbf{n}^{*},b^{*},\,\xi^{*}\,;\,\alpha^{*},\beta^{*}\,) & = & \mathbf{0}\,{\color{white},}
	\\
	\alpha_{i}^{*} & \overset{{\color{white}1}}{\geq} & 0\,, & \textnormal{for}\;\; i = 1,2,\ldots,m
	\\
	\beta_{i}^{*} & \overset{{\color{white}1}}{\geq} & 0\,, & \textnormal{for}\;\; i = 1,2,\ldots,m
	\\
	\alpha_{i}^{*} \cdot \left(\,
		\overset{{\color{white}.}}{1} - \xi_{i} - y_{i}\cdot\left(\, \langle\,\mathbf{x}_{i}\,,\mathbf{n}^{*}\,\rangle + b^{*} \,\right)
		\,\right)
		& = & 0\,, & \textnormal{for}\;\; i = 1,2,\ldots,m
	\\
	\beta_{i}^{*} \cdot \xi_{i}^{*} & \overset{{\color{white}1}}{=} & 0\,, & \textnormal{for}\;\; i = 1,2,\ldots,m
	\end{array}
	\right.
\end{equation*}
The condition
\,$\left(\nabla_{(\mathbf{n},b,\xi)}\,L\right)\!(\,\mathbf{n}^{*},b^{*},\,\xi^{*}\,;\,\alpha^{*},\beta^{*}\,) \,=\, \mathbf{0}$\,
is equivalent to:
\begin{equation*}
\left\{\;\;
\begin{array}{ccccl}
\mathbf{0}
& = &
	\nabla_{\mathbf{n}}\,L
& = &
	\mathbf{n}^{*} \, - \, \overset{m}{\underset{i\,=\,1}{\sum}}\;\alpha_{i}^{*}\,y_{i}\cdot\mathbf{x}_{i}
\\
0
&=&
	\nabla_{b}\,L
& = &
	\overset{{\color{white}.}}{\overset{m}{\underset{i\,=\,1}{\sum}}\;\alpha_{i}^{*}\,y_{i}}
\\
\mathbf{0}
& = &
	\nabla_{\xi}\,L
& = &
	\overset{{\color{white}1}}{\lambda}\cdot\mathbf{1}_{m} \, - \, (\alpha^{*} \, + \, \beta^{*})
\end{array}
\right.
\end{equation*}
Given \,$\alpha^{*}$,\, it follows immediately from \,$\nabla_{\mathbf{n}}\,L \,=\, \mathbf{0}$\, that
\begin{equation*}
\mathbf{n}^{*} \;\; = \;\; \overset{m}{\underset{i\,=\,1}{\sum}}\;\alpha_{i}^{*}\,y_{i}\cdot\mathbf{x}_{i}
\end{equation*}
Secondly, \,$\nabla_{\xi}\,L \,=\, \mathbf{0}$\,
\,$\Longrightarrow$\,
\,$ (\alpha^{*} \, + \, \beta^{*}) \, = \, \lambda\cdot\mathbf{1}_{m}$,\,
which in turn implies that the following scenarios are mutually exclusive:
\begin{itemize}
\item
	$\left(\; \overset{{\color{white}.}}{0} \,<\, \alpha_{i}^{*} \,<\, \lambda
	\; \Longleftrightarrow \;
	\overset{{\color{white}.}}{0} \,<\, \beta_{i}^{*} \,<\, \lambda \,\;\right)$\,,
	which implies:
	\begin{equation*}
		\left\{\begin{array}{c}
			1 - \xi_{i}^{*} - y_{i}\cdot\left(\langle\,\mathbf{x}_{i}\,,\mathbf{n}^{*}\,\rangle \overset{{\color{white}.}}{+} b^{*}\right) \, = \, 0
			\\
			\overset{{\color{white}-}}{\xi_{i}^{*}} \,=\, 0
			\end{array}\right\}
	\Longrightarrow
		\left\{\begin{array}{c}
			%b^{*} \,=\, y_{i} - \overset{m}{\underset{j=1}{\sum}}\,\alpha_{j}^{*}\,y_{j}\cdot\langle\,\mathbf{x}_{i}\,,\,\mathbf{x}_{j}\,\rangle
			\langle\,\mathbf{x}_{i}\,,\,\mathbf{n}^{*}\,\rangle + b^{*} \,=\, y_{i}
			\\
			\overset{{\color{white}-}}{\xi_{i}^{*}} \,=\, 0
			\end{array}\right\}
	\Longrightarrow
		\left\{\begin{array}{c}
			b^{*} \,=\, y_{i} - \overset{m}{\underset{j=1}{\sum}}\,\alpha_{j}^{*}\,y_{j}\cdot\langle\,\mathbf{x}_{i}\,,\,\mathbf{x}_{j}\,\rangle
			\\
			\overset{{\color{white}-}}{\xi_{i}^{*}} \,=\, 0
			\end{array}\right\}
	\end{equation*}
	This scenario is that of the ``support vectors''.

\item
	$\left(\; \alpha_{i}^{*} \,=\, \overset{{\color{white}.}}{0}
	\; \Longleftrightarrow \;
	\beta_{i}^{*} \,=\, \lambda \,>\, \overset{{\color{white}.}}{0} \,\;\right)$\,,
	which implies:
	\begin{equation*}
		\xi_{i}^{*} \,=\, 0
	\end{equation*}
	Observations in this scenario are the ones with zero slack; in particular, all such observations are ``correctly classified.''

\item
	$\left(\; \beta_{i}^{*} \,=\, \overset{{\color{white}.}}{0}
	\; \Longleftrightarrow\;
	\alpha_{i}^{*} \,=\, \lambda \,>\, \overset{{\color{white}.}}{0} \,\;\right)$\,,
	which implies
	\begin{equation*}
		1 - \xi_{i}^{*} - y_{i}\cdot\left(\langle\,\mathbf{x}_{i}\,,\mathbf{n}^{*}\,\rangle \overset{{\color{white}.}}{+} b^{*}\right) \, = \, 0
	\quad\Longrightarrow\quad
		\xi_{i}^{*} \, = \, 1 - y_{i}\cdot\left(\langle\,\mathbf{x}_{i}\,,\mathbf{n}^{*}\,\rangle \overset{{\color{white}.}}{+} b^{*}\right)
	\end{equation*}
	Observations in this scenario are the ones with non-negative slack.

\end{itemize}
