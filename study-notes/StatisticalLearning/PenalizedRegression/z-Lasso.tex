
\section{Lasso (Least Absolute Shrinkage and Selection Operator)}
\setcounter{theorem}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%``Lasso'' stands ``Least Absolute Shrinkage and Selection Operator''.
Suppose that
\,$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$,\,
\,$X = \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\, and
\,$\tau > 0$.\,
The \textbf{Lasso estimator} is given by:
\begin{equation*}
\widehat{Y}^{\textnormal{(Lasso)}}
\;\; := \;\;
	X \cdot \widehat{\beta}^{\,\textnormal{(Lasso)}}\,,
\end{equation*}
where
\begin{eqnarray*}
\widehat{\beta}^{\,\textnormal{(Lasso)}}
& \in &
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} \beta_{0}\cdot\mathbf{1}_{n} - X\cdot\beta \,\;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{proposition}[Translations yield equivalent optimization problems]
\label{translationsYieldEquivalent}
\mbox{}
\vskip 0.0cm
\noindent
Suppose that
\,$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$,\,
\,$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\, and
\,$\tau > 0$.\,
Then, for any
\,$c = (c_{0},c_{1},\ldots,c_{p}) \in \Re^{p+1}$,\,
the following two optimization problems:
\begin{equation*}
\Gamma
\;\; := \;\;
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{equation*}
and
\begin{equation*}
\Gamma(c)
\;\; := \;\;
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - c_{0}) - \gamma_{0} - \overset{p}{\underset{j=1}{\sum}}\, (x_{ij}-c_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{equation*}
are equivalent in the sense that
\begin{eqnarray*}
&&
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - c_{0}) - \gamma_{0} - \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-c_{j})\cdot\gamma_{j}\;\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\},
\end{eqnarray*}
and furthermore
\begin{equation*}
\left\{\begin{array}{lcl}
	\left(\;\overset{{\color{white}.}}{\gamma_{0}}\;,\;\gamma\,\right) \,\in\, \Gamma(c)
	&\Longleftrightarrow&
	\left(\,\gamma_{0}+c_{0}-\overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\gamma_{j}\;,\,\gamma\,\right) \,\in\, \Gamma
	\\ \\
	\left(\,\overset{{\color{white}.}}{\beta_{0}}\;,\,\beta\,\right) \,\in\, \Gamma
	&\Longleftrightarrow&
	\left(\,\beta_{0}-c_{0}+\overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\beta_{j}\;,\,\beta\,\right) \,\in\, \Gamma(c)
\end{array}\right.
\end{equation*}
\end{proposition}
\proof
Observe that
\begin{eqnarray*}
&&
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - c_{0}) - \gamma_{0} - \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-c_{j})\cdot\gamma_{j}\;\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} -
		\left({\color{orange}\gamma_{0} + c_{0} - \overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\gamma_{j}}\right)
		- \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot{\color{red}\gamma_{j}}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - {\color{orange}\beta_{0}} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot{\color{red}\beta_{j}}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{eqnarray*}
Hence,
\begin{equation*}
(\,\gamma_{0}^{*}\,,\gamma^{*}\,) \,\in\, \Gamma(c)
\quad\Longrightarrow\quad
\left(\,\gamma_{0}^{*}+c_{0} - \overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\gamma_{j}^{*}\;\,,\;\gamma^{*}\,\right)
\,\in\,\Gamma{\color{white}(c)}
\end{equation*}
and conversely,
\begin{equation*}
(\,\beta_{0}^{*}\,,\beta^{*}\,) \,\in\, \Gamma{\color{white}(c)}
\quad\Longrightarrow\quad
\left(\,\beta_{0}^{*} - c_{0} + \overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\beta_{j}^{*}\;\,,\;\beta^{*}\,\right)
\,\in\, \Gamma(c)
\end{equation*}
This proves the Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{theorem}[WLOG, can centre each predictor variable and remove intercept term]
\label{CanCenterAndRemoveIntercept}
\mbox{}
\vskip 0.0cm
\noindent
Suppose that
\,$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$,\,
\,$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\, and
\,$\tau > 0$.\,
Then, the corresponding ``centered and zero-intercept'' Lasso optimization problem is equivalent to
the original Lasso optimization problem in the sense that
\begin{eqnarray*}
&&
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{\gamma\,\in\,\Re^{p}}{\inf}
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - \overline{y}) \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{eqnarray*}
\end{theorem}
\proof
\begin{eqnarray*}
&&
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \gamma_{0} \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\},
	\quad
	\textnormal{by Proposition \ref{translationsYieldEquivalent}}
\\
& = &
	\underset{\gamma\,\in\,\Re^{p}}{\inf}
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - \overline{y}) \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\},
\end{eqnarray*}
where the last equality follows from the fact that the pertinent Lagrangian function
\,$L : \Re \times \Re^{p} \times [\,0,\infty) \longrightarrow \Re$\,
is:
\begin{equation*}
L(\,\gamma_{0},\gamma,\lambda\,)
\;\; = \;\;
	\dfrac{1}{2}\cdot
	\overset{n}{\underset{i=1}{\sum}}
	\left(\; y_{i} - \gamma_{0} \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
	\, + \,
	\lambda\cdot\left(\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\,-\, \tau 
		\,\right)
\end{equation*}
hence
\begin{equation*}
\dfrac{\partial L}{\partial\gamma_{0}}
\;\; = \;\;
	\overset{n}{\underset{i=1}{\sum}}
	\left(\; y_{i} - \gamma_{0} \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)
	\cdot\left(\,\overset{{\color{white}.}}{-1}\,\right)
\end{equation*}
Consequently, \,$\dfrac{\partial L}{\partial\gamma_{0}} \, = \, 0$\, implies
\begin{eqnarray*}
n\cdot\gamma_{0}
& = &
	\left(\, \overset{n}{\underset{i=1}{\sum}}\; y_{i} \right)
	\;-\;\;
	\overset{n}{\underset{i=1}{\sum}}\;
	\overset{p}{\underset{j=1}{\sum}}\;
	(x_{ij}-\overline{x}_{j})\cdot\gamma_{j}
\;\; = \;\;
	\left(\, \overset{n}{\underset{i=1}{\sum}}\; y_{i} \right)
	\;-\;\;
	\overset{p}{\underset{j=1}{\sum}}\;\,
	\gamma_{j}\cdot
	\underset{0}{\underbrace{\overset{n}{\underset{i=1}{\sum}}\,(x_{ij}-\overline{x}_{j})}}
\;\; = \;\;
	\overset{n}{\underset{i=1}{\sum}}\; y_{i}\,,
\end{eqnarray*}
which in turn implies
\begin{eqnarray*}
\gamma_{0}
& = &
	\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\; y_{i}
\;\; =: \;\;
	\overline{y}
\end{eqnarray*}
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 1.0cm
\noindent
\textbf{Lasso for centered variables, without intercept term}
\vskip 0.0cm
\noindent
By Theorem \ref{CanCenterAndRemoveIntercept}, upon centering variables if necessary,
we may assume \,$y \,\in\, \Re^{n}$\, and
\,$X \,=\, \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right) \,\in\, \Re^{n \times p}$\,
satisfy
\begin{equation*}
\overset{n}{\underset{i=1}{\sum}}\;y_{i} \; = \; 0\,,
\quad\textnormal{and}\quad\quad
\overset{n}{\underset{i=1}{\sum}}\;x_{ij} \; = \; 0\,,
\;\;
\textnormal{for each \,$j = 1,\ldots,p$}
\end{equation*}
The Lasso optimization then becomes:
\begin{eqnarray*}
&&
	\underset{\beta\,\in\,\Re^{p}}{\inf}
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{\beta\,\in\,\Re^{p}}{\inf}
	\left\{\;
		\underset{\lambda\,\geq\,0}{\sup}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda
			\cdot
			\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right)
			\,\right\}
		\;\right\}
\\
& \geq &
	\underset{\lambda\,\geq\,0}{\sup}\,
	\left\{\;
		\underset{\beta\,\in\,\Re^{p}}{\inf}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda
			\cdot
			\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right)
			\,\right\}
		\;\right\},
	\quad
	\textnormal{by Weak Duality}
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{lemma}[Solution of inner minimization of Dual Problem]
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\;y_{i} \; = \; 0$,\,
\item
	$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\, x_{ij} \, = \, 0$,\,
	for each \,$j = 1,\ldots,p$, and
\item
	$\tau > 0$\, and \,$\lambda \,\geq\, 0$.
\end{itemize}
Define the Lagrangian function
\,$L : \Re^{p} \times [\,0,\infty) \longrightarrow \Re$\,
as follows:
\begin{equation*}
L(\beta,\lambda)
\;\; := \;\;
	\dfrac{1}{2}
	\cdot
	\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
	\; + \,
	\lambda\cdot\left(\,\Vert\,\beta\,\Vert_{1} \overset{{\color{white}.}}{-} \tau\,\right)
\end{equation*}
Then,
\begin{enumerate}
\item
	the subdifferential
	\,$\partial_{\beta} L(\beta,\lambda) \,\subset\, \Re^{p}$\,
	is given by
	\begin{equation*}
		\partial_{\beta} L(\beta,\lambda)
	\;\; = \;\;
		\left\{\;
			- \, X^{T} \cdot (\,y - X \cdot \beta\,)
			\, \overset{{\color{white}.}}{+} \,
			\lambda\cdot\mathbf{v}(\beta)
			\;\right\}
	\;\; \subset \;\;
		\Re^{p},
	\end{equation*}
	where \,$\mathbf{v}(\beta) \,=\, (v_{1},\ldots,v_{p})^{T} \,\in\, \Re^{p}$\, is given by
	\begin{equation*}
	v_{j}
	\;\; \in \;\;
		\left\{\begin{array}{cl}
			\{\,1\,\}, & \textnormal{for \,$\beta_{i} > 0$},
			\\
			\left[\,-1,1\right], & \overset{{\color{white}1}}{\textnormal{for \,$\beta_{i} = 0$}},
			\\
			\{\,-1\,\}, & \overset{{\color{white}1}}{\textnormal{for \,$\beta_{i} < 0$}}
			\end{array}\right.
	\end{equation*}
\item
	\begin{equation*}
	\widetilde{\beta}
	\; \in \;
		\underset{\beta\,\in\,\Re^{p}}{\argmin}\;L(\beta,\lambda)
	\quad\Longleftrightarrow\quad
	\mathbf{0}_{p}
	\; \in \;
		\partial_{\beta} L\!\left(\,\widetilde{\beta}\,,\lambda\,\right)
	\; = \;
		\left\{\;
			- \, X^{T} \cdot (\,y - X \cdot \widetilde{\beta}\,)
			\, \overset{{\color{white}.}}{+} \,
			\lambda\cdot\mathbf{v}(\,\widetilde{\beta}\,)
			\;\right\}
	\end{equation*}
	\vskip 0.2cm
	In particular, we have:
	\begin{equation*}
	\widetilde{\beta}(\lambda)\,\in\,\underset{\beta\,\in\,\Re^{p}}{\argmin}\;L(\beta,\lambda)
	\quad\Longleftrightarrow\quad
		L\!\left(\,\widetilde{\beta}(\lambda)\,,\,\lambda\,\right)
		\,=\,
		\underset{\beta\,\in\,\Re^{p}}{\inf}\;L(\beta,\lambda)
	\quad\Longrightarrow\quad
		\dfrac{
			\left(\,y - X\cdot\widetilde{\beta}(\lambda)\right)^{T} \cdot X \cdot \widetilde{\beta}(\lambda)
			}{
			\overset{{\color{white}.}}{\left\Vert\;\widetilde{\beta}(\lambda)\;\right\Vert_{1}}
			}
		\;\; = \;\;
			\lambda
	\end{equation*}
	Substituting the above expression in
	\,$\widetilde{\beta}(\lambda)$\, for \,$\lambda$\,
	into the Lagrangian function
	\,$L\!\left(\,\widetilde{\beta}(\lambda)\,,\,\lambda\,\right)$\,
	yields
	\begin{eqnarray*}
	L\!\left(\,\widetilde{\beta}(\lambda)\,,\,\lambda\,\right)
	& = &
		\dfrac{1}{2}\cdot
		\left\Vert\;y - X\cdot\widetilde{\beta}(\lambda)\;\right\Vert^{2}_{2}
		\; + \;
		\lambda\cdot
		\left(\;\Vert\;\widetilde{\beta}(\lambda)\,\Vert_{1} \,-\, \tau\,\right)
	\\
	& = &
		\dfrac{1}{2} \cdot y^{T} \cdot y
		\; - \;
		\overset{{\color{white}1}}{\dfrac{1}{2}}
		\cdot
		\widetilde{\beta}(\lambda)^{T} \cdot X^{T} \cdot X \cdot \widetilde{\beta}(\lambda)
		\; - \;
		\tau\cdot
		\dfrac{
			\left(\, y - X\cdot\widetilde{\beta}(\lambda)\,\right)^{\!T} \cdot X \cdot \widetilde{\beta}(\lambda)
			}{
			\overset{{\color{white}.}}{\left\Vert\;\widetilde{\beta}(\lambda)\;\right\Vert_{1}}
			}
	\end{eqnarray*}
\end{enumerate}
\begin{eqnarray*}
&&
	\underset{\beta\,\in\,\Re^{p}}{\argmin}\;
	\left\{\;\;
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\; + \,
		\lambda\cdot\left(\,\Vert\,\beta\,\Vert_{1} \overset{{\color{white}.}}{-} \tau\,\right)
		\;\;\right\}
\\
& = &
	\underset{\beta\,\in\,\Re^{p}}{\argmin}\;
	\left\{\;\;
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \overset{p}{\underset{j=1}{\sum}}\;x_{ij}\beta_{j}\right)^{\!\!2}
		\; + \,
		\lambda\cdot
		\left(\,
			\overset{p}{\underset{j=1}{\sum}}\;\vert\;\beta_{j}\,\vert - \tau
			\,\right)
		\;\right\}
\end{eqnarray*}
\end{lemma}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{theorem}[Strong Duality]
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\;y_{i} \; = \; 0$,\,
\item
	$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\, x_{ij} \, = \, 0$,\,
	for each \,$j = 1,\ldots,p$, and
\item
	$\tau > 0$\, and \,$\lambda \,\geq\, 0$.
\end{itemize}
Define the Lagrangian function
\,$L : \Re^{p} \times [\,0,\infty) \longrightarrow \Re$\,
as follows:
\begin{equation*}
L(\beta,\lambda)
\;\; := \;\;
	\dfrac{1}{2}
	\cdot
	\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
	\; + \,
	\lambda\cdot\left(\,\Vert\,\beta\,\Vert_{1} \overset{{\color{white}.}}{-} \tau\,\right)
\end{equation*}
Then, the following statements hold:
\begin{enumerate}
\item
	Suppose \,$\beta^{*}$\, solves the Lasso Primal Problem, i.e.
	\begin{equation*}
	\beta^{*}
	\;\; \in \;\;
		\underset{\beta\,\in\,\Re^{p}}{\argmin}\,
		\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
	\end{equation*}
	and \,$\lambda^{*} \,\geq\, 0$\, is the corresponding Lagrangian multiplier;
	in other words,
	\begin{equation*}
	\lambda^{*}
	\;\; = \;\;
		\dfrac{
			\left(\,y - X\cdot\beta^{*}\right)^{T} \cdot X \cdot \beta^{*}
			}{
			\overset{{\color{white}.}}{\left\Vert\;\beta^{*}\,\right\Vert_{1}}
			}.
	\end{equation*}
	Then, \,$\lambda^{*}$\, solves the Lasso Dual Problem, i.e.
	\begin{eqnarray*}
	\lambda^{*}
	& \in &
		\underset{\lambda\,\geq\,0}{\argmax}
		\left\{\;
			\underset{\beta\,\in\,\Re^{p}}{\inf}\; L(\beta,\lambda)
			\;\right\}
	\\
	& = &
		\underset{\lambda\,\geq\,0}{\argmax}\,
		\left\{\;
			\underset{\beta\,\in\,\Re^{p}}{\inf}\,
			\left\{\;\,
				\dfrac{1}{2}
				\cdot
				\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
				\; + \,
				\lambda
				\cdot
				\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right)
				\,\right\}
			\;\right\}
	\\
	& = &
		\underset{\lambda\,\geq\,0}{\argmax}\,
		\left\{\;
			-\,\lambda\cdot\tau
			\; + \,
			\underset{\beta\,\in\,\Re^{p}}{\inf}\,
			\left\{\;\,
				\dfrac{1}{2}
				\cdot
				\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
				\; + \,
				\lambda \cdot \Vert\;\beta\,\Vert_{1} 
				\,\right\}
			\;\right\}
	\end{eqnarray*}
	Furthermore,
	\begin{eqnarray*}
	&&
		{\color{red}\underset{\beta\,\in\,\Re^{p}}{\inf}
		\left\{\;\;
			\left.
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\;\;\,\right\vert\;\;
			\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
			\;\;\right\}}
	\;\; = \;\;
		\underset{\beta\,\in\,\Re^{p}}{\inf}
		\left\{\;
			\underset{\lambda\,\geq\,0}{\sup}\; L(\beta,\lambda)
			\;\right\}
	\\
	& = &
		\overset{{\color{white}1}}{\underset{\lambda\,\geq\,0}{\sup}\; L(\beta^{*},\lambda)}
	\;\; = \;\;
		L(\beta^{*},\lambda^{*})
	\;\; = \;\;
		\underset{\beta\,\in\,\Re^{p}}{\inf}\; L(\beta,\lambda^{*})
	\\
	& = &
		\underset{\beta\,\in\,\Re^{p}}{\inf}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda^{*}
			\cdot
			\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right) \,\right\}
	\\
	& = &
		-\,\lambda^{*}\cdot\tau
		\; + \,
		{\color{red}\underset{\beta\,\in\,\Re^{p}}{\inf}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda^{*} \cdot \Vert\;\beta\,\Vert_{1} 
			\,\right\}}
%	\\
%	& = &
%		\underset{\lambda\,\geq\,0}{\sup}
%		\left\{\;
%			\underset{\beta\,\in\,\Re^{p}}{\inf}\; L(\beta,\lambda)
%			\;\right\}
%	\\
%	& = &
%		\underset{\beta\,\in\,\Re^{p}}{\inf}
%		\left\{\;
%			\underset{\lambda\,\geq\,0}{\sup}\; L(\beta,\lambda)
%			\;\right\}
%	\;\; = \;\;
%		\underset{\beta\,\in\,\Re^{p}}{\inf}
%		\left\{\;\;
%			\left.
%			\dfrac{1}{2}
%			\cdot
%			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
%			\;\;\,\right\vert\;\;
%			\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
%			\;\;\right\}
	\end{eqnarray*}
\end{enumerate}
\end{theorem}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
