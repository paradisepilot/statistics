
\section{Lasso (Least Absolute Shrinkage and Selection Operator)}
\setcounter{theorem}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.3cm
\subsection{Lasso -- general formulation}
\vskip 0.0cm
\noindent
Suppose that
\,$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$,\,
\,$X = \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\, and
\,$\tau > 0$.\,
The \textbf{Lasso estimator} is given by:
\begin{equation*}
\widehat{Y}^{\textnormal{(Lasso)}}
\;\; := \;\;
	X \cdot \widehat{\beta}^{\,\textnormal{(Lasso)}}\,,
\end{equation*}
where
\begin{eqnarray*}
\widehat{\beta}^{\,\textnormal{(Lasso)}}
& \in &
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} \beta_{0}\cdot\mathbf{1}_{n} - X\cdot\beta \,\;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\subsection{Lasso -- with centered variables and without intercept term}
\vskip 0.0cm
\noindent
By Theorem \ref{CanCenterAndRemoveIntercept}, upon centering variables if necessary,
we may assume \,$y \,\in\, \Re^{n}$\, and
\,$X \,=\, \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right) \,\in\, \Re^{n \times p}$\,
satisfy
\begin{equation*}
\overset{n}{\underset{i=1}{\sum}}\;y_{i} \; = \; 0\,,
\quad\textnormal{and}\quad\quad
\overset{n}{\underset{i=1}{\sum}}\;x_{ij} \; = \; 0\,,
\;\;
\textnormal{for each \,$j = 1,\ldots,p$}
\end{equation*}
The Lasso optimization then becomes:
\begin{equation}
\label{LassoCenteredVariablesNoIntercept}
	\underset{\beta\,\in\,\Re^{p}}{\inf}
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
\end{equation}
\textbf{Observations}
\begin{itemize}
\item
	The minimization problem \eqref{LassoCenteredVariablesNoIntercept}
	has a compact domain and continuous objective function.
	Hence, \eqref{LassoCenteredVariablesNoIntercept} has a finite infimum value,
	and the infimum value is attained at some point(s) in its domain.
\item
	The collection of least-squares solutions of the corresponding unconstrained problem is
	\begin{eqnarray*}
	\mathcal{LS}(X,y)
	& := &
		\underset{\beta\,\in\,\Re^{p}}{\argmin}
		\left\{\;\;
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\;\;\right\}
	\\
	& = &
		\left\{\;
			\left.
			\overset{{\color{white}.}}{\beta} \in \Re^{p}
			\;\;\right\vert\;
			X^{T} \cdot (\,y - X\cdot\beta\,) \,=\, 0
			\,\;\right\}
		\;\; = \;\;
		\left\{\;
			\left.
			\overset{{\color{white}.}}{\beta} \in \Re^{p}
			\;\;\right\vert\;
			X^{T} \cdot X \cdot \beta \, = \, X^{T} \cdot y
			\,\;\right\}
	\end{eqnarray*}
\item
	Consider the case where \,{\color{red}$p \,>\, n$}.
	
	\vskip 0.2cm
	The least-squares solution is then non-unique;
	in other words, \,$\mathcal{LS}(X,y)$\, contains more than one element.

	\vskip 0.2cm
	Recall that
	\,$\ker(X^{T} \cdot X) \,=\, \ker(X)$.\,
	Indeed, it is immediate that
	\,$\ker(X) \,\subset\, \ker(X^{T} \cdot X)$.\,
	Conversely, \,$v \,\in\, \ker(X^{T} \cdot X)$\,
	\,$\Longleftrightarrow$\, \,$X^{T} \cdot X \cdot v \,=\, 0$\,
	\,$\Longrightarrow$\, \,$\Vert\,X \cdot v\,\Vert^{2} \,=\, v^{T} \cdot X^{T} \cdot X \cdot v \,=\, 0$\,
	\,$\Longrightarrow$\, \,$X \cdot v \,=\, 0$,\,
	i.e. \,$v \in \ker(X)$.
	This proves the reverse inclusion that
	\,$\ker(X^{T} \cdot X) \subset \ker(X)$.

	\vskip 0.2cm
	Next, note that
	\begin{equation*}
	\mathcal{LS}(X,y)
	\;\; = \;\;
		\beta^{*} \,+\, \ker(X^{T}\cdot X)
	\;\; = \;\;
		\beta^{*} \,+\, \ker(X),
	\quad
	\textnormal{for each \,$\beta^{*} \,\in\, \mathcal{LS}(X,y)$}
	\end{equation*}
	We thus see that
	\,{\color{red}$\mathcal{LS}(X,y) \,\subset\, \Re^{p}$\,
	is a translate of the subspace
	\,$\ker(X) \,\subset\, \Re^{p}$.}
	
\item
	Consider still the case where \,{\color{red}$p \,>\, n$}.
	
	\vskip 0.2cm
	Define:
	\begin{equation*}
	\tau_{0}
	\;\; := \;\;
		\underset{\beta\,\in\,\mathcal{LS}(X,y)}{\min}
		\left\{\;
			\Vert\; \overset{{\color{white}.}}{\beta} \;\Vert_{1}
			\;\right\}
	\;\; = \;\;
		\underset{\beta\,\in\,\ker(X)}{\min}
		\left\{\;
			\Vert\; \beta^{*} + \overset{{\color{white}.}}{\beta} \;\Vert_{1}
			\;\right\},
	\end{equation*}
	where \,$\beta^{*}$\, is an arbitrary element of \,$\mathcal{LS}(X,y)$.
	
	\vskip 0.2cm
	We see that, for \,$\tau \,\geq\, \tau_{0}$,\, the minimization problem
	\,\eqref{LassoCenteredVariablesNoIntercept}\,
	is equivalent to its unconstrained counterpart.

	\vskip 0.2cm
	On the other hand, for \,$\tau \,<\, \tau_{0}$,\, each solution of the minimization problem
	\,\eqref{LassoCenteredVariablesNoIntercept}\,
	must lie on the boundary of its domain,
	since \,$\tau \,<\, \tau_{0}$\, implies that
	all critical points of the objective function lie strictly outside
	of the domain of \,\eqref{LassoCenteredVariablesNoIntercept}.
	In other words,
	{\color{red}\begin{equation*}
	\tau \,<\, \tau_{0}
	\quad\Longrightarrow\quad
		\underset{\beta\,\in\,\Re^{p}}{\argmin}
		\left\{\;\,
			\left.
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\;\;\,\right\vert\;\;
			\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
			\,\;\right\}
		\;\;\textnormal{\large$\subset$}\;\;
		\left\{\;
			\left.
			\overset{{\color{white}1}}{\beta} \in \Re^{p}
			\;\,\right\vert\;\,
			\Vert\;\beta\,\Vert_{1} \, = \, \tau
			\;\right\}
	\end{equation*}}
\end{itemize}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{Dual Problem of the Lasso with centered variables and without intercept term}
\vskip 0.0cm
\noindent
\begin{eqnarray*}
&&
	\underset{\beta\,\in\,\Re^{p}}{\inf}
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{\beta\,\in\,\Re^{p}}{\inf}
	\left\{\;
		\underset{\lambda\,\geq\,0}{\sup}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda
			\cdot
			\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right)
			\,\right\}
		\;\right\}
\\
& \geq &
	\underset{\lambda\,\geq\,0}{\sup}\,
	\left\{\;
		\underset{\beta\,\in\,\Re^{p}}{\inf}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda
			\cdot
			\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right)
			\,\right\}
		\;\right\},
	\quad
	\textnormal{by Weak Duality}
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{lemma}[Solution of inner minimization of Dual Problem]
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\;y_{i} \; = \; 0$,\,
\item
	$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\, x_{ij} \, = \, 0$,\,
	for each \,$j = 1,\ldots,p$, and
\item
	$\tau > 0$\, and \,$\lambda \,\geq\, 0$.
\end{itemize}
Define the Lagrangian function
\,$L : \Re^{p} \times [\,0,\infty) \longrightarrow \Re$\,
as follows:
\begin{equation*}
L(\beta,\lambda)
\;\; := \;\;
	\dfrac{1}{2}
	\cdot
	\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
	\; + \,
	\lambda\cdot\left(\,\Vert\,\beta\,\Vert_{1} \overset{{\color{white}.}}{-} \tau\,\right)
\end{equation*}
Then,
\begin{enumerate}
\item
	the subdifferential
	\,$\partial_{\beta} L(\beta,\lambda) \,\subset\, \Re^{p}$\,
	is given by
	\begin{equation*}
		\partial_{\beta} L(\beta,\lambda)
	\;\; = \;\;
		\left\{\;
			- \, X^{T} \cdot (\,y - X \cdot \beta\,)
			\, \overset{{\color{white}.}}{+} \,
			\lambda\cdot\mathbf{v}(\beta)
			\;\right\}
	\;\; \subset \;\;
		\Re^{p},
	\end{equation*}
	where \,$\mathbf{v}(\beta) \,=\, (v_{1},\ldots,v_{p})^{T} \,\in\, \Re^{p}$\, is given by
	\begin{equation*}
	v_{j}
	\;\; \in \;\;
		\left\{\begin{array}{cl}
			\{\,1\,\}, & \textnormal{for \,$\beta_{i} > 0$},
			\\
			\left[\,-1,1\right], & \overset{{\color{white}1}}{\textnormal{for \,$\beta_{i} = 0$}},
			\\
			\{\,-1\,\}, & \overset{{\color{white}1}}{\textnormal{for \,$\beta_{i} < 0$}}
			\end{array}\right.
	\end{equation*}
\item
	\begin{equation*}
	\widetilde{\beta}
	\; \in \;
		\underset{\beta\,\in\,\Re^{p}}{\argmin}\;L(\beta,\lambda)
	\quad\Longleftrightarrow\quad
	\mathbf{0}_{p}
	\; \in \;
		\partial_{\beta} L\!\left(\,\widetilde{\beta}\,,\lambda\,\right)
	\; = \;
		\left\{\;
			- \, X^{T} \cdot (\,y - X \cdot \widetilde{\beta}\,)
			\, \overset{{\color{white}.}}{+} \,
			\lambda\cdot\mathbf{v}(\,\widetilde{\beta}\,)
			\;\right\}
	\end{equation*}
	\vskip 0.2cm
	In particular, we have:
	\begin{equation*}
	\widetilde{\beta}(\lambda)\,\in\,\underset{\beta\,\in\,\Re^{p}}{\argmin}\;L(\beta,\lambda)
	\quad\Longleftrightarrow\quad
		L\!\left(\,\widetilde{\beta}(\lambda)\,,\,\lambda\,\right)
		\,=\,
		\underset{\beta\,\in\,\Re^{p}}{\inf}\;L(\beta,\lambda)
	\quad\Longrightarrow\quad
		\dfrac{
			\left(\,y - X\cdot\widetilde{\beta}(\lambda)\right)^{T} \cdot X \cdot \widetilde{\beta}(\lambda)
			}{
			\overset{{\color{white}.}}{\left\Vert\;\widetilde{\beta}(\lambda)\;\right\Vert_{1}}
			}
		\;\; = \;\;
			\lambda
	\end{equation*}
	Substituting the above expression in
	\,$\widetilde{\beta}(\lambda)$\, for \,$\lambda$\,
	into the Lagrangian function
	\,$L\!\left(\,\widetilde{\beta}(\lambda)\,,\,\lambda\,\right)$\,
	yields
	\begin{eqnarray*}
	L\!\left(\,\widetilde{\beta}(\lambda)\,,\,\lambda\,\right)
	& = &
		\dfrac{1}{2}\cdot
		\left\Vert\;y - X\cdot\widetilde{\beta}(\lambda)\;\right\Vert^{2}_{2}
		\; + \;
		\lambda\cdot
		\left(\;\Vert\;\widetilde{\beta}(\lambda)\,\Vert_{1} \,-\, \tau\,\right)
	\\
	& = &
		\dfrac{1}{2} \cdot y^{T} \cdot y
		\; - \;
		\overset{{\color{white}1}}{\dfrac{1}{2}}
		\cdot
		\widetilde{\beta}(\lambda)^{T} \cdot X^{T} \cdot X \cdot \widetilde{\beta}(\lambda)
		\; - \;
		\tau\cdot
		\dfrac{
			\left(\, y - X\cdot\widetilde{\beta}(\lambda)\,\right)^{\!T} \cdot X \cdot \widetilde{\beta}(\lambda)
			}{
			\overset{{\color{white}.}}{\left\Vert\;\widetilde{\beta}(\lambda)\;\right\Vert_{1}}
			}
	\end{eqnarray*}
\end{enumerate}
\begin{eqnarray*}
&&
	\underset{\beta\,\in\,\Re^{p}}{\argmin}\;
	\left\{\;\;
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\; + \,
		\lambda\cdot\left(\,\Vert\,\beta\,\Vert_{1} \overset{{\color{white}.}}{-} \tau\,\right)
		\;\;\right\}
\\
& = &
	\underset{\beta\,\in\,\Re^{p}}{\argmin}\;
	\left\{\;\;
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \overset{p}{\underset{j=1}{\sum}}\;x_{ij}\beta_{j}\right)^{\!\!2}
		\; + \,
		\lambda\cdot
		\left(\,
			\overset{p}{\underset{j=1}{\sum}}\;\vert\;\beta_{j}\,\vert - \tau
			\,\right)
		\;\right\}
\end{eqnarray*}
\end{lemma}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{theorem}[Strong Duality]
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\;y_{i} \; = \; 0$,\,
\item
	$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\,
	such that
	\,$\overset{n}{\underset{i=1}{\sum}}\, x_{ij} \, = \, 0$,\,
	for each \,$j = 1,\ldots,p$, and
\item
	$\tau > 0$\, and \,$\lambda \,\geq\, 0$.
\end{itemize}
Define the Lagrangian function
\,$L : \Re^{p} \times [\,0,\infty) \longrightarrow \Re$\,
as follows:
\begin{equation*}
L(\beta,\lambda)
\;\; := \;\;
	\dfrac{1}{2}
	\cdot
	\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
	\; + \,
	\lambda\cdot\left(\,\Vert\,\beta\,\Vert_{1} \overset{{\color{white}.}}{-} \tau\,\right)
\end{equation*}
Then, the following statements hold:
\begin{enumerate}
\item
	Suppose \,$\beta^{*}$\, solves the Lasso Primal Problem, i.e.
	\begin{equation*}
	\beta^{*}
	\;\; \in \;\;
		\underset{\beta\,\in\,\Re^{p}}{\argmin}\,
		\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
		\;\;\,\right\vert\;\;
		\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
		\;\;\right\}
	\end{equation*}
	and \,$\lambda^{*} \,\geq\, 0$\, is the corresponding Lagrangian multiplier;
	in other words,
	\begin{equation*}
	\lambda^{*}
	\;\; = \;\;
		\dfrac{
			\left(\,y - X\cdot\beta^{*}\right)^{T} \cdot X \cdot \beta^{*}
			}{
			\overset{{\color{white}.}}{\left\Vert\;\beta^{*}\,\right\Vert_{1}}
			}.
	\end{equation*}
	Then, \,$\lambda^{*}$\, solves the Lasso Dual Problem, i.e.
	\begin{eqnarray*}
	\lambda^{*}
	& \in &
		\underset{\lambda\,\geq\,0}{\argmax}
		\left\{\;
			\underset{\beta\,\in\,\Re^{p}}{\inf}\; L(\beta,\lambda)
			\;\right\}
	\\
	& = &
		\underset{\lambda\,\geq\,0}{\argmax}\,
		\left\{\;
			\underset{\beta\,\in\,\Re^{p}}{\inf}\,
			\left\{\;\,
				\dfrac{1}{2}
				\cdot
				\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
				\; + \,
				\lambda
				\cdot
				\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right)
				\,\right\}
			\;\right\}
	\\
	& = &
		\underset{\lambda\,\geq\,0}{\argmax}\,
		\left\{\;
			-\,\lambda\cdot\tau
			\; + \,
			\underset{\beta\,\in\,\Re^{p}}{\inf}\,
			\left\{\;\,
				\dfrac{1}{2}
				\cdot
				\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
				\; + \,
				\lambda \cdot \Vert\;\beta\,\Vert_{1} 
				\,\right\}
			\;\right\}
	\end{eqnarray*}
	Furthermore,
	\begin{eqnarray*}
	&&
		{\color{red}\underset{\beta\,\in\,\Re^{p}}{\inf}
		\left\{\;\;
			\left.
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\;\;\,\right\vert\;\;
			\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
			\;\;\right\}}
	\;\; = \;\;
		\underset{\beta\,\in\,\Re^{p}}{\inf}
		\left\{\;
			\underset{\lambda\,\geq\,0}{\sup}\; L(\beta,\lambda)
			\;\right\}
	\\
	& = &
		\overset{{\color{white}1}}{\underset{\lambda\,\geq\,0}{\sup}\; L(\beta^{*},\lambda)}
	\;\; = \;\;
		L(\beta^{*},\lambda^{*})
	\;\; = \;\;
		\underset{\beta\,\in\,\Re^{p}}{\inf}\; L(\beta,\lambda^{*})
	\\
	& = &
		\underset{\beta\,\in\,\Re^{p}}{\inf}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda^{*}
			\cdot
			\left(\, \Vert\;\beta\,\Vert_{1} \, \overset{{\color{white}.}}{-} \, \tau \,\right) \,\right\}
	\\
	& = &
		-\,\lambda^{*}\cdot\tau
		\; + \,
		{\color{red}\underset{\beta\,\in\,\Re^{p}}{\inf}\,
		\left\{\;\,
			\dfrac{1}{2}
			\cdot
			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
			\; + \,
			\lambda^{*} \cdot \Vert\;\beta\,\Vert_{1} 
			\,\right\}}
%	\\
%	& = &
%		\underset{\lambda\,\geq\,0}{\sup}
%		\left\{\;
%			\underset{\beta\,\in\,\Re^{p}}{\inf}\; L(\beta,\lambda)
%			\;\right\}
%	\\
%	& = &
%		\underset{\beta\,\in\,\Re^{p}}{\inf}
%		\left\{\;
%			\underset{\lambda\,\geq\,0}{\sup}\; L(\beta,\lambda)
%			\;\right\}
%	\;\; = \;\;
%		\underset{\beta\,\in\,\Re^{p}}{\inf}
%		\left\{\;\;
%			\left.
%			\dfrac{1}{2}
%			\cdot
%			\left\Vert\;\, y \overset{{\color{white}.}}{-} X\cdot\beta \;\right\Vert_{2}^{2}
%			\;\;\,\right\vert\;\;
%			\Vert\;\beta\,\Vert_{1} \, - \, \tau \, \leq \, 0
%			\;\;\right\}
	\end{eqnarray*}
\end{enumerate}
\end{theorem}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
