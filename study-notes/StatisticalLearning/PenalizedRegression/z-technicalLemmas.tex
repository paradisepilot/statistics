
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Technical lemmas}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.2cm
\begin{proposition}[Translations yield equivalent optimization problems]
\label{translationsYieldEquivalent}
\mbox{}
\vskip 0.0cm
\noindent
Suppose that
\,$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$,\,
\,$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\, and
\,$\tau > 0$.\,
Then, for any
\,$c = (c_{0},c_{1},\ldots,c_{p}) \in \Re^{p+1}$,\,
the following two optimization problems:
\begin{equation*}
\Gamma
\;\; := \;\;
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{equation*}
and
\begin{equation*}
\Gamma(c)
\;\; := \;\;
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\argmin}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - c_{0}) - \gamma_{0} - \overset{p}{\underset{j=1}{\sum}}\, (x_{ij}-c_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{equation*}
are equivalent in the sense that
\begin{eqnarray*}
&&
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - c_{0}) - \gamma_{0} - \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-c_{j})\cdot\gamma_{j}\;\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\},
\end{eqnarray*}
and furthermore
\begin{equation*}
\left\{\begin{array}{lcl}
	\left(\;\overset{{\color{white}.}}{\gamma_{0}}\;,\;\gamma\,\right) \,\in\, \Gamma(c)
	&\Longleftrightarrow&
	\left(\,\gamma_{0}+c_{0}-\overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\gamma_{j}\;,\,\gamma\,\right) \,\in\, \Gamma
	\\ \\
	\left(\,\overset{{\color{white}.}}{\beta_{0}}\;,\,\beta\,\right) \,\in\, \Gamma
	&\Longleftrightarrow&
	\left(\,\beta_{0}-c_{0}+\overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\beta_{j}\;,\,\beta\,\right) \,\in\, \Gamma(c)
\end{array}\right.
\end{equation*}
\end{proposition}
\proof
Observe that
\begin{eqnarray*}
&&
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - c_{0}) - \gamma_{0} - \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-c_{j})\cdot\gamma_{j}\;\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} -
		\left({\color{orange}\gamma_{0} + c_{0} - \overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\gamma_{j}}\right)
		- \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot{\color{red}\gamma_{j}}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - {\color{orange}\beta_{0}} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot{\color{red}\beta_{j}}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{eqnarray*}
Hence,
\begin{equation*}
(\,\gamma_{0}^{*}\,,\gamma^{*}\,) \,\in\, \Gamma(c)
\quad\Longrightarrow\quad
\left(\,\gamma_{0}^{*}+c_{0} - \overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\gamma_{j}^{*}\;\,,\;\gamma^{*}\,\right)
\,\in\,\Gamma{\color{white}(c)}
\end{equation*}
and conversely,
\begin{equation*}
(\,\beta_{0}^{*}\,,\beta^{*}\,) \,\in\, \Gamma{\color{white}(c)}
\quad\Longrightarrow\quad
\left(\,\beta_{0}^{*} - c_{0} + \overset{p}{\underset{j=1}{\sum}}\,c_{j}\cdot\beta_{j}^{*}\;\,,\;\beta^{*}\,\right)
\,\in\, \Gamma(c)
\end{equation*}
This proves the Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.5cm
\begin{theorem}[WLOG, can centre each predictor variable and remove intercept term]
\label{CanCenterAndRemoveIntercept}
\mbox{}
\vskip 0.0cm
\noindent
Suppose that
\,$Y = (y_{1},\ldots,y_{n})^{T} \in \Re^{n}$,\,
\,$X \in \left(\,\overset{{\color{white}-}}{x_{ij}}\,\right)\in\Re^{n \times p}$\, and
\,$\tau > 0$.\,
Then, the corresponding ``centered and zero-intercept'' Lasso optimization problem is equivalent to
the original Lasso optimization problem in the sense that
\begin{eqnarray*}
&&
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\;
			{\color{white}.}y_{i} - \beta_{0}{\color{white}.}
			-\,
			\overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}
			\,\right)^{\!\!2}
		{\color{white}..........}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{{\color{white}......}\gamma\,\in\,\Re^{p}{\color{white}......}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\;
			(y_{i} - \overline{y})
			\;-\,
			\overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}
			\,\right)^{\!\!2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\end{eqnarray*}
\end{theorem}
\proof
\begin{eqnarray*}
&&
	\underset{(\beta_{0},\beta)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \beta_{0} - \overset{p}{\underset{j=1}{\sum}}\; x_{ij}\cdot\beta_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\beta_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\}
\\
& = &
	\underset{(\gamma_{0},\gamma)\,\in\,\Re\times\Re^{p}}{\inf}\;
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; y_{i} - \gamma_{0} \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\},
	\quad
	\textnormal{by Proposition \ref{translationsYieldEquivalent}}
\\
& = &
	\underset{\gamma\,\in\,\Re^{p}}{\inf}
	\left\{\;\;
		\left.
		\dfrac{1}{2}
		\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\; (y_{i} - \overline{y}) \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
		\;\;\,\right\vert\;\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\, - \,
		\tau \, \leq \, 0
		\;\;\right\},
\end{eqnarray*}
where the last equality follows from the fact that the pertinent Lagrangian function
\,$L : \Re \times \Re^{p} \times [\,0,\infty) \longrightarrow \Re$\,
is:
\begin{equation*}
L(\,\gamma_{0},\gamma,\lambda\,)
\;\; = \;\;
	\dfrac{1}{2}\cdot
	\overset{n}{\underset{i=1}{\sum}}
	\left(\; y_{i} - \gamma_{0} \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)^{2}
	\, + \,
	\lambda\cdot\left(\;
		\overset{p}{\underset{j=1}{\sum}}\;
		\vert\;\gamma_{j}\,\vert
		\,-\, \tau 
		\,\right)
\end{equation*}
hence
\begin{equation*}
\dfrac{\partial L}{\partial\gamma_{0}}
\;\; = \;\;
	\overset{n}{\underset{i=1}{\sum}}
	\left(\; y_{i} - \gamma_{0} \;-\, \overset{p}{\underset{j=1}{\sum}}\; (x_{ij}-\overline{x}_{j})\cdot\gamma_{j}\,\right)
	\cdot\left(\,\overset{{\color{white}.}}{-1}\,\right)
\end{equation*}
Consequently, \,$\dfrac{\partial L}{\partial\gamma_{0}} \, = \, 0$\, implies
\begin{eqnarray*}
n\cdot\gamma_{0}
& = &
	\left(\, \overset{n}{\underset{i=1}{\sum}}\; y_{i} \right)
	\;-\;\;
	\overset{n}{\underset{i=1}{\sum}}\;
	\overset{p}{\underset{j=1}{\sum}}\;
	(x_{ij}-\overline{x}_{j})\cdot\gamma_{j}
\;\; = \;\;
	\left(\, \overset{n}{\underset{i=1}{\sum}}\; y_{i} \right)
	\;-\;\;
	\overset{p}{\underset{j=1}{\sum}}\;\,
	\gamma_{j}\cdot
	\underset{0}{\underbrace{\overset{n}{\underset{i=1}{\sum}}\,(x_{ij}-\overline{x}_{j})}}
\;\; = \;\;
	\overset{n}{\underset{i=1}{\sum}}\; y_{i}\,,
\end{eqnarray*}
which in turn implies
\begin{eqnarray*}
\gamma_{0}
& = &
	\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\; y_{i}
\;\; =: \;\;
	\overline{y}
\end{eqnarray*}
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
