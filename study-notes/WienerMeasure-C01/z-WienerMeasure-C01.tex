
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{The Wiener measure on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\newcommand{\Pitzk}{\Pi_{t_{0}t_{1}\ldots t_{k}}}
\newcommand{\pitok}{\pi_{t_{1}\ldots t_{k}}}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

\begin{definition}[Wiener measure on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$]
\mbox{}\vskip 0.2cm
\noindent
A Borel probability measure $W$ on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$ is called a
\textbf{Wiener measure} if it satisfies the following two conditions:
\begin{enumerate}
\item	Its induced measure $W\circ\ev_{0}^{-1}$ on $\Re$
		via the evaluation map $\ev_{0} : \Czo \longrightarrow \Re : f \longmapsto f(0)$
		is the point-mass measure on $\Re$ concentrated at $0 \in \Re$, i.e.
		\begin{equation*}
		W\!\left(\,\ev_{0}^{-1}\!\left(\,\left\{\,0\,\right\}\,\right)\,\right)
		\;\; = \;\;
		W\!\left(\left\{\;
			f \in \Czo
			\;\left\vert\;
			f(0) \overset{{\color{white}1}}{=} 0
			\right.
		\;\right\}\right)
		\;\; = \;\;
		1\,.
		\end{equation*}
\item	For any $0 \leq t_{0} < t_{1} < t_{2} < \cdots < t_{k} \leq 1$,
		\begin{equation*}
		W \circ \Pitzk^{-1}
		\;\; = \;\;
		N\!\left(\;
		\mathbf{\mu} = \mathbf{0}\,,\,
		\overset{{\color{white}1}}{\Sigma} = \diag\!\left(\,t_{1}-t_{0},\; \ldots\; ,\; t_{k}-t_{k-1}\,\right)
		\;\right),
		\end{equation*}
		where the map \;$\Pitzk : \Czo \longrightarrow \Re^{k}$\; is defined as follows:
		\begin{equation*}
		\Pitzk \;:\; \Czo \longrightarrow \Re^{k} \;:\;
		f \;\longmapsto\; \left(\,\overset{{\color{white}.}}{f}(t_{1}) - f(t_{0}),\,f(t_{2}) - f(t_{1}),\,\ldots,\,f(t_{k}) - f(t_{k-1})\,\right).
		\end{equation*}
\end{enumerate}
\end{definition}

\begin{theorem}[Finite-dimensional distributions of the Wiener measure]
\label{WienerFiniteDimensionalDistributions}
\mbox{}\vskip 0.2cm
\noindent
Suppose $W$ is a Wiener measure defined on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$.
Then, for any pairwise distinct \;$0 \;\leq\; t_{1},\; t_{2}, \;\ldots\;,\; t_{k} \leq 1$,
		\begin{equation*}
		W \circ \pitok^{-1}
		\;\; = \;\;
		N\!\left(\;
		\mathbf{\mu} = \mathbf{0}\,,\,
		\Sigma = \left[\;\min\{\overset{{\color{white}1}}{t}_{i},t_{j}\}\;\right]_{1\leq i,j\leq k}
		\;\right),
		\end{equation*}
		where the map \;$\pitok : \Czo \longrightarrow \Re^{k}$\; is defined as follows:
		\begin{equation*}
		\pitok \;:\; \Czo \longrightarrow \Re^{k} \;:\;
		f \;\longmapsto\; \left(\,\overset{{\color{white}.}}{f}(t_{1}),\,f(t_{2}),\,\ldots,\,f(t_{k})\,\right).
		\end{equation*}
\end{theorem}
\proof
Let $t_{0} \,:= \, 0$.
Re-labeling the $t_{i}$'s if necessary, without loss of generality, we may assume that
$t_{1} < t_{2} < \cdots < t_{k}$.
Let $\mathcal{B}$ denote the Borel $\sigma$-algebra of $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$.
Then, $\left(\Czo,\mathcal{B},W\right)$ is the probability space
obtained by equipping the measurable space $\left(\Czo,\mathcal{B}\right)$
with the Wiener measure $W$.

\vskip 0.3cm
\noindent
Define the following $\Re$-valued random variables on the probability space $\left(\Czo,\mathcal{B},W\right)$:
\begin{equation*}
\begin{array}{lclrcll}
Z_{t_{1}-t_{0}} &:& \Czo \longrightarrow \Re, & Z_{t_{1}-t_{0}}(f) & := & f(t_{1}) \\
\overset{{\color{white}1}}{Z}_{t_{i}-t_{i-1}} &:& \Czo \longrightarrow \Re, & Z_{t_{i}-t_{i-1}}(f) & := & f(t_{i}) - f(t_{i-1}), & \textnormal{for \;$i = 2, \ldots, k$} \\
\overset{{\color{white}1}}{L}_{t_{i}} &:& \Czo \longrightarrow \Re, & L_{t_{i}}(f) & := & f(t_{i}), & \textnormal{for \;$i = 1, \ldots, k$} \\
\end{array}
\end{equation*}
Then, we have
\begin{equation*}
	\left(\!
	\begin{array}{c}
	L_{t_{1}}(f) \\ \\ L_{t_{2}}(f) \\ \\ \vdots \\ \\ L_{t_{k}}(f) 
	\end{array}
	\!\right)
	\;\; = \;\;
	\left(\!
	\begin{array}{c}
	f(t_{1}) \\ \\ f(t_{2}) \\ \\ \vdots \\ \\ f(t_{k}) 
	\end{array}
	\!\right)
	\;\; = \;\;
	\underset{\textnormal{\large$T$}}{\underbrace{
	\left[
	\begin{array}{ccccccc}
	1 & 0 & 0 & \cdots & \cdots & 0 & 0 \\
	1 & 1 & 0 & \cdots & \cdots & 0 & 0 \\
	1 & 1 & 1 & \cdots & \cdots & 0 & 0 \\
	\vdots & \vdots & \vdots & \ddots & \ddots & 0 & 0 \\
	\vdots & \vdots & \vdots & \ddots & \ddots & 0 & 0 \\
	1 & 1 & 1 & \cdots & \cdots & 1 & 0 \\
	1 & 1 & 1 & \cdots & \cdots & 1 & 1 \\
	\end{array}
	\right]
	}}
	\cdot
	\left(\!
	\begin{array}{c}
	f(t_{1}) \\ \\ f(t_{2}) - f(t_{1}) \\ \\ \vdots \\ \\ f(t_{k}) - f(t_{k-1})
	\end{array}
	\!\right)
	\;\; = \;\;
	T \cdot
	\left(\!
	\begin{array}{c}
	Z_{t_{1}-t_{0}}(f) \\ \\ Z_{t_{2}-t_{1}}(f) \\ \\ \vdots \\ \\ Z_{t_{k}-t_{k-1}}(f)
	\end{array}
	\!\right)
\end{equation*}

\vskip 0.3cm
\noindent
The present Theorem will be proved once we establish that the $\Re^{k}$-valued random variable\\
$\pitok = \left(\,L_{t_{1}},\,L_{t_{2}},\,\ldots,\,L_{t_{k}}\,\right) : \Czo \longrightarrow \Re^{k}$
defined on $\left(\Czo,\mathcal{B},W\right)$ has the following multivariate Gaussian distribution:
\begin{equation*}
\pitok
\;\; = \;\;
\left(\!
\begin{array}{c}
L_{t_{1}} \\ \\ L_{t_{2}} \\ \\ \vdots \\ \\ L_{t_{k}} 
\end{array}
\!\right)
\;\; \sim \;\;
N\!\left(\;
\mathbf{\mu} = \mathbf{0}\,,\,
\Sigma = \left[\;\min\{\overset{{\color{white}1}}{t}_{i},t_{j}\}\;\right]_{1\leq i,j\leq k}
\;\right).
\end{equation*}

\vskip 0.3cm
\noindent
To this end, note that since $t_{0} := 0$, by the definition of a Wiener measure,
the $\Re^{k}$-valued random variable\\
$\left(\,Z_{t_{1}-t_{0}},\,Z_{t_{2}-t_{1}},\,\ldots,\,Z_{t_{k}-t_{k-1}}\,\right) : \Czo \longrightarrow \Re^{k}$\;
equals \;$\Pitzk$\; $W$-almost-surely (since $W\!\left(f(0)\overset{{\color{white}:}}{=}0\right)=1$).
These two $\Re^{k}$-valued random variables therefore have the same probability distribution.
By the definition of a Wiener measure again, this common distribution is the following
multivariate Gaussian distribution:
\begin{equation*}
	\left(\!
	\begin{array}{c}
	Z_{t_{1}-t_{0}} \\ \\ Z_{t_{2}-t_{1}} \\ \\ \vdots \\ \\ Z_{t_{k}-t_{k-1}}
	\end{array}
	\!\right)
	\;\; \sim \;\;
	N\!\left(\;
	\mathbf{\mu} = \mathbf{0}\,,\,
	\overset{{\color{white}1}}{\Sigma} = \diag\!\left(\,t_{1}-t_{0},\; \ldots\; ,\; t_{k}-t_{k-1}\,\right)
	\;\right).
\end{equation*}
Since the $\Re^{k}$-valued random variable
$\left(\,L_{t_{1}},\,L_{t_{2}},\,\ldots,\,L_{t_{k}}\,\right) : \Czo \longrightarrow \Re^{k}$
can be obtained from\\
$\left(\,Z_{t_{1}-t_{0}},\,Z_{t_{2}-t_{1}},\,\ldots,\,Z_{t_{k}-t_{k-1}}\,\right): \Czo \longrightarrow \Re^{k}$
via the non-singular linear transformation $T : \Re^{k} \longrightarrow \Re^{k}$,
it follows that $\left(\,L_{t_{1}},\,L_{t_{2}},\,\ldots,\,L_{t_{k}}\,\right)$ also has a
multivariate Gaussian distribution.
Since
$\left(\,Z_{t_{1}-t_{0}},\,Z_{t_{2}-t_{1}},\,\ldots,\,Z_{t_{k}-t_{k-1}}\,\right)$
has mean vector zero, so does 
$\left(\,L_{t_{1}},\,L_{t_{2}},\,\ldots,\,L_{t_{k}}\,\right)$.
It remains only to compute the covariance matrix of
$\left(\,L_{t_{1}},\,L_{t_{2}},\,\ldots,\,L_{t_{k}}\,\right)$,
which we now do: For $t_{i} < t_{j}$, we have
\begin{eqnarray*}
	\Cov\!\left(\,L_{t_{i}}\,,\,L_{t_{j}}\,\right)
	& = & \Cov\!\left(\,
		Z_{t_{1}-t_{0}} + Z_{t_{2}-t_{1}} + \cdots + Z_{t_{i} - t_{i-1}}\,,\,
		Z_{t_{1}-t_{0}} + Z_{t_{2}-t_{1}} + \cdots + Z_{t_{j} - t_{j-1}}
		\,\right)
	\\
	& = & \Cov\!\left(\,
		\sum_{a\,=\,1}^{i}\,Z_{t_{a}-\,t_{a-1}} \,,\,
		\sum_{b\,=\,1}^{j}\,Z_{t_{b}-\,t_{b-1}}
		\,\right)
	\;\; = \;\; \sum_{a\,=\,1}^{i}\,\sum_{b\,=\,1}^{j}\Cov\!\left(\,Z_{t_{a}-\,t_{a-1}}\,,\,Z_{t_{b}-\,t_{b-1}}\,\right)
	\\
	& = & \sum_{a\,=\,1}^{i}\Cov\!\left(\,Z_{t_{a}-\,t_{a-1}}\,,\,Z_{t_{a}-\,t_{a-1}}\,\right)
	\;\;=\;\; \sum_{a\,=\,1}^{i}\Var\!\left(\,Z_{t_{a}-\,t_{a-1}}\,\right)
	\;\;=\;\; \sum_{a\,=\,1}^{i}\left(\,t_{a}-\,t_{a-1}\,\right)
	\\
	& = & \left(\,t_{1}-\,t_{0}\,\right) \;+\; \left(\,t_{2}-\,t_{1}\,\right)
		\;+\; \cdots 
		\;+\; \left(\,t_{i-1}-\,t_{i-2}\,\right) \;+\; \left(\,t_{i}-\,t_{i-1}\,\right)
	\\
	& = & t_{i} \;\; = \;\; \min\!\left\{\,t_{i}\,,t_{j}\,\right\},
\end{eqnarray*}
as required.
This completes the proof of the Theorem.
\qed

\begin{theorem}[Uniqueness of the Wiener measure]
\label{WienerMeasureUniqueness}
\mbox{}\vskip 0.2cm
\noindent
Any two Wiener measures on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$ must in fact be equal.
\end{theorem}
\proof
By Theorem \ref{WienerFiniteDimensionalDistributions}, the collection of finite-dimensional distributions
of a Wiener measure on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$ is completely determined.
Thus, any two Wiener measures $W_{1}$ and $W_{2}$ must the exactly the same finite-dimensional distributions,
which in turn implies that $W_{1}$ and $W_{2}$ must agree on the entire collection of finite-dimensional
subsets of $\Czo$. Lastly, recall that the finite-dimensional subsets of $\Czo$ form a separating class of the
Borel $\sigma$-algebra of $\left(\Czo,w\Vert\,\cdot\,\Vert_{\infty}\right)$ (Example 1.3, p.11, \cite{Billingsley1999}).
We may now conclude that $W_{1} = W_{2}$, as Borel probability measures on
$\left(\Czo,w\Vert\,\cdot\,\Vert_{\infty}\right)$.
\qed

\begin{theorem}[Existence of the Wiener measure]
\label{WienerMeasureExistence}
\mbox{}\vskip 0.2cm
\noindent
There exists a Wiener measure on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$.
\end{theorem}
\proof
Let $\xi_{1}, \xi_{2}, \ldots\, : \Omega \longrightarrow \Re$ be a sequence of
independent and identically distributed standard Gaussian $\Re$-valued
random variables defined on the probability space $(\Omega,\mathcal{A},\mu)$
(i.e. with expectation value zero and common finite variance $\sigma^{2} = 1$).
Define the random variables:
\begin{equation*}
	\left\{\begin{array}{ccccll}
	S_{0}
	&:&\overset{{\color{white}1}}{\Omega} \longrightarrow \Re
	&:& \omega \;\longmapsto\; 0,
	& \textnormal{and}
	\\ \\
	S_{n}
	&:&	\Omega \longrightarrow \Re
	&:&	\omega \;\longmapsto\; \overset{n}{\underset{i=1}{\textnormal{\Large$\sum$}}}\;\xi_{i}(\omega),
	& \textnormal{for each $n \in \N$}.
	\end{array}\right.
\end{equation*}

\vskip 0.2cm
\noindent
For each $n \in \N$, define \,$X^{(n)} \,:\, \Omega \;\longrightarrow\;C[0,1]$\, as follows:
\begin{equation*}
	X^{(n)}(\omega)(t)
	\;\; := \;\;
	\dfrac{1}{\sqrt{n}}
	\left\{\;
	S_{i-1}(\omega) \;+\; n\left(t - \dfrac{i-1}{n}\right)\xi_{i}(\omega)
	\,\right\},
	\;\;
	\textnormal{for each $\omega \in \Omega$, \;$t \in \left[\frac{i-1}{n},\frac{i}{n}\right]$, \;$i = 1,2,3,\ldots,n$}.
\end{equation*}

\begin{center}
\begin{minipage}{6.0in}
\noindent
\textbf{Claim 1:}
\vskip 0.1cm
\noindent
The sequence
\;$\left\{\,P_{n} \overset{{\color{white}-}}{:=} P_{X^{(n)}}\right\}_{n\in\N}$\;
of Borel probability measures on
$\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$ induced by the $X^{(n)}$'s is tight.
\end{minipage}
\end{center}

\vskip 0.3cm
\noindent
We defer the proof of Claim 1 below.
Granting the validity of Claim 1, we see that
$\left\{\,P_{n} \overset{{\color{white}-}}{:=} P_{X^{(n)}}\,\right\}_{n\in\N}$\;
admits a weakly convergent subsequence $\left\{\,P_{n(i)}\,\right\}_{i\in\N}$,
say $P_{n(i)} \overset{d}{\longrightarrow} W$, as $i \longrightarrow \infty$,
where $W$ is some Borel probability measure
on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$.

\vskip 0.5cm
\begin{center}
\begin{minipage}{6.0in}
\noindent
\textbf{Claim 2:}\quad
$W$ is a Wiener measure on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$.
\end{minipage}
\end{center}

\vskip 0.5cm
\noindent
Of course, Claim 2 is the desired conclusion of the present Theorem.
Thus, the proof of the Theorem will be complete once we prove
Claim 1 and Claim 2, which we now do.

\vskip 0.5cm
\noindent
Proof of Claim 1:\quad
We will use Lemma \ref{tightnessRandomWalk}.
Since $\xi_{1}, \xi_{2}, \ldots\,$ are independent and identically distributed,
they satisfy trivially the stationarity hypothesis in Lemma \ref{tightnessRandomWalk}.
Thus, Claim 1 will follow once we establish the following:
\begin{equation*}
	\underset{\lambda\rightarrow\infty}{\lim}\;\;
	\underset{n\rightarrow\infty}{\limsup}\;\;
	\lambda^{2}\cdot
	P\!\left(\;\underset{1\,\leq\,k\,\leq\,n}{\max}\,\vert\,S_{k}\,\vert\,\geq\,\lambda\,\sqrt{n}\;\right)
	\;\;=\;\; 0.
\end{equation*}
Since, for each $k \in \N$, $S_{k} \,=\, \xi_{1} + \xi_{2} + \cdots + \xi_{k}$
is a sum of $k$ independent standard Gaussian $\Re$-valued random variables,
$S_{k}/\sqrt{k}$ itself is a standard Gaussian $\Re$-valued random variable.
Now, observe that:
\begin{eqnarray*}
P\!\left(\;
	\underset{1\leq k \leq n}{\max}\,\vert\,S_{k}\,\vert \,\overset{{\color{white}:}}{\geq}\,\lambda\,\sqrt{n}
\;\right)
&\leq&
	3 \cdot \underset{1\leq k \leq n}{\max}\;
	P\!\left(\;
		\vert\,S_{k}\,\vert \,\overset{{\color{white}1}}{\geq}\,\dfrac{\lambda\,\sqrt{n}}{3}
	\;\right),
	\quad
	\textnormal{by Etemadi's inequality (Theorem \ref{EtemadiInequality})}
\\
&=&
	3 \cdot \underset{1\leq k \leq n}{\max}\;
	P\!\left(\;
		\left\vert\,\dfrac{S_{k}}{\sqrt{k}}\,\right\vert \,\overset{{\color{white}1}}{\geq}\,\dfrac{\lambda\,\sqrt{n}}{3\sqrt{k}}
	\;\right)
\\
&=&
	3 \cdot \underset{1\leq k \leq n}{\max}\;
	P\!\left(\;
		\left\vert\,Z\,\right\vert \,\overset{{\color{white}1}}{\geq}\,\dfrac{\lambda\,\sqrt{n}}{3\sqrt{k}}
	\;\right),
	\quad
	\textnormal{where $Z \sim N(0,1)$}
\\
&\leq&
	3 \cdot \underset{1\leq k \leq n}{\max}\;
	\left(\dfrac{3\sqrt{k}}{\lambda\,\sqrt{n}}\right)^{4}
	\cdot
	E\!\left(\, \left\vert\,Z\,\right\vert^{4} \,\right),
	\quad
	\textnormal{by Chebyshev's inequality}
\\
&=&
	3 \cdot \dfrac{3^{4}}{\lambda^{4}} \cdot 3,
	\quad
	\textnormal{since $E\!\left(\, \left\vert\,Z\,\right\vert^{4} \,\right) = 3$}
\\
&\leq&
	\dfrac{3^{6}}{\lambda^{4}},
\end{eqnarray*}
which implies
\begin{equation*}
	\underset{\lambda\rightarrow\infty}{\lim}\;
	\underset{n\rightarrow\infty}{\limsup}\;\;
	\lambda^{2}\cdot
	P\!\left(\;\underset{1\,\leq\,k\,\leq\,n}{\max}\,\vert\,S_{k}\,\vert\,\geq\,\lambda\,\sqrt{n}\;\right)
\;\;\leq\;\;
	\underset{\lambda\rightarrow\infty}{\lim}\;
	\underset{n\rightarrow\infty}{\limsup}\;\;
	\lambda^{2}\cdot\dfrac{3^{6}}{\lambda^{4}}
\;\;=\;\;
	\underset{\lambda\rightarrow\infty}{\lim}\;
	\dfrac{3^{6}}{\lambda^{2}}
\;\;=\;\; 0\,.
\end{equation*}
This proves Claim 1.

\vskip 0.5cm
\noindent
Proof of Claim 2:\quad
First of all, $W$ above was chosen to be the weak limit of the subsequence $P_{n(i)}$,
so we have $P_{n(i)} \overset{d}{\longrightarrow} W$.
Now, recall that $\ev_{0} : \Czo \longrightarrow \Re : f \longmapsto f(0)$ is continuous.
Hence,
\begin{eqnarray*}
&&
	P_{n(i)} \;\overset{d}{\longrightarrow}\; W
\\
&\Longrightarrow&
	\underset{{\color{white}i}}{P_{n(i)}}\circ\ev_{0}^{-1} \;\overset{d}{\longrightarrow}\; W\circ\ev_{0}^{-1},
	\quad
	\textnormal{by the Continuous Mapping Theorem (Theorem 2.7, p.21, \cite{Billingsley1999})}
\\
&\Longrightarrow&
	\underset{i\rightarrow\infty}{\lim\sup}\;P_{n(i)}\circ\ev_{0}^{-1}\!\left(\left\{\,0\,\right\}\right)
	\;\leq\; W\circ\ev_{0}^{-1}\!\left(\left\{\,0\,\right\}\right),
	\quad
	\textnormal{by the Portmanteau Theorem (Theorem 2.1, p.16, \cite{Billingsley1999})}
\\
&\Longrightarrow&
	\underset{i\rightarrow\infty}{\lim\sup}\;P\!\left(\,X^{(n(i))}(0) = 0\,\right)
	\;\leq\; W\circ\ev_{0}^{-1}\!\left(\left\{\,0\,\right\}\right)
\\
&\Longrightarrow&
	1 \;=\; \underset{i\rightarrow\infty}{\lim\sup}\;P\!\left(\,S_{0} = 0\,\right)
	\;\leq\; W\circ\ev_{0}^{-1}\!\left(\left\{\,0\,\right\}\right),
\end{eqnarray*}
which implies
\begin{equation*}
W\circ\ev_{0}^{-1}\!\left(\left\{\,0\,\right\}\right) \;\; = \;\; 1.
\end{equation*}
Secondly, since, for each $0 \,\leq\, t_{0} \,<\, t_{1} \,<\, t_{2} \,<\, \ldots \,<\, t_{k} \,\leq\, 1$,
the map
\begin{equation*}
	\Pitzk \;:\; \Czo \longrightarrow \Re^{k} \;:\;
	f \;\longmapsto\; \left(\,\overset{{\color{white}.}}{f}(t_{1}) - f(t_{0}),\,f(t_{2}) - f(t_{1}),\,\ldots,\,f(t_{k}) - f(t_{k-1})\,\right)
\end{equation*}
is continuous (proof is routine, hence omitted),
by the Continuous Mapping Theorem (Theorem 2.7, p.21, \cite{Billingsley1999}),
we have
$P_{n(i)}\circ\Pitzk^{-1} \overset{d}{\longrightarrow} W\circ\Pitzk^{-1}$.
By Theorem \ref{multivariateGaussianScalingLimit}, we have also
\begin{equation*}
	P_{n}\circ\Pitzk^{-1}
	\;\; \overset{d}{\longrightarrow} \;\;
	N\!\left(\;
	\mathbf{\mu} = \mathbf{0}\,,\,
	\overset{{\color{white}1}}{\Sigma} = \diag\!\left(\,t_{1}-t_{0},\; \ldots\; ,\; t_{k}-t_{k-1}\,\right)
	\;\right),
	\;\;
	\textnormal{as \;$n \longrightarrow \infty$},
\end{equation*}
in particular,
\begin{equation*}
	P_{n(i)}\circ\Pitzk^{-1}
	\;\; \overset{d}{\longrightarrow} \;\;
	N\!\left(\;
	\mathbf{\mu} = \mathbf{0}\,,\,
	\overset{{\color{white}1}}{\Sigma} = \diag\!\left(\,t_{1}-t_{0},\; \ldots\; ,\; t_{k}-t_{k-1}\,\right)
	\;\right),
	\;\;
	\textnormal{as \;$i \longrightarrow \infty$}.
\end{equation*}
Thus, both \;$W\circ\Pitzk^{-1}$\; and 
\;$
N\!\left(\;
	\mathbf{\mu} = \mathbf{0}\,,\,
	\overset{{\color{white}1}}{\Sigma} = \diag\!\left(\,t_{1}-t_{0},\; \ldots\; ,\; t_{k}-t_{k-1}\,\right)
\;\right)
$\;
are weak limits of \;$P_{n(i)}\circ\Pitzk^{-1}$, as $i \longrightarrow \infty$.
By Theorem \ref{weakLimitUniqueness}, it follows that
\begin{equation*}
	W\circ\Pitzk^{-1}
	\;\; = \;\;
	N\!\left(\;
	\mathbf{\mu} = \mathbf{0}\,,\,
	\overset{{\color{white}1}}{\Sigma} = \diag\!\left(\,t_{1}-t_{0},\; \ldots\; ,\; t_{k}-t_{k-1}\,\right)
	\;\right).
\end{equation*}
This proves that $W$ is indeed a Wiener measure on $\left(\Czo,\Vert\,\cdot\,\Vert_{\infty}\right)$.
This completes the proof of Claim 2, as well as that of the present Theorem.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
