
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Concavity of the log likelihood of a {\color{red}canonical} exponential family}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Exponential family, Definition 2.2, p.96, \cite{Shao2003}]
\mbox{}\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\nu)$ is a measure space, where $\nu$ is a $\sigma$-finite measure.
\item
	$\left\{\,P_{\theta}\,\right\}_{\theta \in \Theta}$ is a family, indexed by the nonempty set $\Theta$,
	of probability measures defined on the measurable space $(\Omega,\mathcal{A})$ such that,
	for each $\theta \in \Theta$, $P_{\theta}$ is absolutely continuous with respect to $\nu$.
	Hence, the probability density function
	\begin{equation*}
	\dfrac{\d P_{\theta}}{\d\,\nu} \; : \; \Omega \; \longrightarrow \; [0,\infty)
	\end{equation*}
	of $P_{\theta}$ with respect to $\nu$ exists, for each $\theta \in \Theta$.
\end{itemize}
Then, the family $\left\{\,P_{\theta}\,\right\}_{\theta \in \Theta}$ is called an \textbf{exponential family} if
there exist
\begin{itemize}
\item
	an $\Re^{p}$-valued function $\eta : \Theta \longrightarrow \Re^{p}$\,,
\item
	a Borel $\Re^{p}$-valued function $X : (\Omega,\mathcal{A}) \longrightarrow (\Re^{p},\mathcal{O}(\Re^{p}))$\,, and
\item
	a non-negative Borel function $h : (\Omega,\mathcal{A}) \longrightarrow (\Re,\mathcal{O}(\Re))$ with
	\,$\nu\!\left(\,\left\{\,\left.\omega\overset{{\color{white}.}}{\in}\Omega\,\;\right\vert\,h(\omega) > 0\,\right\}\,\right) > 0$
\end{itemize}
such that the following statements hold:
\begin{enumerate}
\item
	For every $\theta \in \Theta$, we have
	\begin{equation*}
	0 \;\; < \;\;
		\int_{\Omega}\,
			\exp\!\left(\,\eta(\theta)^{T} \overset{{\color{white}-}}{\cdot} X(\omega)\,\right) \cdot h(\omega)
		\;\d\nu(\omega)
	\;\; < \;\; \infty\,.
	\end{equation*}
	Consequently, $\kappa : \Theta \longrightarrow \Re$ given by
	\begin{equation*}
	\kappa(\theta)
	\;\; := \;\;
		\int_{\Omega}\,
			\exp\!\left(\,\eta(\theta)^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
		\;\d\nu(\omega)\,
	\end{equation*}
	is a well-defined positive function on $\Theta$.
\item
	The probability density function
	$\dfrac{\d P_{\theta}}{\d\,\nu} : \Omega \longrightarrow [0,\infty)$
	of each $P_{\theta}$ with respect to $\nu$ can be expressed as:
	\begin{equation*}
	\dfrac{\d P_{\theta}}{\d\,\nu}(\omega)
	\;\; = \;\;
		\dfrac{1}{\kappa(\theta)}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \eta(\theta)^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}
		\,,
		\quad
		\textnormal{for each \,$\omega\in\Omega$}\,.
	\end{equation*}
\end{enumerate}
$\Theta$ is called the \textbf{parameter space} of
the exponential family $\{\,P_{\theta}\,\}_{\theta\in\Theta}$.
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Canonical exponential family]
\mbox{}\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\nu)$ is a measure space, where $\nu$ is a $\sigma$-finite measure.
\item
	$\Theta \subset \Re^{p}$ is a nonempty subset of $\Re^{p}$.
\item
	$\left\{\,P_{\theta}\,\right\}_{\theta \in \Theta}$ is a family, indexed by $\Theta$,
	of probability measures defined on the measurable space $(\Omega,\mathcal{A})$ such that,
	for each $\theta \in \Theta$, $P_{\theta}$ is absolutely continuous with respect to $\nu$.
	Hence, the probability density function
	\begin{equation*}
	\dfrac{\d P_{\theta}}{\d\,\nu} \; : \; \Omega \; \longrightarrow \; [0,\infty)
	\end{equation*}
	of $P_{\theta}$ with respect to $\nu$ exists, for each $\theta \in \Theta$.
\end{itemize}
Then, the family $\left\{\,P_{\theta}\,\right\}_{\theta \in \Theta}$ is called a
\textbf{{\color{red}canonical} exponential family}
if there exist
\begin{itemize}
\item
	a Borel $\Re^{p}$-valued function $X : (\Omega,\mathcal{A}) \longrightarrow (\Re^{p},\mathcal{O}(\Re^{p}))$\,, and
\item
	a non-negative Borel function $h : (\Omega,\mathcal{A}) \longrightarrow (\Re,\mathcal{O}(\Re))$ with
	\,$\nu\!\left(\,\left\{\,\left.\omega\overset{{\color{white}.}}{\in}\Omega\,\;\right\vert\,h(\omega) > 0\,\right\}\,\right) > 0$
\end{itemize}
such that the following statements hold:
\begin{enumerate}
\item
	The set $\Theta \subset \Re^{p}$ is ``maximal''
	in the sense that the following set-theoretic equality holds:
	\begin{equation*}
	\Theta
	\;\; = \;\;
		\left\{\;
			\theta \in \Re^{p}
			\;\left\vert\;\;
			0 \, < \,
			\int_{\Omega}\,
				\exp\!\left(\,\theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
			\;\d\nu(\omega)
			\, < \, \infty
			\right.
		\;\right\}.
	\end{equation*}	
\item
	The probability density function
	$\dfrac{\d P_{\theta}}{\d\,\nu} : \Omega \longrightarrow [0,\infty)$
	of each $P_{\theta}$ with respect to $\nu$ can be expressed as:
	\begin{equation*}
	\dfrac{\d P_{\theta}}{\d\,\nu}(\omega)
	\;\; = \;\;
		\dfrac{1}{\kappa(\theta)}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}
		\,,
		\quad
		\textnormal{for each \,$\omega\in\Omega$}\,,
	\end{equation*}
	where $\kappa : \Theta \longrightarrow \Re$ is the strictly positive
	$\Re$-valued function defined on $\Theta \subset \Re^{p}$ as follows:
	\begin{equation*}
	\kappa(\theta)
	\;\; := \;\;
		\int_{\Omega}\,
			\exp\!\left(\,\theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
		\;\d\nu(\omega)\,.
	\end{equation*}
\end{enumerate}
$\Theta$ is called the \textbf{parameter space} of
the canonical exponential family $\{\,P_{\theta}\,\}_{\theta\in\Theta}$.
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{theorem}\quad
The parameter space of a {\color{red}canonical} exponential family is convex.
\end{theorem}
\proof
Suppose that $(\Omega,\mathcal{A},\nu)$ is a $\sigma$-finite measure space,
$\Theta \subset \Re^{p}$, $\{\,P_{\theta}\,\}_{\theta\in\Theta}$ is a canonical exponential family 
of probability measures defined on $(\Omega,\mathcal{A})$ such that the probability
density function $\dfrac{\d P_{\theta}}{\d\,\nu}$ of each $P_{\theta}$ is given by
\begin{equation*}
	\dfrac{\d P_{\theta}}{\d\,\nu}(\omega)
	\;\; = \;\;
		\dfrac{1}{\kappa(\theta)}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \theta^{T} \overset{{\color{white}-}}{\cdot} X(\omega) \;\right\}
		\,,
		\quad
		\textnormal{for each \,$\omega\in\Omega$}\,,
\end{equation*}
where
$X : (\Omega,\mathcal{A}) \longrightarrow (\Re^{p},\mathcal{O}(\Re^{p}))$
is a Borel $\Re^{p}$-valued function,
$h : (\Omega,\mathcal{A}) \longrightarrow (\Re,\mathcal{O}(\Re))$
is a non-negative Borel function, and
$\kappa : \Theta \longrightarrow \Re$ is the strictly positive
$\Re$-valued function defined on $\Theta \subset \Re^{p}$ as follows:
\begin{equation*}
\kappa(\theta)
\;\; := \;\;
	\int_{\Omega}\,
		\exp\!\left(\,\theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
	\;\d\nu(\omega)\,.
\end{equation*}
By the hypothesis that $\{\,P_{\theta}\,\}_{\theta\in\Theta}$ is a canonical exponential family, we have
\begin{equation*}
\Theta
\;\; = \;\;
	\left\{\;
		\theta \in \Re^{p}
		\;\left\vert\;\;
		\int_{\Omega}\,
			\exp\!\left(\,\theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
		\;\d\nu(\omega)
		\, < \, \infty
		\right.
	\;\right\}.
\end{equation*}
In order to prove that $\Theta$ is convex (as a subset of $\Re^{p}$),
we need to prove the following:
\begin{equation*}
\lambda \cdot \theta_{1} + (1 - \lambda) \cdot \theta_{2} \in \Theta\,,
\quad
\textnormal{for each \,$\lambda \in (0,1)$, and for each \,$\theta_{1}, \theta_{2} \in \Theta$}\,.
\end{equation*}
To this end, first note that
\begin{eqnarray*}
&&
	\int_{\Omega}\,
		\exp\!\left(\,
			(\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2})^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
\\
&=&
	\int_{\{h>0\}}\,
		\exp\!\left(\,
			(\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2})^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
	\;+\;
	\int_{\{h=0\}}\,
		\exp\!\left(\,
			(\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2})^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
\\
&=&
	\int_{\{h>0\}}\,
		\exp\!\left(\,
			(\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2})^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
\\
&>& 0\,,
	\quad
	\textnormal{since the exponential function is strictly positive and
	$\nu\!\left(\{\,h \overset{{\color{white}.}}{>} 0\,\}\right) > 0$}\,.
\end{eqnarray*}
Secondly, by {\color{red}H\"older's inequality} (Theorem 31.3, p.256, \cite{Aliprantis1998}),
\begin{eqnarray*}
&&
	\int_{\Omega}\,
		\exp\!\left(\,
			(\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2})^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
\\
&=&
	\int_{\Omega}\,
		\exp\!\left(\,
			\lambda\cdot\theta_{1}^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
		\exp\!\left(\,
			(1-\lambda)\cdot\theta_{2}^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
\\
&\leq&
	\left(\;
	\int_{\Omega}\,
		\exp\!\left(\,
			\lambda\cdot\theta_{1}^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)^{1/\lambda}
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
	\;\right)^{\lambda}
	\cdot
	\left(\;
	\int_{\Omega}\,
		\exp\!\left(\,
			(1-\lambda)\cdot\theta_{2}^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)^{1/(1-\lambda)}
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
	\;\right)^{1-\lambda}
\\
&=&
	\left(\;
	\int_{\Omega}\,
		\exp\!\left(\,
			\theta_{1}^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
	\;\right)^{\lambda}
	\cdot
	\left(\;
	\int_{\Omega}\,
		\exp\!\left(\,
			\theta_{2}^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
	\;\right)^{1-\lambda}
\\
&<& \infty
\end{eqnarray*}
Thus, we have show that for every $\lambda \in (0,1)$ and every $\lambda_{1}, \lambda_{2} \in \Theta$, we have
\begin{equation*}
0 \;\; < \;\;
	\int_{\Omega}\,
		\exp\!\left(\,
			(\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2})^{T}
			\!\overset{{\color{white}-}}{\cdot}\!
			X(\omega)
		\,\right)
		\cdot
	 	h(\omega)
	\;\d\nu(\omega)
\;\; < \;\; \infty\,;
\end{equation*}
equivalently, $\lambda\cdot\theta_{1}+(1-\lambda)\cdot\theta_{2} \in \Theta$, as desired.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{theorem}[Score and Hessian of the log likelihood of a canonical exponential family]
\mbox{}\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\nu)$ is a measure space, where $\nu$ is a $\sigma$-finite measure.
\item
	$X : (\Omega,\mathcal{A}) \longrightarrow (\Re^{p},\mathcal{O}(\Re^{p}))$
	is a Borel $\Re^{p}$-valued function.
\item
	$h : (\Omega,\mathcal{A}) \longrightarrow (\Re,\mathcal{O}(\Re))$
	is a non-negative Borel function such that
	\,$\nu\!\left(\,\left\{\,\left.\omega\overset{{\color{white}.}}{\in}\Omega\,\;\right\vert\,h(\omega) > 0\,\right\}\,\right) > 0$.
\item
	\begin{equation*}
	\Theta
	\;\; := \;\;
		\left\{\;
			\theta \in \Re^{p}
			\;\left\vert\;\;
			0 \, < \,
			\int_{\Omega}\,
				\exp\!\left(\,\theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
			\;\d\nu(\omega)
			\, < \, \infty
			\right.
		\;\right\}
	\;\; \subset \;\; \Re^{p}
	\end{equation*}
	is a nonempty open subset of $\Re^{p}$.
\item
	$\left\{\,P_{\theta}\,\right\}_{\theta \in \Theta}$ is a canonical exponential family, indexed by $\Theta$,
	of probability measures defined on the measurable space $(\Omega,\mathcal{A})$ such that,
	for each $\theta \in \Theta$, $P_{\theta}$ is absolutely continuous with respect to $\nu$, and
	the probability density function of $P_{\theta}$ with respect to $\nu$ is given by:
	\begin{equation*}
	f_{\theta} \; := \; \dfrac{\d P_{\theta}}{\d\,\nu}
	\; : \; \Omega \; \longrightarrow \; [0,\infty) \; : \;
	\omega \; \longmapsto \;
		\dfrac{1}{\kappa(\theta)}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}\,,
	\end{equation*}
	where $\kappa : \Theta \longrightarrow \Re$ is the strictly positive
	$\Re$-valued function defined on $\Theta \subset \Re^{p}$ as follows:
	\begin{equation*}
	\kappa(\theta)
	\;\; := \;\;
		\int_{\Omega}\,
			\exp\!\left(\,\theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega)\,\right) \cdot h(\omega)
		\;\d\nu(\omega)\,.
	\end{equation*}
\item
	The functions
	\begin{equation*}
	\Theta \; \longrightarrow \; \Re
	\; : \; \theta \; \longmapsto \; \int_{\Omega}\,f_{\theta}(\omega)\,\d\nu(\omega)
	\quad\quad\textnormal{and}\quad\quad
	\Theta \; \longrightarrow \; \Re^{p}
	\; : \; \theta \; \longmapsto \; \int_{\Omega}\,X(\omega) \cdot f_{\theta}(\omega)\,\d\nu(\omega)
	\end{equation*}
	are differentiable with respect to $\theta \in \Theta$.
	For each $\omega \in \Omega$, the function
	\begin{equation*}
	\Theta \; \longrightarrow \; \Re
	\; : \; \theta \; \longmapsto \; f_{\theta}(\omega)
	\end{equation*}
	is differentiable with respect to $\theta \in \Theta$.
	For each $\theta \in \Theta$, the following two equalities hold:
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\theta_{i}} \left(\; \int_{\Omega}\,f_{\theta}(\omega)\,\d\nu(\omega) \;\right)
	{\color{white}\;\cdot X(\omega)}
	&=&
	\int_{\Omega}\, \left(\,\dfrac{\partial}{\partial\,\theta_{i}} f_{\theta}(\omega)\right) \,\d\nu(\omega)\,,
	\quad\textnormal{and}
	\\
	\dfrac{\partial}{\partial\,\theta_{i}} \left(\; \int_{\Omega}\,X(\omega) \cdot f_{\theta}(\omega)\,\d\nu(\omega) \;\right)
	&=&
	\int_{\Omega}\, \left(\,X(\omega) \cdot \dfrac{\partial}{\partial\,\theta_{i}} f_{\theta}(\omega)\right) \,\d\nu(\omega)\,.
	\end{eqnarray*}
\end{itemize}
Then, the following statements hold:
\begin{enumerate}
\item\label{gradientLogKappa}
	The gradient vector of \,$\log\kappa(\theta)$\, with respect to $\theta$ can be given by:
	\begin{equation*}
	\nabla_{\theta}\!\left[\;\overset{{\color{white}.}}{\log}\,\kappa(\theta)\;\right]
	\;\; = \;\;
		E_{\theta}\!\left[\,X\,\right]\,,
	\quad
	\textnormal{for each \,$\theta\in\Theta$}\,.
	\end{equation*}
	Equivalently, in component form:
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\theta_{i}} \log \kappa(\theta)
	& = &
		\dfrac{1}{\kappa(\theta)} \cdot \dfrac{\partial\,\kappa(\theta)}{\partial\,\theta_{i}}
	\\
	& = &
		E_{\theta}\!\left[\,X_{i}\,\right]
	\;\; := \;\;
		\int_{\Omega}\, X_{i}(\omega) \cdot f_{\theta}(\omega)\,\d\nu(\omega)\,,
	\quad
	\textnormal{for each \,$i = 1,2,\ldots,p$}\,.
	\end{eqnarray*}
\item\label{formulaScoreVector}
	The score vector (i.e. gradient vector) of the log-likelihood $\log(f_{\theta})$ with respect to $\theta$ is given by:
	\begin{equation*}
	\nabla_{\theta}\!\left[\;\overset{{\color{white}.}}{\log}\,f_{\theta}(\omega)\;\right]
	\;\; = \;\;
		X(\omega) \, - \, E_{\theta}\!\left[\,X\,\right]\,,
	\quad
	\textnormal{for each \,$\omega\in\Omega$, \,$\theta\in\Theta$}\,.
	\end{equation*}	
	Equivalently, in component form:
	\begin{equation*}
	\dfrac{\partial}{\partial \theta_{i}}\left[\;\overset{{\color{white}.}}{\log}\,f_{\theta}(\omega)\;\right]
	\;\; = \;\;
		X_{i}(\omega) \, - \, E_{\theta}\!\left[\,X_{i}\,\right]\,,
	\quad
	\textnormal{for each \,$\omega\in\Omega$, \,$\theta\in\Theta$, \,$i = 1,2,\ldots,p$}\,.
	\end{equation*}	
\item\label{formulaHessianMatrix}
	The Hessian matrices of $\log\kappa(\theta)$ and the log-likelihood $\log(f_{\theta})$
	with respect to $\theta$ can be given by:
	\begin{equation*}
	\Hess_{\,\theta}\!\left[\;\overset{{\color{white}.}}{\log}\,f_{\theta}(\omega)\;\right]
	\;\; = \;\;
		-\,\Hess_{\,\theta}\!\left[\;\overset{{\color{white}.}}{\log}\,\kappa(\theta)\;\right]
	\;\; = \;\;
		-\,\Var_{\theta}\!\left(\,X\,\right)\,,
	\quad
	\textnormal{for each \,$\omega\in\Omega$, \,$\theta\in\Theta$}\,.
	\end{equation*}	
	Equivalently, in component form:
	\begin{eqnarray*}
	\dfrac{\partial^{2}}{\partial \theta_{i}\,\partial\theta_{j}}\left[\;\overset{{\color{white}.}}{\log}\,f_{\theta}(\omega)\;\right]
	& = &
		-\,\dfrac{\partial^{2}}{\partial \theta_{i}\,\partial\theta_{j}}\left[\;\overset{{\color{white}.}}{\log}\,\kappa(\theta)\;\right]
	\;\; = \;\;
		-\,\overset{{\color{white}.}}{\Cov}_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right),
	\end{eqnarray*}	
	for each \,$\omega\in\Omega$, \,$\theta\in\Theta$, \,$i = 1,2,\ldots,p$, where
	\begin{equation*}
	\overset{{\color{white}.}}{\Cov}_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right)
	\;\; := \;\;
		\int_{\Omega}\,
			\left(\,\overset{{\color{white}.}}{X}_{i}(\omega) - E_{\theta}\!\left[\,X_{i}\,\right]\,\right)
			\cdot
			\left(\,\overset{{\color{white}.}}{X}_{j}(\omega) - E_{\theta}\!\left[\,X_{j}\,\right]\,\right)
			\cdot
			f_{\theta}(\omega)
		\;\d\nu(\omega)\,.
	\quad
	\end{equation*}	
\end{enumerate}
\end{theorem}
\proof
\begin{enumerate}
\item
	Note that
	\begin{eqnarray*}
	\dfrac{\partial\,\kappa(\theta)}{\partial\,\theta_{i}}
	& = &
		\dfrac{\partial}{\partial\,\theta_{i}}
		\left(\;
			\int_{\Omega}\,
			\exp\!\left(\,\theta^{T}\cdot X(\omega)\,\right)\cdot h(\omega)
			\;\d\nu(\omega)
		\;\right)
	\;\; = \;\;
		\int_{\Omega}\,
			\left(\;
			h(\omega)
			\cdot
			\dfrac{\partial}{\partial\,\theta_{i}}\,
			\exp\!\left(\,\theta^{T}\cdot X(\omega)\,\right)
			\;\right)
		\;\d\nu(\omega)
	\\
	&=&
		\int_{\Omega}\,
			\left(\;
			\overset{{\color{white}-}}{h}(\omega)
			\cdot
			X_{i}(\omega)
			\cdot
			\exp\!\left(\,\theta^{T}\cdot X(\omega)\,\right)
			\;\right)
		\;\d\nu(\omega)
	\end{eqnarray*}
	Hence,
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\theta_{i}} \log \kappa(\theta)
	&=&
		\dfrac{1}{\kappa(\theta)} \cdot \dfrac{\partial\,\kappa(\theta)}{\partial\,\theta_{i}}
	\;\; = \;\;
		\dfrac{1}{\kappa(\theta)}
		\cdot
		\int_{\Omega}\,
			\left(\;
			X_{i}(\omega)
			\cdot
			\overset{{\color{white}-}}{h}(\omega)
			\cdot
			\exp\!\left(\,\theta^{T}\cdot X(\omega)\,\right)
			\;\right)
		\;\d\nu(\omega)
	\\
	& = &
		\int_{\Omega}\,
			\left(\;
			X_{i}(\omega)
			\cdot
			\dfrac{
				h(\omega) \cdot \exp\!\left(\,\theta^{T}\cdot X(\omega)\,\right)
				}{
				\kappa(\theta)
				}
			\;\right)
		\;\d\nu(\omega)
	\\
	& = &
		\int_{\Omega}\,
			X_{i}(\omega) \cdot f_{\theta}(\omega)
		\;\d\nu(\omega)
	\;\; =: \;\;
		E_{\theta}\!\left[\,X_{i}\,\right]
	\end{eqnarray*}
\item
	First, note that
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\theta_{i}}\,f_{\theta}(\omega)
	& = &
		\dfrac{\partial}{\partial\,\theta_{i}}
		\left[\;
			\dfrac{1}{\kappa(\theta)}
			\cdot
			h(\omega)
			\cdot
			\exp\left\{\; \theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}
		\;\right]
	\\
	&=&
		\dfrac{-1}{\kappa(\theta)^{2}}
		\cdot
		\dfrac{\partial\,\kappa(\theta)}{\partial\,\theta_{i}}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}
		\; + \;
		\dfrac{1}{\kappa(\theta)}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}
		\cdot
		X_{i}(\omega)
	\\
	&=&
		\left(\;
			\dfrac{-1}{\kappa(\theta)}
			\cdot
			\dfrac{\partial\,\kappa(\theta)}{\partial\,\theta_{i}}
			\; + \;
			X_{i}(\omega)
		\;\right)
		\cdot
		\dfrac{1}{\kappa(\theta)}
		\cdot
		h(\omega)
		\cdot
		\exp\left\{\; \theta^{T} \!\overset{{\color{white}-}}{\cdot}\! X(\omega) \;\right\}
	\\
	&=&
		\left(\;
			X_{i}(\omega)
			\; - \;
			\dfrac{\partial\,}{\partial\,\theta_{i}} \log \kappa(\theta)
		\;\right)
		\cdot
		f_{\theta}(\omega)
	\\
	&=&
		\left(\;
			\overset{{\color{white}.}}{X}_{i}(\omega) \, - \, E_{\theta}\!\left[\,X_{i}\,\right]
		\;\right)
		\cdot
		f_{\theta}(\omega)\,,
		\quad
		\textnormal{by \eqref{gradientLogKappa}}
	\end{eqnarray*}
	Hence,
	\begin{equation*}
	\dfrac{\partial}{\partial\,\theta_{i}} \log f_{\theta}(\omega)
	\;\; = \;\;
		\dfrac{1}{f_{\theta}(\omega)}
		\cdot
		\dfrac{\partial}{\partial\,\theta_{i}}\,f_{\theta}(\omega)
	\;\; = \;\;
		X_{i}(\omega) \, - \, E_{\theta}\!\left[\,X_{i}\,\right],
	\end{equation*}
	as required.
\item
	\textbf{Claim 1:}\quad
	For each $i,j = 1, 2, \ldots, p$, we have
	\begin{equation*}
	\dfrac{\partial}{\partial\,\theta_{j}}\,E_{\theta}\!\left[\,X_{i}\,\right]
	\;\; = \;\;
		\Cov_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right)
	\;\; := \;\;
		\int_{\Omega}\,
			\left(\,\overset{{\color{white}.}}{X}_{i}(\omega) - E_{\theta}\!\left[\,X_{i}\,\right]\,\right)
			\cdot
			\left(\,\overset{{\color{white}.}}{X}_{j}(\omega) - E_{\theta}\!\left[\,X_{j}\,\right]\,\right)
			\cdot
			f_{\theta}(\omega)
		\;\d\nu(\omega)
	\end{equation*}
	Proof of Claim 1:\quad
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\theta_{j}}\,E_{\theta}\!\left[\,X_{i}\,\right]
	& = &
		\dfrac{\partial}{\partial\,\theta_{j}}
		\left(\;
			\int_{\Omega}\,
			X_{i}(\omega) \cdot f_{\theta}(\omega)
			\;\d\nu(\omega)
		\;\right)
	\;\; = \;\;
		\int_{\Omega}\,
			X_{i}(\omega) \cdot
			\dfrac{\partial}{\partial\,\theta_{j}}\,f_{\theta}(\omega)
		\;\d\nu(\omega)
	\\
	& = &
		\int_{\Omega}\,
			X_{i}(\omega) \cdot
			\left(\,\overset{{\color{white}.}}{X}_{j}(\omega) - E_{\theta}\!\left[\,X_{j}\,\right]\,\right)
			\cdot
			f_{\theta}(\omega)
		\;\d\nu(\omega)\,,
		\quad
		\textnormal{see proof of \eqref{formulaScoreVector} above}
	\\
	& = &
		\int_{\Omega}\,
			\left(\,\overset{{\color{white}.}}{X}_{i}(\omega) - E_{\theta}\!\left[\,X_{i}\,\right] + E_{\theta}\!\left[\,X_{i}\,\right]\,\right)
			\cdot
			\left(\,\overset{{\color{white}.}}{X}_{j}(\omega) - E_{\theta}\!\left[\,X_{j}\,\right]\,\right)
			\cdot
			f_{\theta}(\omega)
		\;\d\nu(\omega)
	\\
	& = &
		{\color{white}+}\;
		\int_{\Omega}\,
			\left(\,\overset{{\color{white}.}}{X}_{i}(\omega) - E_{\theta}\!\left[\,X_{i}\,\right]\,\right)
			\cdot
			\left(\,\overset{{\color{white}.}}{X}_{j}(\omega) - E_{\theta}\!\left[\,X_{j}\,\right]\,\right)
			\cdot
			f_{\theta}(\omega)
		\;\d\nu(\omega)
	\\
	&&
		+\;\;
		E\!\left[\,X_{i}\,\right]
		\cdot
		\int_{\Omega}\,
			\left(\,\overset{{\color{white}.}}{X}_{j}(\omega) - E_{\theta}\!\left[\,X_{j}\,\right]\,\right)
			\cdot
			f_{\theta}(\omega)
		\;\d\nu(\omega)
	\\
	& \overset{{\color{white}\vert}}{=} &
		\Cov_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right) \; + \; 0
	\;\; = \;\;
		\Cov_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right),
		\quad
		\textnormal{as desired}.
	\end{eqnarray*}
	This proves Claim 1.
	
	\vskip 0.5cm
	\noindent
	Now,
	\begin{eqnarray*}
	\dfrac{\partial^{2}}{\partial\theta_{i}\,\partial\theta_{j}} \left[\;\overset{{\color{white}.}}{\log} \, f_{\theta}(\omega)\;\right]
	&=&
		\dfrac{\partial}{\partial\,\theta_{i}}\!
		\left(\;
			\dfrac{\partial}{\partial\,\theta_{j}}\, \log \, f_{\theta}(\omega)
		\;\right)
	\\
	&=&
		\dfrac{\partial}{\partial\,\theta_{i}}\!
		\left(\;
			\overset{{\color{white}-}}{X}_{j}(\omega) \, - \, E_{\theta}\!\left[\,X_{j}\,\right]
		\;\right),
		\quad
		\textnormal{by \eqref{formulaScoreVector}}
	\\
	&\overset{{\color{white}\vert}}{=}&
		-\; \Cov_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right),
		\quad
		\textnormal{by Claim 1}.
	\end{eqnarray*}
	
	\vskip 0.5cm
	\noindent
	Lastly,
	\begin{eqnarray*}
	\dfrac{\partial^{2}}{\partial\theta_{i}\,\partial\theta_{j}} \left[\;\overset{{\color{white}.}}{\log} \, \kappa(\theta)\;\right]
	&=&
		\dfrac{\partial}{\partial\,\theta_{i}}\!
		\left(\;
			\dfrac{\partial}{\partial\,\theta_{j}}\, \log \, \kappa(\theta)
		\;\right)
	\\
	&=&
		\dfrac{\partial}{\partial\,\theta_{i}}\!
		\left(\; E_{\theta}\!\left[\,X_{j}\,\right] \;\right),
		\quad
		\textnormal{by \eqref{formulaScoreVector}}
	\\
	&\overset{{\color{white}\vert}}{=}&
		\Cov_{\theta}\!\left(\,X_{i}\,,\,X_{j}\,\right),
		\quad
		\textnormal{by Claim 1}.
	\end{eqnarray*}
\end{enumerate}
The proof of the present Theorem is now complete.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
