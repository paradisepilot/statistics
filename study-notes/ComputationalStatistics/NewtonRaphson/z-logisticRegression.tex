
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 1.0cm

\section{Maximum likelihood estimation for logistic regression via the Newton-Raphson method}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space\,,
	\,and
	\,$n \in \N$\, is a natural number.
\item
	$Y^{(1)}, \ldots, Y^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$\,
	are binary random variables.
\end{itemize}
Suppose:
\begin{itemize}
\item
	The binary random variables \,$Y^{(i)}$, $i = 1, 2, \ldots, n$,\,
	are independent (but NOT necessarily identically distributed).
\item
	There exist
	\,$\beta \in \Re^{p}$\,
	and
	\,$x^{(1)}, x^{(2)}, \ldots, x^{(n)} \in \Re^{p}$\,,
	such that
	\begin{equation*}
	\pi_{i}
	\;\; := \;\;
		P\!\left(\; Y^{(i)} = 1 \;\right)
	\;\; = \;\;
		E\!\left[\; Y^{(i)}  = 1 \;\right]
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\,\beta^{T} \cdot x^{(i)} \,\right)
			}{
			1 \,+\, \exp\!\left(\,\beta^{T} \cdot x^{(i)} \,\right)
			}\,,
	\quad
	\textnormal{for each \,$i = 1,2,\ldots,n$}\,.
	\end{equation*}
\end{itemize}
Then, the following statements are true:
\begin{enumerate}
\item
	The joint probability distribution of \;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, is given by:
	\begin{eqnarray*}
	P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\; \beta^{T} \cdot \overset{n}{\underset{i=1}{\sum}}\;y_{i}\,x^{(i)} \,\right)
		}{
			\overset{n}{\underset{i=1}{\prod}} \left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
		}\,.
	\end{eqnarray*}
%	Consequently, if we define:
%	\begin{equation*}
%	\begin{array}{ccccccccl}
%		\kappa
%		& : & \Re^{p} & \longrightarrow & \Re
%		& : & \beta & \longmapsto & \overset{n}{\underset{i=1}{\prod}}
%			\left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right),
%	\\
%		\overset{{\color{white}\vert}}{Z}
%		& : & \Omega & \longrightarrow & \Re^{p}
%		& : & \omega & \longmapsto & \overset{n}{\underset{i=1}{\sum}}\;Y^{(i)}(\omega) \cdot x^{(i)}\,,
%		\quad
%		\textnormal{and}
%	\\
%		\overset{{\color{white}\vert}}{h}
%		& : & \Omega & \longrightarrow & \Re
%		& : & \omega & \longmapsto & 1\,,
%	\end{array}
%	\end{equation*}
%	then \,$P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)$\,
%	can be expressed as:
%	\begin{eqnarray*}
%	P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
%	\;\; = \;\;
%		\dfrac{
%			\exp\!\left(\; \beta^{T} \cdot \overset{n}{\underset{i=1}{\sum}}\;y_{i}\,x^{(i)} \,\right)
%		}{
%			\overset{n}{\underset{i=1}{\prod}} \left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
%		}
%	\;\; = \;\;
%		\dfrac{h(\omega)}{\kappa(\,\beta\,)} \cdot \exp\!\left(\;\beta^{T} \overset{{\color{white}\vert}}{\cdot} Z(\omega)\;\right)
%	\end{eqnarray*}
	In particular, the joint probability distribution of \;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, belongs to
	a canonical exponential family.
\item
	The logarithm of the joint probability distribution of
	\;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, is given by:
	\begin{eqnarray*}
	\log\,L
		&=&
		\log\,P\!\left(\; Y^{(1)}=y_{1},\,Y^{(2)}=y_{2},\,\ldots,\;Y^{(n)}=y_{n} \;\right)
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot(\,\beta^{T} \cdot x^{(i)}\,)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp(\,\beta^{T} \cdot x^{(i)}\,)\,\right)
			\;\right\}
	\end{eqnarray*}
\item\label{logisticScoreEqn}
	The gradient vector of \,$\log L$\, with respect to $\beta$ is given by:
	\begin{equation*}
	\nabla_{\beta}\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)
	\;\; = \;\;
		X^{T} \cdot \left(\, y \,-\, \pi(\beta) \,\right),
	\end{equation*}
	and the (vectorial) score equation is thus given by:
	\begin{equation*}
	%\nabla_{\beta}\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)
	%\;\; = \;\;
		X^{T} \cdot \left(\, y \,-\, \pi(\beta) \,\right)
	\;\; = \;\;
		0_{p} \;\; \in \;\; \Re^{p}\,,
	\end{equation*}
	where $y \in \Re^{n}$ and $X \in \Re^{n \times p}$ are respectively defined by
	\begin{equation*}
	y
	\;\; := \;\;
		\left(\;\begin{array}{c}
		y_{1} \\
		\underset{{\color{white}.}}{\overset{{\color{white}.}}{\vdots}} \\
		y_{n}
		\end{array}\;\right)
	\;\in\;
		\Re^{n \times 1}\,, 
	\quad\textnormal{and}\quad
	X
	\;\;=\;\;
		\left(\;\;\overset{{\color{white}.}}{X}_{ik}\;\;\right)
	\;\;:=\;\;
		\left(\;\; x^{(i)}_{k} \;\;\right)
	\;\;=\;\;
		\left(\;\;
			\begin{array}{c} (x^{(1)})^{T} \\ (x^{(2)})^{T} \\ \underset{{\color{white}1}}{\vdots} \\ (x^{(n)})^{T} \end{array}
			\;\;\right)
	\; \in \; \Re^{n \times p}\,,
	\end{equation*}
	%\begin{equation*}
	%y \,:=\, \left(\;y_{1},\ldots,y_{n}\;\right)^{T} \in \Re^{n \times 1}\,, 
	%\end{equation*}
	and \,$\pi : \Re^{p} \longrightarrow \Re^{n}$\, is given by
	\begin{equation*}
	\pi(\beta) \;\;:=\;\; \left(\;
		\begin{array}{c} \pi_{1}(\beta) \\ \underset{{\color{white}^{.}}}{\vdots} \\ \pi_{n}(\beta) \end{array}
		\;\right),
	\quad{where}\quad
	\pi(\beta)
	\; := \;
		\dfrac{\exp(\,\beta^{T} \cdot x^{(i)}\,)}{1 + \exp(\,\beta^{T} \cdot x^{(i)}\,)}
	\; = \;
		\dfrac{1}{1 + \exp(\,{\color{red}-}\,\beta^{T} \cdot x^{(i)}\,)}
	\end{equation*}
	The score equation can be given equivalently in component form by:
	\begin{equation*}
	\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \left(\,y_{i} - \pi_{i}\,\right)
	\;\; = \;\; 0\,,
	\quad
	\textnormal{for each \,$k = 1, 2,\ldots,p$}\,;
	\end{equation*}
	or equivalently but more explicitly,
	\begin{equation*}
	\overset{n}{\underset{i=1}{\sum}}\;\,
	\dfrac{x^{(i)}_{k}}{1 \,+\, \exp\!\left({\color{red}-}\;\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)}
	\;\; = \;\;
	\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}\,,
	\quad
	\textnormal{for each \,$k = 1, 2,\ldots,p$}\,.
	\end{equation*}
\newpage
\item
	The Newton-Raphson method, when applied to solve the score equation for $\beta \in \Re^{p}$
	in \eqref{logisticScoreEqn} above, takes the following explicit form:
	\begin{center}
	\begin{minipage}{6in}
	\underline{\textnormal{\bf Newton-Raphson method applied to solve the score equation from logistic regression}}
	\vskip 0.3cm
	\noindent
	\textnormal{Choose $\varepsilon > 0$. ($\varepsilon > 0$ will be used as the termination threshold.)
	\vskip 0.4cm
	\textnormal{\bf INITIALIZATION}
		\begin{itemize}
		\item
			Choose \,$\beta^{(0)} \in \Re^{p}$\, arbitrarily.
		\item
			Define \,$\beta^{(1)} \in \Re^{p}$\, as the solution of the following (vectorial) linear equation:
			\begin{equation*}
			X^{T} \cdot D(\beta^{(0)}) \cdot X \cdot {\color{red}\beta^{(1)}}
				\;\; = \;\; X^{T} \cdot D(\beta^{(0)}) \cdot X \cdot \beta^{(0)}
				\;+\; X^{T}\cdot\!\left(\,y - \pi(\beta^{(0)})\,\right).
			\end{equation*}
		\item
			Let \,$k \,:=\, 1$\,.
		\end{itemize}
	\vskip 0.2cm
	\textnormal{\bf WHILE} \; $\left(\;\Vert\,\beta^{(k)} \,-\, \beta^{(k-1)}\,\Vert > \varepsilon\;\right)$ \; \textnormal{\bf DO}
	\begin{itemize}
	\item
		Define \,$\beta^{(k+1)} \in \Re^{p}$\, as the solution of the following (vectorial) linear equation:
		\begin{equation*}
			X^{T} \cdot D(\beta^{(k)}) \cdot X \cdot {\color{red}\beta^{(k+1)}}
			\;\; = \;\;
				X^{T} \cdot D(\beta^{(k)}) \cdot X \cdot \beta^{(k)}
				\;+\; X^{T}\cdot\!\left(\,y - \pi(\beta^{(k)})\,\right).
		\end{equation*}
	\item
		Increment \,$k$.	
	\end{itemize}
	\textnormal{\bf ENDWHILE}
	\vskip 0.4cm
	\textnormal{\bf OUTPUT}
	\begin{itemize}
	\item
		Return \,$\beta^{(k)} \in \Re^{p}$\, as approximate solution to the score equation
		in \eqref{logisticScoreEqn}.
	\end{itemize}
	}
	\end{minipage}
	\end{center}
\end{enumerate}
\end{proposition}
\proof
\begin{enumerate}
\item
	First, note that
	\begin{equation*}
	\dfrac{\pi_{i}}{1 - \pi_{i}}
	\;\; = \;\;
		\dfrac{\exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}{1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}
		\cdot
		\dfrac{1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}{1}
	\;\; = \;\;
		\exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)
	\end{equation*}
	Hence,
	\begin{eqnarray*}
	&&
		P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\prod}}\;\, \pi_{i}^{y_{i}} \cdot \left(1 - \pi_{i}\right)^{1-y_{i}}
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\prod}}\; \left(\dfrac{\pi_{i}}{1-\pi_{i}}\right)^{y_{i}} \cdot \left(1 - \pi_{i}\right)
	\\
	&=&
		\overset{n}{\underset{i=1}{\prod}}\;
			\left(\, \exp\!\left(\beta^{T}\cdot x^{(i)}\right) \,\right)^{y_{i}}
			\cdot
			\left(\, \dfrac{1}{1+\exp(\beta^{T}\cdot x^{(i)})} \,\right)
	\;\; = \;\;
		\dfrac{
			\overset{n}{\underset{i=1}{\prod}}\;\exp\!\left(\,\beta^{T}\cdot y_{i}\,x^{(i)}\,\right)
		}{
			\overset{n}{\underset{i=1}{\prod}}\;\left(\,\overset{{\color{white}.}}{1} + \exp(\beta^{T}\cdot x^{(i)}) \,\right)
		}
	\\
	&=&
		\dfrac{
			\exp\!\left(\; \overset{n}{\underset{i=1}{\sum}}\;\beta^{T}\cdot y_{i}\,x^{(i)} \;\right)
		}{
			\overset{n}{\underset{i=1}{\prod}}\,\left(\,\overset{{\color{white}.}}{1} + \exp(\beta^{T}\cdot x^{(i)}) \,\right)
		}
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\; \beta^{T} \cdot \overset{n}{\underset{i=1}{\sum}}\;y_{i}\,x^{(i)} \,\right)
		}{
			\overset{n}{\underset{i=1}{\prod}}\,\left(\,\overset{{\color{white}.}}{1} + \exp(\beta^{T}\cdot x^{(i)}) \,\right)
		}
	\end{eqnarray*}
\item
	\begin{eqnarray*}
	\log L
	&=&
		\log\,P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\\
	&=&
		\log\!\left(\;\,
			\overset{n}{\underset{i=1}{\prod}}\;\, \pi_{i}^{y_{i}} \cdot \left(1 - \pi_{i}\right)^{1-y_{i}}
			\,\right)
	\;\;=\;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\log\,\pi_{i}
			\, \overset{{\color{white}\vert}}{+} \,
			(1-y_{i})\cdot\log\!\left(1 - \pi_{i}\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\log\left(\,\dfrac{\pi_{i}}{1 - \pi_{i}}\,\right)
			\, \overset{{\color{white}\vert}}{+} \,
			\log\!\left(1 - \pi_{i}\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\left(\,\beta^{T} \cdot x^{(i)}\,\right)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)\,\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)\,\right)
			\;\right\}	
	\end{eqnarray*}
\item
	The partial derivative of $\log\,L$ with respect to $\beta_{k}$ is:
	\begin{eqnarray*}
	\dfrac{\partial\,\log L}{\partial\,\beta_{k}}
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\,x^{(i)}_{k}
			\; \overset{{\color{white}\vert}}{-} \;
			\dfrac{
				\exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}{
				1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}
			\cdot
			x^{(i)}_{k}
			\;\right\}	
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\,x^{(i)}_{k}
			\; \overset{{\color{white}\vert}}{-} \;
			\pi_{i} \cdot x^{(i)}_{k}
			\;\right\}	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\;\;
		x^{(i)}_{k}	
		\cdot
		\left(\; y_{i} \; \overset{{\color{white}.}}{-} \; \pi_{i} \;\right)
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
		\; - \;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \pi_{i}
	\end{eqnarray*}
	Thus, the score equations are
	\begin{eqnarray*}
	\dfrac{\partial\,\log L}{\partial\,\beta_{k}} \;=\; 0
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \pi_{i}
		\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
	\\
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot
		\dfrac{
			\exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
			}{
			1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
			}
		\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
	\\
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot
		\dfrac{1}{1 \,+\, \exp\!\left({\color{red}-}\;\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)}
		\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
	\end{eqnarray*}
	for each $k = 1,2,\ldots,p$.
\item
	By \eqref{logisticScoreEqn}, the score equation is $g(\,\beta\,;\,X,y\,) = 0$,
	where $\beta$ is the unknown to be solved, and
	$g : \Re^{p} \longrightarrow \Re^{p}$ is defined by
	\begin{eqnarray*}
	g\!\left(\,\overset{{\color{white}.}}{\beta}\,;\,X,y\,\right){\color{white}.}
	& = &
		{\color{white}-}\, X^{T} \cdot \left(\,y \,\overset{{\color{white}.}}{-}\, \pi(\beta)\,\right)
	\end{eqnarray*}
	where the matrix $X$ is given by:
	\begin{equation*}
	X \;\in\; \Re^{n \times p}\,,
	\quad
	X_{ik} \; = \; x^{(i)}_{k}\,,
	\end{equation*}
	the component functions \,$g_{k}(\,\beta\,;\,X,y\,)$,\, \,$k = 1,2,\ldots,p$,\,
	of \,$g : \Re^{p} \longrightarrow \Re^{p}$\, can be given by:
	\begin{equation*}
	g_{k}(\,\beta\,;\,X,y\,)
	\;\; := \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \left(\,y_{i} - \pi_{i}(\beta)\,\right),
	\end{equation*}
	and the component functions \,$\pi_{i}(\beta)$, $k = 1,2,\ldots,p$,\, can be given by:
	\begin{equation*}
	\pi_{i}(\beta)
	\;\ := \;\;
		\dfrac{\exp(\beta^{T} \cdot x^{(i)})}{1+\exp(\beta^{T}\cdot x^{(i)})}
	\;\ = \;\;
		\dfrac{1}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}\,.
	\end{equation*}
	Now, recall that the Newton-Raphson iterations proceed as follows:
	Given $\beta^{(m)} \in \Re^{p}$, where $m \in \N$, $\beta^{(m+1)} \in \Re^{p}$
	is defined to be the solution of the following vectorial linear equation:
	\begin{equation*}
	\left[\,\overset{{\color{white}\vert}}{\nabla}_{\beta}\,g(\,\beta^{(m)}\,;\,X,y\,)\,\right] \cdot {\color{red}\beta^{(m+1)}}
	\;\; = \;\;
		\left[\,\overset{{\color{white}\vert}}{\nabla}_{\beta}\,g(\,\beta^{(m)}\,;\,X,y\,)\,\right] \cdot \beta^{(m)}
		\; - \;
		g(\,\beta^{(m)}\,;\,X,y\,)
	\end{equation*}
	Hence, in order to complete the proof of the present Proposition,
	it remains only to work out an expression for
	\,$\overset{{\color{white}\vert}}{\nabla}_{\beta}\,g(\,\beta\,;\,X,y\,)$\,.
	To this end, first note that
	\begin{eqnarray*}
	\dfrac{\partial\,\pi_{i}(\beta)}{\partial\,\beta_{k}}
	&=&
		\left[\,1+\exp(\,-\,\beta^{T}\cdot x^{(i)})\,\right]^{-2}
		\cdot
		\exp(\,-\,\beta^{T}\cdot x^{(i)})
		\cdot
		x^{(i)}_{k}
	\\
	&=&
		\dfrac{1}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}
		\cdot
		\dfrac{\exp(\,-\,\beta^{T}\cdot x^{(i)})}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}
		\cdot
		x^{(i)}_{k}
	\\
	&=&
		\dfrac{1}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}
		\cdot
		\dfrac{1}{1+\exp(\beta^{T}\cdot x^{(i)})}
		\cdot
		x^{(i)}_{k}
	\\
	&=&
		\pi_{i}(\beta)
		\cdot
		\left(1\overset{{\color{white}.}}{-}\pi_{i}(\beta)\right)
		\cdot
		x^{(i)}_{k}
	\end{eqnarray*}
	Hence, we see that
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\beta_{l}}\!\left(\,\overset{{\color{white}\vert}}{g}_{k}(\,\beta\,;\,X,y\,)\,\right)
	& = &
		-\;\overset{n}{\underset{i=1}{\sum}}\;x^{(i)}_{k}\cdot\dfrac{\partial\,\pi_{i}(\beta)}{\partial\,\beta_{l}}
	\;\; = \;\;
		-\;\overset{n}{\underset{i=1}{\sum}}\;x^{(i)}_{k}\cdot
		\pi_{i}(\beta)
		\cdot
		\left(1\overset{{\color{white}.}}{-}\pi_{i}(\beta)\right)
		\cdot
		x^{(i)}_{l}
	\end{eqnarray*}
	In the matrix notation, the above equation becomes
	\begin{equation*}
	\nabla_{\beta}\,g(\,\beta\,,\,X,y\,)
	\;\; = \;\;
		-\,X^{T} \cdot D(\beta) \cdot X\,,
	\end{equation*}
	where
	\begin{equation*}
	D(\beta) \; := \; \diag\!\left(\;
		\overset{{\color{white}-}}{\pi}_{1}(\beta) \cdot (\,1-\pi_{1}(\beta)\,)\,,
		\,\ldots\,,
		\pi_{n}(\beta) \cdot (\,1-\pi_{n}(\beta)\,)
		\;\right)
		\;\in\;
		\Re^{n \times n}\,.
	\end{equation*}
	The desired statement now follows immediately from the preceding expression of
	$\nabla_{\beta}\,g(\,\beta\,,\,X,y\,)$.
\end{enumerate}
This completes the proof of the present Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
