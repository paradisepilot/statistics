
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 1.0cm

\section{Maximum likelihood estimation for logistic regression via the Newton-Raphson method}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space\,,
	\,and
	\,$n \in \N$\, is a natural number.
\item
	$Y^{(1)}, \ldots, Y^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$\,
	are binary random variables.
\end{itemize}
Suppose:
\begin{itemize}
\item
	The binary random variables \,$Y^{(i)}$, $i = 1, 2, \ldots, n$,\,
	are independent (but NOT necessarily identically distributed).
\item
	There exist
	\,$\beta \in \Re^{p}$\,
	and
	\,$x^{(1)}, x^{(2)}, \ldots, x^{(n)} \in \Re^{p}$\,,
	such that
	\begin{equation*}
	\pi_{i}
	\;\; := \;\;
		P\!\left(\; Y^{(i)} = 1 \;\right)
	\;\; = \;\;
		E\!\left[\; Y^{(i)}  = 1 \;\right]
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\,\beta^{T} \cdot x^{(i)} \,\right)
			}{
			1 \,+\, \exp\!\left(\,\beta^{T} \cdot x^{(i)} \,\right)
			}\,,
	\quad
	\textnormal{for each \,$i = 1,2,\ldots,n$}\,.
	\end{equation*}
\end{itemize}
Then, the following statements are true:
\begin{enumerate}
\item
	The joint probability distribution of \;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, is given by:
	\begin{eqnarray*}
	P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\; \beta^{T} \cdot \overset{n}{\underset{i=1}{\sum}}\;y_{i}\,x^{(i)} \,\right)
		}{
			\overset{n}{\underset{i=1}{\prod}} \left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
		}\,.
	\end{eqnarray*}
%	Consequently, if we define:
%	\begin{equation*}
%	\begin{array}{ccccccccl}
%		\kappa
%		& : & \Re^{p} & \longrightarrow & \Re
%		& : & \beta & \longmapsto & \overset{n}{\underset{i=1}{\prod}}
%			\left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right),
%	\\
%		\overset{{\color{white}\vert}}{Z}
%		& : & \Omega & \longrightarrow & \Re^{p}
%		& : & \omega & \longmapsto & \overset{n}{\underset{i=1}{\sum}}\;Y^{(i)}(\omega) \cdot x^{(i)}\,,
%		\quad
%		\textnormal{and}
%	\\
%		\overset{{\color{white}\vert}}{h}
%		& : & \Omega & \longrightarrow & \Re
%		& : & \omega & \longmapsto & 1\,,
%	\end{array}
%	\end{equation*}
%	then \,$P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)$\,
%	can be expressed as:
%	\begin{eqnarray*}
%	P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
%	\;\; = \;\;
%		\dfrac{
%			\exp\!\left(\; \beta^{T} \cdot \overset{n}{\underset{i=1}{\sum}}\;y_{i}\,x^{(i)} \,\right)
%		}{
%			\overset{n}{\underset{i=1}{\prod}} \left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
%		}
%	\;\; = \;\;
%		\dfrac{h(\omega)}{\kappa(\,\beta\,)} \cdot \exp\!\left(\;\beta^{T} \overset{{\color{white}\vert}}{\cdot} Z(\omega)\;\right)
%	\end{eqnarray*}
	In particular, the joint probability distribution of \;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, belongs to
	a canonical exponential family.
\item
	The logarithm of the joint probability distribution of
	\;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, is given by:
	\begin{eqnarray*}
	\log\,L
		&=&
		\log\,P\!\left(\; Y^{(1)}=y_{1},\,Y^{(2)}=y_{2},\,\ldots,\;Y^{(n)}=y_{n} \;\right)
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot(\,\beta^{T} \cdot x^{(i)}\,)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp(\,\beta^{T} \cdot x^{(i)}\,)\,\right)
			\;\right\}
	\end{eqnarray*}
\item\label{logisticScoreEqn}
	The gradient
	\,$\nabla\!\left(\,\log L\,\right) : \Re^{p} \longrightarrow \Re^{p}$\,
	of
	\,$\log L$\,
	with respect to $\beta$ is given by:
	\begin{equation*}
	\nabla\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)(\beta)
	\;\; = \;\;
		X^{T} \cdot \left(\, y \,\overset{{\color{white}.}}{-}\, \pi(\beta) \right),
	\end{equation*}
	and the (vectorial) score equation
	\,$\nabla\!\left(\,\log L\,\right)(\beta) \,=\, 0$\,
	is thus given by:
	\begin{equation*}
		X^{T} \cdot \left(\, y \,\overset{{\color{white}.}}{-}\, \pi(\beta) \right)
	\;\; = \;\;
		0_{p} \;\; \in \;\; \Re^{p}\,,
	\end{equation*}
	where $y \in \Re^{n}$ and $X \in \Re^{n \times p}$ are respectively defined by
	\begin{equation*}
	y
	\;\; := \;\;
		\left(\;\begin{array}{c}
		y_{1} \\
		\underset{{\color{white}.}}{\overset{{\color{white}.}}{\vdots}} \\
		y_{n}
		\end{array}\;\right)
	\;\in\;
		\Re^{n \times 1}\,, 
	\quad\textnormal{and}\quad
	X
	\;\;=\;\;
		\left(\;\;\overset{{\color{white}.}}{X}_{ik}\;\;\right)
	\;\;:=\;\;
		\left(\;\; x^{(i)}_{k} \;\;\right)
	\;\;=\;\;
		\left(\;\;
			\begin{array}{c} (x^{(1)})^{T} \\ (x^{(2)})^{T} \\ \underset{{\color{white}1}}{\vdots} \\ (x^{(n)})^{T} \end{array}
			\;\;\right)
	\; \in \; \Re^{n \times p}\,,
	\end{equation*}
	and \,$\pi : \Re^{p} \longrightarrow \Re^{n}$\, is given by
	\begin{equation*}
	\pi(\beta) \;\;:=\;\; \left(\;
		\begin{array}{c} \pi_{1}(\beta) \\ \underset{{\color{white}^{.}}}{\vdots} \\ \pi_{n}(\beta) \end{array}
		\;\right),
	\quad{where}\quad
	\pi_{i}(\beta)
	\; := \;
		\dfrac{\exp(\,\beta^{T} \cdot x^{(i)}\,)}{1 + \exp(\,\beta^{T} \cdot x^{(i)}\,)}
	\; = \;
		\dfrac{1}{1 + \exp(\,{\color{red}-}\,\beta^{T} \cdot x^{(i)}\,)}
	\end{equation*}
\item\label{DerivativePi}
	The derivative
	\,$\nabla\pi : \Re^{p} \longrightarrow \Re^{n \times p}$\,
	of
	\,$\pi : \Re^{p} \longrightarrow \Re^{n}$\,
	is given by
	\begin{equation*}
	\left(\,\nabla\pi\,\right)(\beta)
	\;\; = \;\;
		D(\beta) \cdot X\,,
	\end{equation*}
	where
	\,$D : \Re^{p} \longrightarrow \Re^{n \times n}$\,
	is defined by
	\begin{equation*}
	D(\beta) \; := \; \diag\!\left(\;
		\overset{{\color{white}-}}{\pi}_{1}(\beta) \cdot (\,1-\pi_{1}(\beta)\,)\,,
		\,\ldots\,,
		\pi_{n}(\beta) \cdot (\,1-\pi_{n}(\beta)\,)
		\;\right)
		\;\in\;
		\Re^{n \times n}\,.
	\end{equation*}
\item\label{HessianLogLikelihood}
	The Hessian
	\,$\nabla^{2}\!\left(\,\log L\,\right) : \Re^{p} \longrightarrow \Re^{p \times p}$\,
	of the log-likelihood \,$\log L$\, is given by:
	\begin{equation*}
	\nabla^{2}\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)(\beta)
	\;\; = \;\;
		-\, X^{T} \cdot \left(\,\nabla\pi\,\right)(\beta)
	\;\; = \;\;
		-\, X^{T} \cdot D(\beta) \cdot X\,.
	\end{equation*}
\newpage
\item
	The Newton-Raphson method, when applied to solve the score equation for $\beta \in \Re^{p}$
	in \eqref{logisticScoreEqn} above, takes the following explicit form:
	\begin{center}
	\setlength{\fboxsep}{2em}
	\fbox{\begin{minipage}{5.25in}
	\begin{center}
		\textnormal{\LARGE\bf The Newton-Raphson method }
		\vskip 0.25cm
		\textnormal{\large\bf applied to solve the score equation of logistic regression}
		\vskip -0.1cm
		\rule{4.575in}{.3pt}
	\end{center}
	\textnormal{
		\textbf{INITIALIZATION}
		\begin{itemize}
		\item
			Choose $\varepsilon > 0$ (error tolerance).
		\item
			Choose \,$\beta^{(0)} \in \Re^{p}$\, arbitrarily.
		\item
			Define \,$\beta^{(1)} \in \Re^{p}$\, as the solution of the following (vectorial) linear equation:
			\begin{equation*}
			X^{T} \cdot D(\beta^{(0)}) \cdot X \cdot {\color{red}\beta^{(1)}}
				\;\; = \;\; X^{T} \cdot D(\beta^{(0)}) \cdot X \cdot \beta^{(0)}
				\;+\; X^{T}\cdot\!\left(\,y - \pi(\beta^{(0)})\,\right).
			\end{equation*}
		\item
			Initialize \,$k$\, to $1$.
		\end{itemize}
		\vskip 0.2cm
		\textbf{WHILE} \; $\left(\;\Vert\,\beta^{(k)} \,-\, \beta^{(k-1)}\,\Vert > \varepsilon\;\right)$ \; \textnormal{\bf DO}
		\begin{itemize}
		\item
			Define \,$\beta^{(k+1)} \in \Re^{p}$\, as the solution of the following (vectorial) linear equation:
			\begin{equation*}
				X^{T} \cdot D(\beta^{(k)}) \cdot X \cdot {\color{red}\beta^{(k+1)}}
				\;\; = \;\;
					X^{T} \cdot D(\beta^{(k)}) \cdot X \cdot \beta^{(k)}
					\;+\; X^{T}\cdot\!\left(\,y - \pi(\beta^{(k)})\,\right).
			\end{equation*}
		\item
			Increment \,$k$\, by $1$.	
		\end{itemize}
		\textbf{ENDWHILE}
		\vskip 0.4cm
		\textbf{OUTPUT}
		\begin{itemize}
		\item
			Return \,$\beta^{(k)} \in \Re^{p}$\, as approximate solution to the score equation
			in \eqref{logisticScoreEqn}.
		\end{itemize}
	} %\textnormal
	\end{minipage}}
	\end{center}
\end{enumerate}
\end{proposition}
\proof
\begin{enumerate}
\item
	First, note that
	\begin{equation*}
	\dfrac{\pi_{i}}{1 - \pi_{i}}
	\;\; = \;\;
		\dfrac{\exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}{1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}
		\cdot
		\dfrac{1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}{1}
	\;\; = \;\;
		\exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)
	\end{equation*}
	Hence,
	\begin{eqnarray*}
	&&
		P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\prod}}\;\, \pi_{i}^{y_{i}} \cdot \left(1 - \pi_{i}\right)^{1-y_{i}}
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\prod}}\; \left(\dfrac{\pi_{i}}{1-\pi_{i}}\right)^{y_{i}} \cdot \left(1 - \pi_{i}\right)
	\\
	&=&
		\overset{n}{\underset{i=1}{\prod}}\;
			\left(\, \exp\!\left(\beta^{T}\cdot x^{(i)}\right) \,\right)^{y_{i}}
			\cdot
			\left(\, \dfrac{1}{1+\exp(\,\beta^{T}\cdot x^{(i)}\,)} \,\right)
	\;\; = \;\;
		\dfrac{
			\overset{n}{\underset{i=1}{\prod}}\;\exp\!\left(\,\beta^{T}\cdot y_{i}\,x^{(i)}\,\right)
		}{
			\overset{n}{\underset{i=1}{\prod}}\;\left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
		}
	\\
	&=&
		\dfrac{
			\exp\!\left(\; \overset{n}{\underset{i=1}{\sum}}\;\beta^{T}\cdot y_{i}\,x^{(i)} \;\right)
		}{
			\overset{n}{\underset{i=1}{\prod}}\,\left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
		}
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\; \beta^{T} \cdot \overset{n}{\underset{i=1}{\sum}}\;y_{i}\,x^{(i)} \,\right)
		}{
			\overset{n}{\underset{i=1}{\prod}}\,\left(\,\overset{{\color{white}.}}{1} + \exp(\,\beta^{T}\cdot x^{(i)}\,) \,\right)
		}
	\end{eqnarray*}
\item
	\begin{eqnarray*}
	\log L
	&=&
		\log\,P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\\
	&=&
		\log\!\left(\;\,
			\overset{n}{\underset{i=1}{\prod}}\;\, \pi_{i}^{y_{i}} \cdot \left(1 - \pi_{i}\right)^{1-y_{i}}
			\,\right)
	\;\;=\;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\log\,\pi_{i}
			\, \overset{{\color{white}\vert}}{+} \,
			(1-y_{i})\cdot\log\!\left(1 - \pi_{i}\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\log\left(\,\dfrac{\pi_{i}}{1 - \pi_{i}}\,\right)
			\, \overset{{\color{white}\vert}}{+} \,
			\log\!\left(1 - \pi_{i}\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\left(\,\beta^{T} \cdot x^{(i)}\,\right)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)\,\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\left(\,\overset{p}{\underset{l=1}{\sum}}\;\,\beta_{l}\,x^{(i)}_{l}\,\right)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp\!\left(\,\overset{p}{\underset{l=1}{\sum}}\;\,\beta_{l}\,x^{(i)}_{l}\,\right)\,\right)
			\;\right\}	
	\\
	&=&
		\overset{p}{\underset{l=1}{\sum}}\;\,
		\beta_{l}\,
		\cdot
		\left(\; \overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{l} \cdot y_{i} \right)
	\; \overset{{\color{white}\vert}}{-} \;\,
		\overset{n}{\underset{i=1}{\sum}}\;
		\log\!\left(\,1 + \exp\!\left(\,\overset{p}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)\,\right)
	\end{eqnarray*}
\item
	The partial derivative of $\log\,L$ with respect to $\beta_{k}$ is:
	\begin{eqnarray*}
	\dfrac{\partial\,\log L}{\partial\,\beta_{k}}
	&=&
		\left(\; \overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i} \right)
		\; - \;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			\dfrac{
				\exp\!\left(\,\overset{p}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}{
				1 + \exp\!\left(\,\overset{p}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}
			\cdot
			x^{(i)}_{k}
			\;\right\}	
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			x^{(i)}_{k} \cdot y_{i}
			\; \overset{{\color{white}\vert}}{-} \;
			x^{(i)}_{k} \cdot \pi_{i}(\beta)
			\;\right\}	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\;\;
		x^{(i)}_{k}	
		\cdot
		\left(\; y_{i} \; \overset{{\color{white}.}}{-} \; \pi_{i}(\beta) \,\right)
	\end{eqnarray*}
	Equivalently, in matrix form, we have
	\begin{equation*}
	\nabla\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)(\beta)
	\;\; = \;\;
		X^{T} \cdot \left(\, y \,\overset{{\color{white}.}}{-}\, \pi(\beta) \right).
	\end{equation*}
	Hence the score equation
	\,$\nabla\!\left(\,\log L\,\right) \,=\, 0$\,
	in explicit terms can be given by:
	\begin{equation*}
	X^{T} \cdot \left(\, y \,\overset{{\color{white}.}}{-}\, \pi(\beta) \right) \;\; = \;\; 0_{p}\,,
	\end{equation*}
	as desired.
	Incidentally, the score equations in component form are
	\begin{eqnarray*}
	\dfrac{\partial\,\log L}{\partial\,\beta_{k}} \;=\; 0
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\;
		x^{(i)}_{k} 
		\cdot
		\left(\; y_{i} \; \overset{{\color{white}.}}{-} \; \pi_{i}(\beta) \,\right)
		\;\; = \;\; 0
	\\
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\;
		x^{(i)}_{k} 
		\cdot
		\left(\;
			y_{i}
			\;\overset{{\color{white}.}}{-}\;
			\dfrac{
				\exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}{
				1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}
			\,\right)
		\;\; = \;\; 0
	\\
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\;
		x^{(i)}_{k} 
		\cdot
		\left(\;
			y_{i}
			\;\overset{{\color{white}.}}{-}\;
			\dfrac{1}{1 \,+\, \exp\!\left({\color{red}-}\;\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)}
			\,\right)
		\;\; = \;\; 0
	\end{eqnarray*}
	for each $k = 1,2,\ldots,p$.
\item
	Simply note that
	\begin{eqnarray*}
	\dfrac{\partial\,\pi_{i}(\beta)}{\partial\,\beta_{k}}
	&=&
		\left[\,1+\exp(\,-\,\beta^{T}\cdot x^{(i)})\,\right]^{-2}
		\cdot
		\exp(\,-\,\beta^{T}\cdot x^{(i)})
		\cdot
		x^{(i)}_{k}
	\\
	&=&
		\dfrac{1}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}
		\cdot
		\dfrac{\exp(\,-\,\beta^{T}\cdot x^{(i)})}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}
		\cdot
		x^{(i)}_{k}
	\\
	&=&
		\dfrac{1}{1+\exp(\,-\,\beta^{T}\cdot x^{(i)})}
		\cdot
		\dfrac{1}{1+\exp(\beta^{T}\cdot x^{(i)})}
		\cdot
		x^{(i)}_{k}
	\\
	&=&
		\pi_{i}(\beta)
		\cdot
		\left(1\overset{{\color{white}.}}{-}\pi_{i}(\beta)\right)
		\cdot
		x^{(i)}_{k}\,,
	\end{eqnarray*}
	which is equivalent to:
	\begin{equation*}
	\left(\,\nabla\pi\,\right)(\beta)
	\;\; = \;\;
		D(\beta) \cdot X\,,
	\end{equation*}
	where
	\begin{equation*}
	D(\beta) \; := \; \diag\!\left(\;
		\overset{{\color{white}-}}{\pi}_{1}(\beta) \cdot (\,1-\pi_{1}(\beta)\,)\,,
		\,\ldots\,,
		\pi_{n}(\beta) \cdot (\,1-\pi_{n}(\beta)\,)
		\;\right)
		\;\in\;
		\Re^{n \times n}\,.
	\end{equation*}
\item
	By \eqref{DerivativePi}, we have
	\begin{equation*}
	\nabla^{2}\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)(\beta)
	\;\; = \;\;
		\nabla_{\beta}\!\left(\;\overset{{\color{white}.}}{\nabla}_{\beta}\!\left(\,\log L\,\right)\;\right)
	\;\; = \;\;
		\nabla_{\beta}\!\left(\;X^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \pi(\beta)\right) \,\right)
	\;\; = \;\;
		-\, X^{T} \cdot \left(\,\nabla\pi\,\right)(\beta)
	\;\; = \;\;
		-\, X^{T} \cdot D(\beta) \cdot X\,.
	\end{equation*}
\item
	By \eqref{logisticScoreEqn}, the score equation is
	\,$g(\,\beta\,;\,X,y\,) = 0$,\,
	where $\beta$ is the unknown to be solved for, and
	\,$g : \Re^{p} \longrightarrow \Re^{p}$\,
	is simply the gradient
	\,$\nabla\!\left(\,\log L\,\right)$\,
	of the log-likelihood
	\,$\log L$.
	In other words,
	\begin{equation*}
	g\!\left(\,\beta\,;\,X,y\,\right)
	\;\; := \;\;
		\nabla\!\left(\,\log L\,\right)(\beta)
	\;\; = \;\;
		X^{T} \cdot \left(\,y \,\overset{{\color{white}.}}{-}\, \pi(\beta)\,\right).
	\end{equation*}
	Now, recall (see Theorem \eqref{NewtonRaphson}) that,
	in order to produce an approximate solution to
	\,$g(\beta\,;\,X,y) = 0$,\,
	the Newton-Raphson iterations proceed as follows:
	Choose $\beta^{(1)} \in \Re^{p}$ arbitrarily.
	Then, for each $m \in \N$, $\beta^{(m+1)} \in \Re^{p}$
	is defined to be the solution of the following vectorial linear equation:
	\begin{equation}\label{nablaGBetaMPlusOne}
	\left[\,(\,\overset{{\color{white}\vert}}{\nabla}_{\beta}\,g\,)(\,\beta^{(m)}\,;\,X,y\,)\,\right] \cdot {\color{red}\beta^{(m+1)}}
	\;\; = \;\;
		\left[\,(\,\overset{{\color{white}\vert}}{\nabla}_{\beta}\,g\,)(\,\beta^{(m)}\,;\,X,y\,)\,\right] \cdot \beta^{(m)}
		\; - \;
		g(\,\beta^{(m)}\,;\,X,y\,)\,.
	\end{equation}
	Hence, in order to complete the proof of the present Proposition,
	it remains only to derive an explicit expression for
	\,$(\,\overset{{\color{white}.}}{\nabla}_{\beta}\,g\,)(\,\beta\,;\,X,y\,)$\,.
	But, by \eqref{HessianLogLikelihood}, we have
	\begin{equation*}
	(\,\nabla g\,)(\beta)
	\;\; = \;\;
		\nabla\!\left(\;\overset{{\color{white}.}}{\nabla}\log L\;\right)(\beta)
	\;\; = \;\;
		\nabla^{2}\!\left(\,\overset{{\color{white}.}}{\log}\,L\,\right)(\beta)
	\;\; = \;\;
		-\, X^{T} \cdot D(\beta) \cdot X\,.
	\end{equation*}
	Substituting the above into \eqref{nablaGBetaMPlusOne} yields:
	\begin{equation*}
	\left[\; -\, X^{T} \cdot \overset{{\color{white}-}}{D}(\beta) \cdot X \;\right]
	\cdot
	\beta^{(m+1)}
	\;\; = \;\;
		\left[\; -\, X^{T} \cdot \overset{{\color{white}-}}{D}(\beta) \cdot X \;\right]
		\cdot
		\beta^{(m)}
		\; - \;
		g(\,\beta^{(m)}\,;\,X,y\,)\,,
	\end{equation*}
	which simplifies to
	\begin{equation*}
	X^{T} \cdot \overset{{\color{white}-}}{D}(\beta) \cdot X
	\cdot
	\beta^{(m+1)}
	\;\; = \;\;
		X^{T} \cdot \overset{{\color{white}-}}{D}(\beta) \cdot X
		\cdot
		\beta^{(m)}
		\; + \;
		g(\,\beta^{(m)}\,;\,X,y\,)\,,
	\end{equation*}
	as desired.
\end{enumerate}
This completes the proof of the present Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
