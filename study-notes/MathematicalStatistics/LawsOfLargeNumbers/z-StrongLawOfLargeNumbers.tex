
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{The Strong Law of Large Numbers}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\cite{vanDerVaart1996}
%\cite{Kosorok2008}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{theorem}[The Strong Law of Large Numbers (Theorem 8.3.5, p.263, \cite{Dudley2002})]
\mbox{}\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
\item
	$X_{1}, X_{2}, \ldots : (\Omega,\mathcal{A},\mu) \longrightarrow \Re$ are
	independent and identically distributed random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\end{itemize}
Then,
\begin{enumerate}
\item\label{FiniteExpectiationImpliesStrongLaw}
	\begin{equation*}
	E\!\left[\,\vert\,\overset{{\color{white}.}}{X}_{1}\,\vert\,\right] \;<\; +\infty
	\quad\quad\Longrightarrow\quad\quad
	P\!\left(\;\,
		\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}
		\;\;\textnormal{exists and equals}\;\;
		E[\,X_{1}\,]
		\;\right)
	\;\; = \;\; 1
	\end{equation*}
\item
	\begin{equation*}
	E\!\left[\,\vert\,\overset{{\color{white}.}}{X}_{1}\,\vert\,\right] \;=\; +\infty
	\quad\quad\Longrightarrow\quad\quad
	P\!\left(\;
		{\color{white}1...}
		\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}
		\;\;\textnormal{does not exist in}\;\; \Re
		{\color{white}1...}
		\,\right)
	\;\; = \;\; 1
	\end{equation*}
\end{enumerate}
\end{theorem}
\proof

\begin{enumerate}
\item

\noindent
\textbf{Claim 0:}\quad
If \eqref{FiniteExpectiationImpliesStrongLaw} holds for non-negative random variables,
then it also holds for general (i.e. not necessarily non-negative) random variables.
\vskip 0.3cm
\noindent
Proof of Claim 0:\quad
Indeed, if \eqref{FiniteExpectiationImpliesStrongLaw} holds for non-negative random variables, then
\begin{equation*}
P\!\left(\;\,
	\underset{n\rightarrow\infty}{\lim}\;
	\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}^{+} \;=\; E[\,X_{1}^{+}\,]
	\,\right)
\;\; = \;\;
	P\!\left(\;\,
		\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}^{-} \;=\; E[\,X_{1}^{-}\,]
		\,\right)
\;\; = \;\; 1\,,
\end{equation*}
whenever \,$E[\,X_{1}^{+}\,] \, < \infty$\, and \,$E[\,X_{1}^{-}\,] \, < \infty$\,.
But, \,$E[\,X_{1}\,] \,<\, \infty$\, $\Longleftrightarrow$ \,$E[\,X_{1}^{+}\,] \, < \infty$\, and \,$E[\,X_{1}^{-}\,] \, < \infty$\,,
and in that case, we have
\begin{eqnarray*}
&&
	\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}
		\;\;\textnormal{exists and equals}\;\;
		E[\,X_{1}\,] \,=\, E[\,X_{1}^{+}\,] \, - \, E[\,X_{1}^{-}\,]
\\
& \Longleftrightarrow &
	\left(\;
		\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}^{+}
		\;\;\textnormal{exists and equals}\;\;
		E[\,X_{1}^{+}\,]
		\;\right)
	\quad\textnormal{and}\quad
	\left(\;
		\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}^{-}
		\;\;\textnormal{exists and equals}\;\;
		E[\,X_{1}^{-}\,]
		\;\right)
\end{eqnarray*}
Hence,
\begin{eqnarray*}
&&
	P\!\left(\;\,
		\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}
		\;\;\textnormal{exists and equals}\;\;
		E[\,X_{1}\,]
		\;\right)
\\
& = &
	P\!\left(\;\,
		\left\{\;
			\underset{n\rightarrow\infty}{\lim}\;
			\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}^{+}
			\;\;\textnormal{exists and equals}\;\;
			E[\,X_{1}^{+}\,]
			\;\right\}
	\;\,\bigcap\;
		\left\{\;
			\underset{n\rightarrow\infty}{\lim}\;
			\dfrac{1}{n}\;\overset{n}{\underset{i=1}{\sum}}\;X_{i}^{-}
			\;\;\textnormal{exists and equals}\;\;
			E[\,X_{1}^{-}\,]
			\;\right\}
		\;\right)
\\
& = &
	1\,,
\end{eqnarray*}
since the intersection of two events each having probability one itself has probability one.
This proves Claim 0.
%Thus, in order to prove \eqref{FiniteExpectiationImpliesStrongLaw},
%we may assume, without loss of generality, in the remainder of the proof that \,$X_{1}$\, is non-negative.

\vskip 0.8cm
\noindent
We now proceed to prove \eqref{FiniteExpectiationImpliesStrongLaw}
under the assumption that \,$X_{1}$\, is non-negative.
For each $i \in \N$, define $Y_{i} \,:=\, X_{i} \cdot 1_{\{X_{i} \leq i\,\}}$.
For each $n \in \N$, define
\begin{equation*}
S_{n} \;\; := \;\; \overset{n}{\underset{i=1}{\sum}} \;\, X_{i}
\quad\;\;\textnormal{and}\quad\;\;
T_{n} \;\; := \;\; \overset{n}{\underset{i=1}{\sum}} \;\, Y_{i}
\end{equation*}

Next, for each $\alpha > 1$ and each $n \in \N$, define
\begin{equation*}
k_{\alpha}(n)
\;\; := \;\;
	\lfloor\, \alpha^{n} \,\rfloor\,,
\end{equation*}
where $\lfloor\,x\,\rfloor$ is the round-down of $x \in \Re$; in other words,
$\lfloor\,x\,\rfloor$ \;$:=$\; the greatest integer $\leq$ $x$, for $x \in \Re$.

\vskip 0.5cm
\noindent
\textbf{Claim 1:}\quad\quad
$P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
	\;=\;0
	\;\right)
\;=\;1$\,, for each $\alpha > 1$.
\vskip 0.3cm
\noindent
Proof of Claim 1:\quad
First, recall that, for each fixed $\alpha > 1$,
\begin{eqnarray*}
&&
	P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
	\;=\;0
	\;\right)
	\;=\;1
\\
&\Longleftrightarrow&
	\textnormal{for each \,$\varepsilon > 0$}\,,\;
	P\!\left(\;\,
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
		\;>\;\varepsilon\,,\;\textnormal{for infinitely many $n$}
		\;\right)
	\;=\;0
%\\
%&\Longleftrightarrow&
%	\textnormal{for each \,$\varepsilon > 0$}\,,\;
%	P\!\left(\;\,
%		\left\vert\;
%		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
%		\;\right\vert
%		\;<\;\varepsilon\,,\;\textnormal{for all but finitely many $n$}
%		\;\right)
%	\;=\;1
\end{eqnarray*}
On the other hand, for each $\alpha > 1$ and $\varepsilon > 0$, we have by
the Borel-Cantelli Lemma (Theorem \ref{theorem:BorelCantelli}(i)):
\begin{eqnarray*}
&&
	\overset{\infty}{\underset{n=1}{\sum}}\;
		P\!\left(\;\,
			\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
			\;>\;\varepsilon
			\;\right)
	\;<\; \infty
\\
&\Longrightarrow&
	P\!\left(\;\,
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
		\;>\;\varepsilon\,,\;\textnormal{for infinitely many $n$}
		\;\right)
	\;=\;0
\end{eqnarray*}
Thus, to prove Claim 1, it suffices to establish that
\begin{equation*}
\overset{\infty}{\underset{n=1}{\sum}}\;
	P\!\left(\;\,
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
		\;>\;\varepsilon
		\;\right)
\;<\; \infty\,,
\quad\textnormal{for each \,$\alpha > 1$\, and \,$\varepsilon > 0$}
\end{equation*}
To this end, let $\alpha > 1$ and $\varepsilon > 0$. Then,
\begin{eqnarray*}
\Sigma(\alpha,\varepsilon)
& := &
	\overset{\infty}{\underset{n=1}{\sum}}\;
	P\!\left(\;\,
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
		\;>\;\varepsilon
		\;\right)
	\;\; = \;\;
	\overset{\infty}{\underset{n=1}{\sum}}\;
	P\!\left(\;\,
		\left\vert\;T_{k_{\alpha}(n)} \overset{{\color{white}.}}{-} E[\,T_{k_{\alpha}(n)}\,]\;\right\vert
		\;>\;\varepsilon \cdot k_{\alpha}(n)
		\;\right)
\\
& \leq &
	\overset{\infty}{\underset{n=1}{\sum}}\;
	\dfrac{
		{\color{white}.}E\!\left(\,\left\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\right\vert^{2}\,\right){\color{white}.}
		}{
		\varepsilon^{2} \cdot k_{\alpha}(n)^{2}
		}\,,
	\quad\textnormal{by Chebyshev's Inequality (Theorem 8.3.1, p.261, \cite{Dudley2002})}
\\
& = &
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{n=1}{\sum}}\;
	\dfrac{
		{\color{white}.}\Var\!\left(\;T_{k_{\alpha}(n)}\,\right){\color{white}.}
		}{
		k_{\alpha}(n)^{2}
		}
	\;\; = \;\;
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{n=1}{\sum}}\;\,
	\dfrac{1}{k_{\alpha}(n)^{2}} \cdot
	\left(\;\overset{k_{\alpha}(n)}{\underset{i=1}{\sum}}\;\Var\!\left(\;Y_{i}\,\right)\,\right)
\\
& = &
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{n=1}{\sum}}\;\;
	\overset{k_{\alpha}(n)}{\underset{i=1}{\sum}}\;
		\dfrac{1}{k_{\alpha}(n)^{2}} \cdot \Var\!\left(\;Y_{i}\,\right)
	\;\; = \;\;
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\underset{k_{\alpha}(n)\,\geq\,i}{\sum}\;
		\dfrac{1}{k_{\alpha}(n)^{2}} \cdot \Var\!\left(\;Y_{i}\,\right)
\\
& = &
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\Var\!\left(\;Y_{i}\,\right)
		\left(\,
			\underset{k_{\alpha}(n)\,\geq\,i}{\sum}\;
			\dfrac{1}{k_{\alpha}(n)^{2}}
			\right)
\end{eqnarray*}
Now, note that
\begin{equation*}
1 \;\;\leq\;\; k_{\alpha}(n) \;\;\leq\;\; \alpha^{n} \;\;\leq\;\; k_{\alpha}(n) + 1
\;\;\leq\;\; k_{\alpha}(n) + k_{\alpha}(n) \;\;=\;\; 2\,k_{\alpha(n)}\,,
\quad
\textnormal{for each $n \in \N$}\,,
\end{equation*}
which implies that
\begin{equation*}
\dfrac{1}{{\color{white}.}2\,k_{\alpha}(n){\color{white}.}} \;\;\leq\;\; \dfrac{1}{\alpha^{n}}
\quad
\textnormal{for each $n \in \N$}\,,
\end{equation*}
which in turn is equivalent to
\begin{equation*}
\dfrac{1}{{\color{white}.}k_{\alpha}(n)^{2}{\color{white}.}} \;\;\leq\;\; \dfrac{4}{\alpha^{2n}}
\quad
\textnormal{for each $n \in \N$}\,,
\end{equation*}
Hence, we have
\begin{eqnarray*}
\Sigma(\alpha,\varepsilon)
& \leq &
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\Var\!\left(\;Y_{i}\,\right)
		\cdot
		\left(\,
			\underset{k_{\alpha}(n)\,\geq\,i}{\sum}\;
			\dfrac{1}{k_{\alpha}(n)^{2}}
			\right)
	\;\; \leq \;\;
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\Var\!\left(\;Y_{i}\,\right)
		\cdot
		\left(\,
			\underset{\alpha^{n}\,\geq\,i}{\sum}\;
			\dfrac{4}{(\alpha^{2})^{n}}
			\right)
\\
& = &
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\Var\!\left(\;Y_{i}\,\right)
		\cdot
		\left(\,
			\underset{\alpha^{-n}\,\leq\,i^{-1}}{\sum}\;
			\dfrac{4}{(\alpha^{2})^{n}}
			\right)
	\;\; \leq \;\;
	\dfrac{1}{\varepsilon^{2}} \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\Var\!\left(\;Y_{i}\,\right)
		\cdot
		\left(\,
			\dfrac{4\,/\,i^{2}}{1-\alpha^{-2}}
			\right)
\\
& = &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\,
	\dfrac{\Var\!\left(\;Y_{i}\,\right)}{i^{2}}
	\;\; \leq \;\;
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\,
	\dfrac{E\!\left(\;Y_{i}^{2}\,\right)}{i^{2}}
\\
& = &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\dfrac{1}{i^{2}} \cdot \int_{0}^{i}\, x^{2} \,\d F(x)
	\;\; = \;\;
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\dfrac{1}{i^{2}} \cdot 
	\left(\;
		\underset{0 \leq k < i}{\sum}\;\int_{k}^{k+1}\, x^{2} \,\d F(x)
		\;\right)
\\
& = &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{i=1}{\sum}}\;\;
	\underset{0 \leq k < i}{\sum}\;\;
	\dfrac{1}{i^{2}} \cdot  \int_{k}^{k+1}\, x^{2} \,\d F(x)
	\;\; = \;\;
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{k=0}{\sum}}\;\;
	\underset{i > k}{\sum}\;\;
	\dfrac{1}{i^{2}} \cdot  \int_{k}^{k+1}\, x^{2} \,\d F(x)
\\
& = &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{k=0}{\sum}}\;
	\left(\,\underset{i > k}{\sum}\;\, \dfrac{1}{i^{2}}\,\right) \cdot
	\int_{k}^{k+1}\, x^{2} \,\d F(x)
\end{eqnarray*}
Now, note that, for $k \geq 1$,
\begin{equation*}
\underset{i > k}{\sum}\;\, \dfrac{1}{i^{2}}
\;\; = \;
	\overset{\infty}{\underset{i = k+1}{\sum}}\;\, \dfrac{1}{i^{2}}
\;\; \leq \;\;
	\int_{k}^{\infty}x^{-2}\,\d x
\;\; = \;\;
	\left. -\,\dfrac{1}{x}\;\right\vert_{k}^{\infty}
\;\; = \;\;
	\dfrac{1}{k}
\;\; \leq \;\;
	\dfrac{2}{k+1}\,,
\end{equation*}
while for $k = 0$,
\begin{equation*}
\underset{i > k}{\sum}\;\, \dfrac{1}{i^{2}}
\;\; = \;
	\overset{\infty}{\underset{i = 1}{\sum}}\;\, \dfrac{1}{i^{2}}
\;\; = \;
	1 + \overset{\infty}{\underset{i = 2}{\sum}}\;\, \dfrac{1}{i^{2}}
\;\; = \;
	1 + \overset{\infty}{\underset{i = {\color{red}1}+1}{\sum}}\;\, \dfrac{1}{i^{2}}
\;\; \leq \;\;
	1 + \dfrac{2}{{\color{red}1}+1}
\;\; = \;
	2
\;\; = \;
	\dfrac{2}{0 + 1}
\end{equation*}
Combining the two preceding inequalities, we have
\begin{equation*}
\underset{i > k}{\sum}\;\, \dfrac{1}{i^{2}}
\;\; \leq \;\;
	\dfrac{2}{k + 1}\,,
\quad\textnormal{for each \,$k = 0, 1, 2, \ldots$}
\end{equation*}
Therefore,
\begin{eqnarray*}
\Sigma(\alpha,\varepsilon)
& \leq &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{k=0}{\sum}}\;
	\left(\,\underset{i > k}{\sum}\;\, \dfrac{1}{i^{2}}\,\right) \cdot
	\int_{k}^{k+1}\, x^{2} \,\d F(x)
\\
& \leq &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{1}{1-\alpha^{-2}}\,\right) \cdot
		\overset{\infty}{\underset{k=0}{\sum}}\;\;
		\dfrac{2}{k+1} \cdot
		\int_{k}^{k+1}\, x^{2} \,\d F(x)
	\;\; = \;\;
		\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{2}{1-\alpha^{-2}}\,\right) \cdot
		\overset{\infty}{\underset{k=0}{\sum}}\;\;
		\int_{k}^{k+1}\, x \cdot \dfrac{x}{k+1} \,\d F(x)
\\
& \leq &
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{2}{1-\alpha^{-2}}\,\right) \cdot
	\overset{\infty}{\underset{k=0}{\sum}}\;\;
	\int_{k}^{k+1}\, x \cdot 1 \;\,\d F(x)
	\;\; = \;\;
	\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{2}{1-\alpha^{-2}}\,\right) \cdot
	\int_{0}^{\infty}\, x \;\,\d F(x)
\\
& = &
		\left(\,\dfrac{4}{\varepsilon^{2}} \cdot \dfrac{2}{1-\alpha^{-2}}\,\right) \cdot {\color{red}E\!\left[\,X_{1}\,\right]
	\;\; < \;\;
		\infty}
\end{eqnarray*}
This proves Claim 1.

\vskip 0.8cm
\noindent
\textbf{Claim 2:}\quad\quad
$P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \;=\; E[\,X_{1}\,]
	\;\right)
\;=\;1$\,, for each $\alpha > 1$.
\vskip 0.3cm
Proof of Claim 2:\quad
First, note that
\begin{equation*}
\underset{n\rightarrow\infty}{\lim}\; E\!\left[\,Y_{n}\,\right] \; = \; {\color{red}E\!\left[\,X_{1}\,\right] \; < \; \infty}\,,
\end{equation*}
which implies, by Lemma \ref{lemma:SequenceOfPartialMeans},
\begin{equation*}
\underset{n\rightarrow\infty}{\lim}\; \dfrac{{\color{white}.}E\!\left[\,T_{n}\,\right]{\color{white}.}}{n}
\; = \;
	\underset{n\rightarrow\infty}{\lim}\;\, \dfrac{1}{n} \cdot
		\overset{n}{\underset{i=1}{\sum}}\;E\!\left[\,Y_{i}\,\right]
\; = \;
	E\!\left[\,X_{1}\,\right]\,,
\end{equation*}
which in turn implies
\begin{equation*}
\underset{n\rightarrow\infty}{\lim}\; \dfrac{{\color{white}.}E\!\left[\,T_{k_{\alpha}(n)}\,\right]{\color{white}.}}{k_{\alpha}(n)}
\; = \;
	\underset{n\rightarrow\infty}{\lim}\;\, \dfrac{1}{k_{\alpha}(n)} \cdot
	\overset{k_{\alpha}(n)}{\underset{i=1}{\sum}}\;E\!\left[\,Y_{i}\,\right]
\; = \;
	E\!\left[\,X_{1}\,\right]\,,
\end{equation*}
since
\,$\left\{\,\overset{{\color{white}.}}{E}\!\left[\,T_{k_{\alpha}(n)}\,\right] /\,k_{\alpha}(n)\,\right\}_{n\in\N}$\,
is a subsequence of
\,$\left\{\,\overset{{\color{white}.}}{E}\!\left[\,T_{n}\,\right] /\, n \,\right\}_{n\in\N}$\,.
(Recall that every subsequence of a convergent sequence of real numbers itself converges to the same limit.)
Now, observe that
\begin{eqnarray*}
\left\vert\;\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \,-\, E[\,X_{1}\,]\;\right\vert
& = &
	\left\vert\;
		\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		\dfrac{{\color{white}.}E\!\left[\,T_{k_{\alpha}(n)}\,\right]{\color{white}.}}{k_{\alpha}(n)}
		\,+\,
		\dfrac{{\color{white}.}E\!\left[\,T_{k_{\alpha}(n)}\,\right]{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		E[\,X_{1}\,]
		\;\right\vert
\\
& \leq &
	\left\vert\;
		\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		\dfrac{{\color{white}.}E\!\left[\,T_{k_{\alpha}(n)}\,\right]{\color{white}.}}{k_{\alpha}(n)}
		\;\right\vert
	\;+\;
	\left\vert\;
		\dfrac{{\color{white}.}E\!\left[\,T_{k_{\alpha}(n)}\,\right]{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		E[\,X_{1}\,]
		\;\right\vert
\\
& = &
	\dfrac{
		{\color{white}.}
		\left\vert\;
			T_{k_{\alpha}(n)} \,\overset{{\color{white}.}}{-}\, E\!\left[\,T_{k_{\alpha}(n)}\,\right]
			\;\right\vert
		{\color{white}.}
		}{
		k_{\alpha}(n)
		}
	\;+\;
	\left\vert\;
		\dfrac{{\color{white}.}E\!\left[\,T_{k_{\alpha}(n)}\,\right]{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		E[\,X_{1}\,]
		\;\right\vert\,,
\end{eqnarray*}
which implies that
\begin{equation*}
	\underset{n\rightarrow\infty}{\lim}\;
	\left\vert\;\dfrac{{\color{white}.}T_{k_{\alpha}(n)}(\omega){\color{white}.}}{k_{\alpha}(n)} \,-\, E[\,X_{1}\,]\;\right\vert
	\; = \; 0
\quad\Longleftrightarrow\quad
	\underset{n\rightarrow\infty}{\lim}\;
	\dfrac{
		{\color{white}.}
		\left\vert\;
			T_{k_{\alpha}(n)} \,\overset{{\color{white}.}}{-}\, E\!\left[\,T_{k_{\alpha}(n)}\,\right]
			\;\right\vert
		{\color{white}.}
		}{
		k_{\alpha}(n)
		}
	\; = \; 0\,,
\quad
\textnormal{for each \,$\omega \in \Omega$}\,.
\end{equation*}
It now follows immediately that
\begin{equation*}
P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \;=\; E[\,X_{1}\,]
	\;\right)
\;\; = \;\;
	P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\;
		\dfrac{{\color{white}.}\vert\;T_{k_{\alpha}(n)} - E[\,T_{k_{\alpha}(n)}\,]\;\vert{\color{white}.}}{k_{\alpha}(n)}
	\;=\;0
	\;\right)
\;\; = \;\;
	1\,,
\end{equation*}
where the last equality above follows by Claim 1.
This completes the proof of Claim 2.

\vskip 0.8cm
\noindent
\textbf{Claim 3:}\quad\quad
$P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}S_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \;=\; E[\,X_{1}\,]
	\;\right)
\;=\;1$\,, for each $\alpha > 1$.
\vskip 0.3cm
\noindent
Proof of Claim 3:\quad
First, observe that
\begin{eqnarray*}
\left\vert\;\dfrac{{\color{white}.}S_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \,-\, E[\,X_{1}\,]\;\right\vert
& = &
	\left\vert\;
		\dfrac{{\color{white}.}S_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
		\,+\,
		\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		E[\,X_{1}\,]
		\;\right\vert
\\
& \leq &
	\dfrac{
		{\color{white}.}
		\left\vert\;
			S_{k_{\alpha}(n)} \,\overset{{\color{white}.}}{-}\, T_{k_{\alpha}(n)}
			\;\right\vert
		{\color{white}.}
		}{
		k_{\alpha}(n)
		}
	\;+\;
	\left\vert\;
		\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
		\,-\,
		E[\,X_{1}\,]
		\;\right\vert\,,
\end{eqnarray*}
which implies that
\begin{eqnarray*}
&&
	P\!\left(\;
		\left\{\;
			\underset{n\rightarrow\infty}{\lim}\,
			\dfrac{{\color{white}.}
				\left\vert\;
					S_{k_{\alpha}(n)} \,\overset{{\color{white}.}}{-}\, T_{k_{\alpha}(n)}
					\;\right\vert
				{\color{white}.}}{
				k_{\alpha}(n)
				}
			\,=\, 0
			\;\right\}
	\;\bigcap\;
		\left\{\;
			\overset{{\color{white}\textnormal{\Large1}}}{\underset{n\rightarrow\infty}{\lim}}\,
			\left\vert\;
			\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
			\,-\,
			E[\,X_{1}\,]
			\;\right\vert
			\,=\, 0
			\;\right\}
	\;\right)
\\
& \leq &
	P\!\left(\;
		\underset{n\rightarrow\infty}{\lim}\,
		\left\vert\;\dfrac{{\color{white}.}S_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \,-\, E[\,X_{1}\,]\;\right\vert
		\,=\, 0
		\;\right)
\end{eqnarray*}
By Claim 2,
\begin{equation*}
P\!\left(\;
	\overset{{\color{white}\textnormal{\Large1}}}{\underset{n\rightarrow\infty}{\lim}}\,
	\left\vert\;
	\dfrac{{\color{white}.}T_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)}
	\,-\,
	E[\,X_{1}\,]
	\;\right\vert
	\,=\, 0
	\;\right)
\;\; = \;\; 1\,,
\end{equation*}
and, recalling that the intersection of two events each having probability one itself has probability one,
we see that Claim 3 follows once we establish
\begin{equation*}
P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}
		\left\vert\;
		S_{k_{\alpha}(n)} \,\overset{{\color{white}.}}{-}\, T_{k_{\alpha}(n)}
		\;\right\vert
		{\color{white}.}}{
		k_{\alpha}(n)
		}
		\,=\, 0
	\;\right)
\;\; = \;\; 1
\end{equation*}
To this end, we shall prove the following stronger statement:
\begin{equation*}
P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}
		\left\vert\;
		S_{n} \,\overset{{\color{white}.}}{-}\, T_{n}
		\;\right\vert
		{\color{white}.}}{
		n
		}
		\,=\, 0
	\;\right)
\;\; = \;\; 1
\end{equation*}
Now,
\begin{equation*}
\overset{\infty}{\underset{n=1}{\sum}}\;P\!\left(\,X_{n} \,\neq\, Y_{n}\,\right)
\;\; = \;\;
	\overset{\infty}{\underset{n=1}{\sum}}\;P\!\left(\,X_{n} \,>\, n\,\right) \cdot 1
\;\; \leq \;\;
	\int_{0}^{\infty}P(X_{1} > t)\,\d\,t
\;\; = \;\;
	{\color{red}E\!\left[\,X_{1}\,\right]
\;\; < \;\;
	\infty}\,,
\end{equation*}
where the last equality follows by Lemma \ref{LemmaMomentsAndTails}.
Thus, by the Borel-Cantelli Lemma (Theorem \ref{theorem:BorelCantelli}(i)) again, we have
\begin{equation*}
P\!\left(\,\overset{{\color{white}.}}{X}_{n} \neq Y_{n}\,,\;\textnormal{for infinitely many \,$n\,\in\,\N$}\,\right)
\;\; = \;\; 0\,,
\end{equation*}
which is equivalent to
\begin{equation*}
P\!\left(\,\overset{{\color{white}.}}{X}_{n} \neq Y_{n}\,,\;\textnormal{for at most finitely many \,$n\,\in\,\N$}\,\right)
\;\; = \;\; 1\,,
\end{equation*}
which is in turn equivalent to
\begin{equation*}
P\!\left(\,\overset{{\color{white}.}}{X}_{n} = Y_{n}\,,\;\textnormal{for all but finitely many \,$n\,\in\,\N$}\,\right)
\;\; = \;\; 1\,.
\end{equation*}
Now, for each
\,$\omega \,\in\, \left\{\,\overset{{\color{white}.}}{X}_{n} = Y_{n}\,,\;\textnormal{for all but finitely many \,$n\,\in\,\N$}\,\right\}$\,, there exists \,$N(\omega) \in \N$\, such that \,$X_{n}(\omega) = Y_{n}(\omega)$\,,\, for each \,$n > N(\omega)$.
Hence, for each
\,$\omega \in \left\{\,\overset{{\color{white}.}}{X}_{n} = Y_{n}\,,\;\textnormal{for all but finitely many \,$n\,\in\,\N$}\,\right\}$\,
and \,$n \geq N(\omega)+1$\,,\, we have
\begin{eqnarray*}
\dfrac{{\color{white}.}
	\left\vert\;
	S_{n}(\omega) \,\overset{{\color{white}.}}{-}\, T_{n}(\omega)
	\;\right\vert
	{\color{white}.}}{
	n
	}
& = &
	\left\vert\;\,
		\dfrac{1}{n}\cdot
		\overset{n}{\underset{i=1}{\sum}}
		\left(\,X_{i}(\omega) \,\overset{{\color{white}.}}{-}\, Y_{i}(\omega)\,\right)
		\;\,\right\vert
\\
& = &
	\left\vert\;\,
		\dfrac{1}{n}\cdot
		\overset{N(\omega)}{\underset{i=1}{\sum}}
		\left(\,X_{i}(\omega) \,\overset{{\color{white}.}}{-}\, Y_{i}(\omega)\,\right)
		\,+\,
		\dfrac{1}{n}\cdot
		\overset{n}{\underset{i=N(\omega)+1}{\sum}}
		\left(\,X_{i}(\omega) \,\overset{{\color{white}.}}{-}\, Y_{i}(\omega)\,\right)
		\;\,\right\vert
\\
& = &
	\left\vert\;\,
		\dfrac{1}{n}\cdot
		\overset{N(\omega)}{\underset{i=1}{\sum}}
		\left(\,X_{i}(\omega) \,\overset{{\color{white}.}}{-}\, Y_{i}(\omega)\,\right)
		\;\,\right\vert
	\;\; \longrightarrow \;\; 0\,,
	\quad\textnormal{as \,$n \, \longrightarrow \, \infty$}\,.
\end{eqnarray*}
It follows that
\begin{equation*}
1
\;\; = \;\; 
	P\!\left(\,\overset{{\color{white}.}}{X}_{n} = Y_{n}\,,\;\textnormal{for all but finitely many \,$n\,\in\,\N$}\,\right)
\;\; \leq \;\;
	P\!\left(\;
		\underset{n\rightarrow\infty}{\lim}\,
		\dfrac{{\color{white}.}
			\left\vert\;
			S_{n} \,\overset{{\color{white}.}}{-}\, T_{n}
			\;\right\vert
			{\color{white}.}}{
			n
			}
			\,=\, 0
		\;\right)\,,
\end{equation*}
which immediately implies:
\begin{equation*}
P\!\left(\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}
		\left\vert\;
		S_{n} \,\overset{{\color{white}.}}{-}\, T_{n}
		\;\right\vert
		{\color{white}.}}{
		n
		}
		\,=\, 0
	\;\right)
\;\; = \;\;
	1\,,
\end{equation*}
as required.
This completes the proof of Claim 3.

\vskip 0.8cm
\noindent
\textbf{Claim 4:}\quad\quad
$P\!\left(\;\,
	\dfrac{1}{\alpha^{2}} \cdot E[\,X_{1}\,]
	\;\leq\;
		\underset{n\rightarrow\infty}{\liminf}\;
		\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\leq\;
		\underset{n\rightarrow\infty}{\limsup}\;
		\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\leq\;
		\alpha^{2} \cdot E[\,X_{1}\,]
	\;\right)
\;=\;1$\,, for each $\alpha > 1$.
\vskip 0.3cm
\noindent
Proof of Claim 4:\quad
First, observe that, for each $\alpha > 1$,
\begin{eqnarray*}
\dfrac{k_{\alpha}(n+1)}{k_{\alpha}(n)}
& = &
	\dfrac{\lfloor\, \alpha^{n+1} \,\rfloor}{\lfloor\, \alpha^{n} \,\rfloor}
\;\; = \;\;
	\dfrac{
		\alpha^{n+1} - \left(\alpha^{n+1} \overset{{\color{white}.}}{-} \lfloor\, \alpha^{n+1} \,\rfloor\right)
	}{
		\alpha^{n} - \left(\alpha^{n} \overset{{\color{white}.}}{-} \lfloor\, \alpha^{n} \,\rfloor\right)
	}
\;\; = \;\;
	\dfrac{
		\alpha^{n+1}/\,\alpha^{n} - \left(\alpha^{n+1} \overset{{\color{white}.}}{-} \lfloor\, \alpha^{n+1} \,\rfloor\right)/\,\alpha^{n}
	}{
		\alpha^{n}/\,\alpha^{n} - \left(\alpha^{n} \overset{{\color{white}.}}{-} \lfloor\, \alpha^{n} \,\rfloor\right)/\,\alpha^{n}
	}
\\
& = &
	\dfrac{
		\alpha - \left(\alpha^{n+1} \overset{{\color{white}.}}{-} \lfloor\, \alpha^{n+1} \,\rfloor\right)/\,\alpha^{n}
	}{
		1 - \left(\alpha^{n} \overset{{\color{white}.}}{-} \lfloor\, \alpha^{n} \,\rfloor\right)/\,\alpha^{n}
	}
	\;\; \longrightarrow \;\; \alpha\,,
	\quad
	\textnormal{as \,$n \,\longrightarrow\, \infty$}
\end{eqnarray*}
In particular, for each \,$\alpha > 1$\,, there exists \,$M(\alpha) \in \N$\, such that
\begin{equation*}
\dfrac{k_{\alpha}(n+1)}{k_{\alpha}(n)}
\;\; < \;\;
	\alpha^{2}\,,
\quad
\textnormal{for each \,$n > M(\alpha)$}
\end{equation*}
Hence, for each \,$\alpha > 1$\, and \,$m, n \in\N$\,
with \,$M(\alpha) < k_{\alpha}(n) < m \leq k_{\alpha}(n+1)$\,, we have
\begin{eqnarray*}
\dfrac{S_{k_{\alpha}(n)}}{k_{\alpha}(n)}
& = &
	\dfrac{k_{\alpha}(n+1)}{k_{\alpha}(n)} \cdot \dfrac{1}{k_{\alpha}(n+1)} \cdot S_{k_{\alpha}(n)}
	\;\; \leq \;\;
		\alpha^{2}\cdot\dfrac{1}{m} \cdot S_{m}
\\
& = &
	\alpha^{2}\cdot\dfrac{S_{m}}{m}
	\;\; \leq \;\;
		\alpha^{2}\cdot\dfrac{S_{m}}{k_{\alpha}(n)}
	\;\; = \;\;
		\alpha^{2} \cdot \dfrac{k_{\alpha}(n+1)}{k_{\alpha}(n)} \cdot \dfrac{1}{k_{\alpha}(n+1)} \cdot S_{m}
	\;\; \leq \;\;
		\alpha^{2} \cdot \alpha^{2} \cdot \dfrac{1}{k_{\alpha}(n+1)} \cdot S_{k_{\alpha}(n+1)}
\\
& \leq &
	\alpha^{4}\cdot\dfrac{S_{k_{\alpha}(n+1)}}{k_{\alpha}(n+1)}
\end{eqnarray*}
In summary, for each \,$\alpha > 1$\, and \,$m, n \in\N$\,
with \,$M(\alpha) < k_{\alpha}(n) < m \leq k_{\alpha}(n+1)$\,, we have
\begin{equation*}
\dfrac{S_{k_{\alpha}(n)}}{k_{\alpha}(n)}
\;\; \leq \;\;
	\alpha^{2}\cdot\dfrac{S_{m}}{m}
\;\; \leq \;\;
	\alpha^{4}\cdot\dfrac{S_{k_{\alpha}(n+1)}}{k_{\alpha}(n+1)}\,,
\end{equation*}
which implies
\begin{equation*}
\dfrac{1}{\alpha^{2}} \cdot \dfrac{S_{k_{\alpha}(n)}}{k_{\alpha}(n)}
\;\; \leq \;\;
	\dfrac{S_{m}}{m}
\;\; \leq \;\;
	\alpha^{2}\cdot\dfrac{S_{k_{\alpha}(n+1)}}{k_{\alpha}(n+1)}\,,
\quad
\textnormal{for each $\alpha > 1$ and $m, n \in \N$ with $M(\alpha) < k_{\alpha}(n) < m \leq k_{\alpha}(n+1)$}\,.
\end{equation*}
Thus, for each \,$\alpha > 1$\, and 
\,$\omega \in \left\{\;
	\underset{n\rightarrow\infty}{\lim}\,
	\dfrac{{\color{white}.}S_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \;=\; E[\,X_{1}\,]
	\;\right\}$\,,
we have
\begin{eqnarray*}
\dfrac{1}{\alpha^{2}} \cdot E\!\left[\,X_{1}\,\right]
& = &
	\dfrac{1}{\alpha^{2}} \cdot \underset{m\rightarrow\infty}{\lim}\;\,\dfrac{S_{k_{\alpha}(n)}(\omega)}{k_{\alpha}(n)}
	\;\; = \;\;
		\dfrac{1}{\alpha^{2}} \cdot \underset{m\rightarrow\infty}{\liminf}\;\,\dfrac{S_{k_{\alpha}(n)}(\omega)}{k_{\alpha}(n)}
\\
& \leq &
	\underset{m\rightarrow\infty}{\liminf}\;\,\dfrac{S_{m}(\omega)}{m}
	\;\; \leq \;\;
		\underset{m\rightarrow\infty}{\limsup}\;\,\dfrac{S_{m}(\omega)}{m}
\\
& \leq &
	\alpha^{2} \cdot \underset{m\rightarrow\infty}{\limsup}\;\,\dfrac{S_{k_{\alpha}(n+1)}(\omega)}{k_{\alpha}(n+1)}
	\;\; = \;\;
		\alpha^{2} \cdot \underset{m\rightarrow\infty}{\lim}\;\,\dfrac{S_{k_{\alpha}(n+1)}(\omega)}{k_{\alpha}(n+1)}
	\;\; = \;\;
		\alpha^{2} \cdot E\!\left[\,X_{1}\,\right]
\end{eqnarray*}
Thus, by Claim 3, it follows that, for each $\alpha > 1$, we have:
\begin{equation*}
1
\;\; = \;\;
	P\!\left(\;
		\underset{n\rightarrow\infty}{\lim}\,
		\dfrac{{\color{white}.}S_{k_{\alpha}(n)}{\color{white}.}}{k_{\alpha}(n)} \;=\; E[\,X_{1}\,]
		\;\right)
\;\; \leq \;\;
	P\!\left(\;
		\dfrac{1}{\alpha^{2}} \cdot E\!\left[\,X_{1}\,\right]
		\; \leq \;
			\underset{m\rightarrow\infty}{\liminf}\;\,\dfrac{S_{m}}{m}
		\;\; \leq \;\;
			\underset{m\rightarrow\infty}{\limsup}\;\,\dfrac{S_{m}}{m}
		\; \leq \;
			\alpha^{2} \cdot E\!\left[\,X_{1}\,\right]
		\;\right),
\end{equation*}
from which Claim 4 immediately follows.

\vskip 0.8cm
\noindent
\textbf{Claim 5:}\quad\quad
$P\!\left(\;\,
	E[\,X_{1}\,]
	\;\leq\;
		\underset{n\rightarrow\infty}{\liminf}\;
		\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\leq\;
		\underset{n\rightarrow\infty}{\limsup}\;
		\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\leq\;
		E[\,X_{1}\,]
	\;\right)
\;=\;1$\,.
\vskip 0.3cm
\noindent
Proof of Claim 5:\quad
For each \,$m \in \N$\,,\, let \,$\alpha_{m} \,:=\, \left(1 + \dfrac{1}{m}\right)$\,.
Then, observe that
\begin{eqnarray*}
&&
	\left\{\;
		E[\,X_{1}\,]
		\;\leq\;
			\underset{n\rightarrow\infty}{\liminf}\;
			\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
		\;\leq\;
			\underset{n\rightarrow\infty}{\limsup}\;
			\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
		\;\leq\;
			E[\,X_{1}\,]
		\;\right\}
\\
& \textnormal{\Large$=$} &
	\overset{\infty}{\underset{m=1}{\bigcap}}\;
	\left\{\;
		\dfrac{1}{\alpha_{m}^{2}} \cdot E[\,X_{1}\,]
		\;\leq\;
			\underset{n\rightarrow\infty}{\liminf}\;
			\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
		\;\leq\;
			\underset{n\rightarrow\infty}{\limsup}\;
			\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
		\;\leq\;
			\alpha_{m}^{2} \cdot E[\,X_{1}\,]
		\;\right\}
\end{eqnarray*}
By Claim 4,
\begin{equation*}
P\!\left(\;
	\dfrac{1}{\alpha_{m}^{2}} \cdot E[\,X_{1}\,]
	\;\leq\;
		\underset{n\rightarrow\infty}{\liminf}\;
		\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\leq\;
		\underset{n\rightarrow\infty}{\limsup}\;
		\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\leq\;
		\alpha_{m}^{2} \cdot E[\,X_{1}\,]
	\;\right)
\;\; = \;\;
	1\,,
\quad
\textnormal{for each \,$m \in \N$}.
\end{equation*}
Now, recall that the intersection of a countable family of events each having probability one itself has probability one.
Thus, we may now conclude that
\begin{eqnarray*}
&&
	P\!\left(\;\,
		E[\,X_{1}\,]
		\;\leq\;
			\underset{n\rightarrow\infty}{\liminf}\;
			\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
		\;\leq\;
			\underset{n\rightarrow\infty}{\limsup}\;
			\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
		\;\leq\;
			E[\,X_{1}\,]
		\;\right)
\\
& = &
	P\!\left(\;
		\overset{\infty}{\underset{m=1}{\bigcap}}\;
		\left\{\;
			\dfrac{1}{\alpha_{m}^{2}} \cdot E[\,X_{1}\,]
			\;\leq\;
				\underset{n\rightarrow\infty}{\liminf}\;
				\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
			\;\leq\;
				\underset{n\rightarrow\infty}{\limsup}\;
				\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
			\;\leq\;
				\alpha_{m}^{2} \cdot E[\,X_{1}\,]
			\;\right\}
		\;\right)
\;\; = \;\;
	1
\end{eqnarray*}
This proves Claim 5.

\vskip 0.5cm
\noindent
It immediately follows from Claim 5 that
\eqref{FiniteExpectiationImpliesStrongLaw} is valid for non-negative random variables;
in other words,
\begin{equation*}
X_{1} \, \geq \, 0 \;\;\textnormal{and}\;\; E\!\left[\,X_{1}\,\right] \,<\, \infty
\quad\Longrightarrow\quad
P\!\left(\;\,
	\underset{n\rightarrow\infty}{\lim}\;
	\dfrac{{\color{white}.}S_{n}{\color{white}.}}{n}
	\;\; \textnormal{exists and equals} \;\;
		E[\,X_{1}\,]
	\;\right)
\;=\;1\,.
\end{equation*}
This full statement of \eqref{FiniteExpectiationImpliesStrongLaw}
(i.e. without the additional non-negativity assumption on $X_{1}$)
now follows at once by Claim 0.

This completes the proof of \eqref{FiniteExpectiationImpliesStrongLaw}.

\item

\end{enumerate}

\qed
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
