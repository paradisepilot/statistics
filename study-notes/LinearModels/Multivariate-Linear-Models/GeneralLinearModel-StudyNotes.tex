
\documentclass{article}

\usepackage{fancyheadings}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}
\usepackage{graphicx}
%\usepackage{doublespace}

\usepackage{KenChuArticleStyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\rank}{\textnormal{rank}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\setcounter{page}{1}

\pagestyle{fancy}

%\input{../CourseSemesterUnique}

\rhead[Study Notes]{Kenneth Chu}
\lhead[Kenneth Chu (300517641)]{Study Notes}
\chead[]{{\Large\bf The General Linear Statistical Model} \\
\vskip 0.1cm \normalsize \today}
\lfoot[]{}
\cfoot[]{}
\rfoot[]{\thepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
Let $Y : \Omega \longrightarrow \Re^{n}$ be an $\Re^{n}$-valued random variable defined on the probability space $\Omega$.  We assume that the expected value $E[\,Y\,]$ of $Y$ exists.  Then, trivially, we have $E[\,Y\,] \in \Re^{n}$.  

\section{Assumption on the expected value of the response variable $Y$}
\setcounter{theorem}{0}\setcounter{equation}{0}

The most fundamental assumption of the General Linear Model is that the expected value of the response variable $Y$ lies in a model-specific subspace of $\Re^{n}$ (this subspace will be called the \emph{estimation space} of the model), in the following sense:  One of the ``components'' of a general linear model is its \emph{model matrix} $X \in \Re^{n \times p}$, and the expected value of the response variable $Y$ is assumed to lie in the column space $\mathcal{C}(X) \subset \Re^{n}$.

\vskip 0.3cm
\noindent In other words:\vskip 0.1cm
\begin{center}
\begin{minipage}{16cm}
\noindent
\textbf{The Estimation Space Assumption}
\begin{equation}\label{StandardMultivariateLinearModel}
E[\,Y\,] \in \mathcal{C}(X);\;\;\textnormal{equivalently},\;\; E[\,Y\,] = X\beta,\; \textnormal{for some (unknown)}\; \beta \in \Re^{p},
\end{equation}
where $\mathcal{C}(X) \subset \Re^{n}$ is the column space of the model matrix $X \in \Re^{n \times p}$.
\end{minipage}
\end{center}

\vskip 0.25cm
\noindent
We will call $\Re^{n}$ the \emph{observation space}, and $\mathcal{C}(X)$ the \emph{estimation space} of the model.

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Assumption of the distribution of the response variable $Y$}
\setcounter{theorem}{0}\setcounter{equation}{0}

In order to make estimation and hypothesis testing computationally feasible, we need to make certain assumptions on the distribution of the response variable $Y$.

\begin{center}
\begin{minipage}{16cm}
\vskip 0.3cm
\noindent
\textbf{Assumptions on the distribution of $Y$:}
\begin{enumerate}
\item	The response variable $Y$ has a multivariate normal distribution.
\item	The components of $Y$ are independent $\Re$-valued random variables.
\item	The variances of the components of $Y$ are all equal.
\end{enumerate}
\end{minipage}
\end{center}

\vskip 0.3cm
\noindent
The assumptions on the expected value and distribution on $Y$ together are equivalent to the following:
\begin{equation}\label{ErrorNormalityIndependenceEqualVariance}
Y \sim N\!\left(X\beta,\sigma^{2}I_{n}\right),\;\textnormal{for some (unknown but fixed)}\;\beta\in\Re^{p},\;\textnormal{and some (unknown but fixed)}\;\sigma > 0.
\end{equation}

Define $\varepsilon := Y - X\beta$.  Then, $\varepsilon : \Omega \longrightarrow \Re^{n}$ is also an $\Re^{n}$-valued random variable, with
\begin{equation}\label{residualDistribution}
\varepsilon \sim N\!\left(0,\sigma^{2}I_{n}\right),\;\;\textnormal{for some}\;\;\sigma > 0.
\end{equation}

\begin{proposition}[Distribution of the full-model error sum-of-squares] \mbox{}
\vskip 0.1cm
\noindent
Let $P_{\mathcal{C}(X)^{\perp}} : \Re^{n} \longrightarrow \Re^{n}$ denote the orthogonal projection operator onto the subspace $\mathcal{C}(X)^{\perp}$.  Then,
\begin{equation*}
\dfrac{\Vert\,P_{\mathcal{C}(X)^{\perp}}(Y)\,\Vert^{2}}{\sigma^{2}} \; \sim \; \chi^{2}\!\left(\rank\!\left(\mathcal{C}(X)^{\perp}\right)\right)
\end{equation*}
\end{proposition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Testing the hypothesis that $H_{0}: E[\,Y\,] \in \mathcal{C}(X_{0}) \subset \mathcal{C}(X)$}
\setcounter{theorem}{0}

\begin{proposition}\mbox{}\vskip 0.1cm
\noindent
Let $P_{\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)} : \Re^{n} \longrightarrow \Re^{n}$ denote the orthogonal projection operator onto the subspace $\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)$.  Then,
\begin{equation*}
\dfrac{\Vert\,P_{\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)}(Y)\,\Vert^{2}}{\sigma^{2}} \; \sim \; \chi^{2}\!\left(\rank\!\left(\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)\right)\,,\,\dfrac{\Vert\,P_{\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)}X\beta\,\Vert^{2}}{2\,\sigma^{2}}\,\right)
\end{equation*}
\end{proposition}

\vskip 0.5cm
\begin{corollary}[Distribution of $F$-statistics under validity of full model]
\begin{equation*}
\dfrac
{\Vert\,P_{\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)}(Y)\,\Vert^{2}\,/\,\rank\!\left(\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)\right)}
{\Vert\,P_{\mathcal{C}(X)^{\perp}}(Y)\,\Vert^{2}\,/\,\rank\!\left(\mathcal{C}(X)^{\perp}\right)}
\;\sim\;
F\!\left(
\rank\!\left(\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)\right)
\,,\,
\rank\!\left(\mathcal{C}(X)^{\perp}\right)
\,;\,
\dfrac{\Vert\,P_{\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)}X\beta\,\Vert^{2}}{2\,\sigma^{2}}
\right)
\end{equation*}
\end{corollary}

\vskip 0.5cm
\begin{corollary}[Distribution of $F$-statistics under validity of reduced model]
\begin{equation*}
\dfrac
{\Vert\,P_{\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)}(Y)\,\Vert^{2}\,/\,\rank\!\left(\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)\right)}
{\Vert\,P_{\mathcal{C}(X)^{\perp}}(Y)\,\Vert^{2}\,/\,\rank\!\left(\mathcal{C}(X)^{\perp}\right)}
\;\sim\;
F\!\left(
\rank\!\left(\mathcal{C}(X_{0})^{\perp}\,\cap\,\mathcal{C}(X)\right)
\,,\,
\rank\!\left(\mathcal{C}(X)^{\perp}\right)
\,;\,
0
\right)
\end{equation*}
\end{corollary}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\section{Testing for the vanishing of linear parametric functions}
%\setcounter{theorem}{0}
%
%\begin{equation*}
%H_{0}: \Lambda'\,\beta = 0
%\end{equation*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Model adequacy checking}
\setcounter{theorem}{0}

Model adequacy checking is large done via examination of the residuals of the model fit.
Recall that the least-squares estimator $\widehat{Y}  : \Omega \longrightarrow \mathcal{C}(X)$
of the response variable $Y : \Omega \longrightarrow \Re^{n}$
is given by:
\begin{equation*}
\widehat{Y} \; = \; X\cdot\left(X^{t}\cdot X\right)^{-1}\cdot X^{t} \cdot Y \; = \; H \cdot Y\,,
\end{equation*}
where $H := X\cdot\left(X^{t}\cdot X\right)^{-1}\cdot X^{t}$ is called the \textbf{hat matrix}
of the model. Recall also that, geometrically speaking, the hat matrix $H$ is simply the orthogonal
projection operator, defined on $\Re^{n}$ (the observation space), onto the column space
$\mathcal{C}(X)$ of $X$ (the estimation space, or the model space).
The \textbf{residual} $\mathbf{e} : \Omega \longrightarrow \mathcal{C}(X)^{\perp}$ is defined to be:
\begin{equation*}
\mathbf{e}
\; := \; Y - \widehat{Y}
\;  = \; \left(I_{n} - H\right) \cdot Y\,,
\end{equation*}
where $I - H$ is the orthogonal projection operator defined on $\Re^{n}$ (the observation space)
onto the orthogonal complement $\mathcal{C}(X)^{\perp}$ of $\mathcal{C}(X)$.
Note that $\mathcal{C}(X)^{\perp}$ can be regarded as the \textbf{error space} of the model.
Recall that our model assumption is:
\begin{equation*}
Y \; = \; X\cdot\beta \, + \, \varepsilon\,,
\end{equation*}
with $\varepsilon \sim N\!\left(0,\sigma^{2}I_{n}\right)$; see \eqref{residualDistribution}.
Note that in general, the codomain of the error term $\varepsilon : \Omega \longrightarrow \Re^{n}$
is NOT $\mathcal{C}(X)^{\perp}$ but all of the observation space $\Re^{n}$.
On the other hand, observe that
\begin{equation*}
\mathbf{e}
\; = \; \left(I_{n} - H\right)\cdot Y 
\; = \; \left(I_{n} - H\right)\cdot \left(X\cdot\beta \, + \, \varepsilon\right)
\; = \; \left(I_{n} - H\right)\cdot \varepsilon\,,
\end{equation*}
since $I_{n} - H$ is the orthogonal projection operator onto $\mathcal{C}(X)^{\perp}$, which maps
$X\cdot\beta \in \mathcal{C}(X)$ to zero.
We thus see that the residual $\mathbf{e} : \Omega \longrightarrow \mathcal{C}(X)^{\perp}$ is the
orthogonal projection of the error term $\varepsilon : \Omega \longrightarrow \Re^{n}$ onto the
error space $\mathcal{C}(X)^{\perp}$.
Or, more strictly speaking, the residual $\mathbf{e} : \Omega \longrightarrow \mathcal{C}(X)^{\perp}$
is the composition
\begin{equation*}
\mathbf{e} :
\;\Omega
\;\overset{\varepsilon}{\longrightarrow} \; \Re^{n}
\;\overset{I_{n}-H}{\longrightarrow} \; \mathcal{C}(X)^{\perp}
\end{equation*}
Furthermore, note that
\begin{eqnarray*}
\textnormal{Var}\!\left(\mathbf{e}\right)
&=&\textnormal{Var}\!\left[\,\left(I_{n} - H\right)\cdot \varepsilon\,\right]
\;\;=\;\;\left(I_{n}-H\right)\cdot\textnormal{Var}\!\left[\,\varepsilon\,\right]\cdot \left(I_{n} - H\right)^{t}
\;\;=\;\;\left(I_{n}-H\right)\cdot\textnormal{Var}\!\left[\,\varepsilon\,\right]\cdot \left(I_{n} - H\right)\\
&=&\left(I_{n}-H\right)\cdot \sigma^{2}I_{n} \cdot \left(I_{n} - H\right)
\;\;=\;\;\sigma^{2}\cdot\left(I_{n}-H\right) \cdot \left(I_{n} - H\right) \\
&=&\sigma^{2}\cdot\left(I_{n}-H\right)\,,
\end{eqnarray*}
where the symmetry and idempotence of the orthogonal projection operator $I_{n} - H$ is used in the above
derivation.
The above observations lead to the following ``model adequacy checks'':
\begin{itemize}
\item 	Generate the scatter plot of the observed residuals $\mathbf{e}$ against the fitted values
		$\widehat{y}$.
		Examine this scatter plot for trends between the observed residuals and the fitted values;
		any trend between the observed residuals and the fitted values may indicate violations
		of model assumptions.

		This adequacy check is based on the following fact:
		\begin{equation*}
		\textnormal{Cov}\!\left(\,\widehat{Y}\,,\,\mathbf{e}\,\right)
		\; = \; \textnormal{Cov}\!\left(\,H\cdot Y\,,\,\left(I_{n}-H\right)\cdot Y\,\right)
		\; = \; H\cdot\textnormal{Cov}\!\left(\,Y\,,\,Y\,\right)\cdot \left(I_{n}-H\right)
		\; = \; H\cdot \sigma^{2} I_{n} \cdot \left(I_{n}-H\right)
		\; = \; 0_{n \times n}
		\end{equation*}

\item 	Generate the scatter plot of the observed residuals $\mathbf{e}$ against the observed values of
		each of the predictor variables (columns of the model matrix $X$). Any trends in any of these
		scatter plots may indicate violations of model assumptions.

\item 	Generate the QQ-plot of the \textbf{Studentized residuals} against the theoretical quantiles
		of the standard Gaussian distribution, where the Studentized residuals are defined as follows:
		\begin{equation*}
		r_{i} \; := \; \dfrac{e_{i}}{\sqrt{\textnormal{MS}_{\textnormal{error}}\left(1 - h_{ii}\right)}}
		\end{equation*}
		where $e_{i}$ is the $i^{\textnormal{th}}$ component of the observed residual $\mathbf{e}$,
		$h_{ii}$ is the $i^{\textnormal{th}}$ diagonal element of the hat matrix
		$H := X \cdot \left(X^{t}\cdot X\right)^{-1}\cdot X^{t}$, and
		$\textnormal{MS}_{\textnormal{error}}$ is the mean squared error of the model fit, which is
		defined as follows:
		\begin{equation*}
		\textnormal{MS}_{\textnormal{error}}
		\; := \;
		\dfrac{1}{n - p}\,\sum_{i=1}^{n}\left(y_{i} - \widehat{y}_{i}\right)^{2}
		\end{equation*}
		Large deviations of the data points on this QQ-plot from the $y = x$ line may indicate
		violations of model assumptions.
		This model adequacy check is based on the observations that (1)
		$\textnormal{MS}_{\textnormal{error}}$ is an unbiased estimator of $\sigma^{2}$, and (2):
		\begin{equation*}
		\mathbf{e} \; \sim \; N\!\left(\,0\,,\,\sigma^{2}\left(I_{n}-H\right)\,\right)\,,
		\end{equation*}
		which in turn implies that, for each $i = 1, \ldots, n$,
		\begin{equation*}
		\dfrac{e_{i}}{\sqrt{\sigma^{2}\left(1 - h_{ii}\right)}}
		\; \sim \;
		N(0,1)
		\end{equation*}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{alpha}
%\bibliographystyle{plain}
%\bibliographystyle{amsplain}
\bibliographystyle{acm}
\bibliography{KenChuBioinformatics}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

