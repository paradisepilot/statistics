
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Chipperfield \textit{et al.}, 2011, \cite{Chipperfield2011}}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{MLE for regression with categorical response and predictor variables}

The precise mathematical formulation of the discussion in
Section 2.1 (entitled ``Contingency tables'') in \cite{Chipperfield2011}
is the following proposition.

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space,
\item
	$n, C, G \in \N$ are natural numbers,
\item
	$Y^{(1)}, \ldots, Y^{(n)} : \Omega \longrightarrow \left[\,C\,\right] := \left\{1,2,\ldots,C\right\}$
	are categorical random variables, and
\item
	$X^{(1)}, \ldots, X^{(n)} : \Omega \longrightarrow \left[\,G\,\right] := \left\{1,2,\ldots,G\right\}$
	are categorical random variables.
\end{itemize}
Suppose:
\begin{itemize}
\item
	the $\left(\,\left[\,C\,\right] \times \left[\,G\,\right]\,\right)$-valued random variables
	$Z^{(i)} := \left(\,Y^{(i)},X^{(i)}\,\right)$, $i = 1, 2, \ldots, n$,
	are independent and identically distributed.
\end{itemize}
Then, the following statements are valid:
\begin{enumerate}
\item
	\label{independenceOfI}
	For each $(c,g) \in \left[\,C\,\right] \times \left[\,G\,\right]$, the probability
	\begin{equation*}
	\pi_{c \vert g} \;\;:=\;\; P\!\left(\left.Y^{(i)} = c \;\right\vert X^{(i)} = g\right)
	\end{equation*}
	is independent of $i \in \{1,2,\ldots,n\}$.
	Similary, for each given $g \in \left[\,G\,\right]$, the probability
	\begin{equation*}
	p_{g} \;\; := \;\; P\!\left(X^{(i)} = g\right)
	\end{equation*}
	is independent of $i \in \{1,2,\ldots,n\}$.
\item
	For each $(c,g) \in \left[\,C\,\right] \times \left[\,G\,\right]$,
	the maximum likelihood estimator (MLE)
	\,$\widehat{\pi}_{c\vert g}$\,
	of the parameter
	\,$\pi_{c\vert g}$\,
	is given by
	\begin{equation*}
	\widehat{\pi}_{c\vert g}
	\;\; = \;\;
		\dfrac{N_{cg}}{\overset{C}{\underset{c=1}{\sum}}\,N_{cg}}
	\;\; = \;\;
		\dfrac{N_{cg}}{N_{g}}\,,
	\end{equation*}	
	and, for each $g \in \left[\,G\,\right]$,
	the MLE
	\,$\widehat{p}_{g}$\,
	of the parameter
	\,$p_{g}$\,
	is given by:
	\begin{equation*}
	\widehat{p}_{g}
	\;\; = \;\;
		\dfrac{N_{g}}{\overset{G}{\underset{g=1}{\sum}}\,N_{g}}
	\;\; = \;\;
		\dfrac{N_{g}}{n}\,,
	\end{equation*}
	where
	\begin{equation*}
	N_{cg}
		\; := \;
		\overset{n}{\underset{i=1}{\sum}}\;
		W^{(i)}_{c \vert g}\,,
	\quad\quad
	N_{g}
		\; := \;
		\overset{n}{\underset{i=1}{\sum}} \;
		\overset{C}{\underset{c=1}{\sum}} \;
		W^{(i)}_{c \vert g}\,,
	\quad\quad
	\textnormal{and}
	\quad\quad
	W^{(i)}_{c \vert g}
		\; := \;
		I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}.
	\end{equation*}
\end{enumerate}
\end{proposition}

\proof
\begin{enumerate}
\item
	This follows trivially from the hypothesis that
	\,$Z^{(i)} \,:=\, \left(\,Y^{(i)},X^{(i)}\,\right)$,\,
	$i \in \{1,2,\ldots,n\}$,\,
	are IID.
\item
	In this proof, for ease of presentation, we will use the following notations:
	\begin{equation*}
	P\!\left(\left.Y = c \;\right\vert X = g\right)
		\; := \;
		\pi_{c\,\vert\,g}
	\quad\quad
	\textnormal{and}
	\quad\quad
	P\!\left(X = g\right)
		\; := \;
		p_{g}
	\end{equation*}
	which are valid because of \eqref{independenceOfI}.
	Now, the joint probability distribution of
	$\left(X^{(1)},Y^{(1)}\right)$, $\left(X^{(2)},Y^{(2)}\right)$, $\ldots$\,, $\left(X^{(n)},Y^{(n)}\right)$
	can be expressed as follows:
	\begin{eqnarray*}
	&&
		P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
	\\
	&=&
		P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}\right) \times \cdots \times P\!\left(X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
	\;\;=\;\;
		\overset{n}{\underset{i=1}{\prod}} \; P\!\left(X^{(i)} = x_{i},Y^{(i)}=y_{i}\right)
	\\
	&=&
		\overset{n}{\underset{i=1}{\prod}} \;
		\left\{\;
			\overset{G}{\underset{g=1}{\prod}} \;\;
			\overset{C}{\underset{c=1}{\prod}} \;
			P\!\left(X = g,Y=c\right)^{W^{(i)}_{c \vert g}}
		\;\right\}
	\;\;=\;\;
		\overset{G}{\underset{g=1}{\prod}} \;\;
		\overset{C}{\underset{c=1}{\prod}} \;
		\left\{\;
			\overset{n}{\underset{i=1}{\prod}} \;
			P\!\left(X = g,Y=c\right)^{W^{(i)}_{c \vert g}}
		\;\right\}
	\\
	&=&
		\overset{G}{\underset{g=1}{\prod}} \;\;
		\overset{C}{\underset{c=1}{\prod}} \;
		\left\{\,
			P\!\left(X = g,Y=c\right)^{\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert g}}
		\;\right\}
	\;\;=\;\;
		\overset{G}{\underset{g=1}{\prod}} \;\;
		\overset{C}{\underset{c=1}{\prod}} \;
		\left\{\;
			\left[\,P\!\left(\left.Y=c\;\right\vert X = g\right) \overset{{\color{white}\vert}}{\cdot} P\!\left(X=g\right)\,\right]
			^{\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert g}}
		\;\right\}
	\\
	&=&
		\overset{G}{\underset{g=1}{\prod}} \;\;
		\overset{C}{\underset{c=1}{\prod}} \;
		\left(\,\pi_{c\,\vert\,g} \overset{{\color{white}\vert}}{\cdot} p_{g}\,\right)
		^{\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert g}}
	\end{eqnarray*}
	where
	\,$W^{(i)}_{c \vert g} \,:=\, I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}$.\,
	The logarithm of this joint probability is thus:
	\begin{eqnarray*}
	&&
		\log\,P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
	\;\;=\;\;
		\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
		\left(\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert g}\right)
		\cdot
		\left(\,\log\,\pi_{c\,\vert\,g} \,\overset{{\color{white}\vert}}{+}\, \log\,p_{g}\,\right)
	\\
	&=&
		\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
		\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert g}\right)
		\cdot
		\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
		\;\; + \;\;
		\overset{G}{\underset{g=1}{\sum}} \, \left(\,\log\,p_{g}\,\right)
		\cdot
		\left(\,
			\overset{n}{\underset{i=1}{\sum}} \;
			\overset{C}{\underset{c=1}{\sum}} \;
			W^{(i)}_{c \vert g}
		\right)
	\\
	&=&
		\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;\,
		N_{cg} \cdot \left(\,\log\,\pi_{c\,\vert\,g}\,\right)
		\;\; + \;\;
		\overset{G}{\underset{g=1}{\sum}} \;\, N_{g} \cdot \left(\,\log\,p_{g}\,\right)
	\\
	&=&
		\overset{G}{\underset{g=1}{\sum}} \;
		\left\{\,
			N_{Cg}
			\cdot
			\log\left(\,1 - \overset{C-1}{\underset{c=1}{\sum}}\pi_{c\,\vert\,g}\,\right)
			\; + \;
			\overset{C-1}{\underset{c=1}{\sum}} \; N_{cg}
			\cdot
			\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
		\,\right\}
	\\
	&&
		+ \;
		\left\{\,
			N_{G}
			\cdot
			\log\left(\,1 - \overset{G-1}{\underset{g=1}{\sum}}\;p_{g}\,\right)
			\, + \,
			\overset{G-1}{\underset{g=1}{\sum}} \; N_{g}
			\cdot
			\left(\,\log\,p_{g}\,\right)
		\,\right\}.
	\end{eqnarray*}
	For each $c \in \{1,2,\ldots,C-1\}$ and $g \in \{1,2,\ldots,G\}$,
	differentiating with respect to \,$\pi_{c \vert g}$\, yields:
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,\pi_{c \vert g}}\,
	\log\,P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
	&=&
		\dfrac{N_{cg}\cdot(\,-1\,)}{1 - \overset{C-1}{\underset{y=1}{\sum}}\pi_{y\,\vert\,g}}
		\; + \;
		\dfrac{N_{cg}}{\pi_{c\,\vert\,g}}
	\\
	&=&
		\dfrac{N_{cg}}{\pi_{c\,\vert\,g}}
		\; - \;
		\dfrac{N_{cg}}{\pi_{C\,\vert\,g}}
	\end{eqnarray*}
	For each $g \in \{1,2,\ldots,G-1\}$, differentiating with respect to \,$p_{g}$\, yields:
	\begin{eqnarray*}
	\dfrac{\partial}{\partial\,p_{g}}\,
	\log\,P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
	&=&
		\dfrac{N_{G}\cdot(\,-1\,)}{1 - \overset{G-1}{\underset{x=1}{\sum}}\,p_{x}}
		\; + \;
		\dfrac{N_{g}}{p_{g}}
	\\
	&=&
		\dfrac{N_{g}}{p_{g}}
		\; - \;
		\dfrac{N_{G}}{p_{G}}
	\end{eqnarray*}
	Setting the left-hand-sides of the above two equations to zero yields:
	There exists $\beta \in \Re$ and, 
	for each $g \in \{1,2,\ldots,G\}$, there exists $\alpha_{g} \in \Re$ such that
	\begin{equation*}
		\begin{array}{ccl}
		\left(\,\pi_{1\vert g},\,\pi_{2\vert g}, \,\ldots\,, \,\pi_{C\vert g}\,\right)
		& = & \alpha_{g} \cdot \left(\,N_{1g},\,N_{2g}, \,\ldots\,,\, N_{Cg}\,\right)
		\\
		\left(\,p_{1},\,p_{2}, \,\ldots\,, \,p_{G}\,\right)
		& \overset{{\color{white}\vert}}{=} & \beta \cdot \left(\,N_{1},\, N_{2}, \,\ldots\,,\, N_{G}\,\right)
		\end{array}
	\end{equation*}
	Recall now that we also have
	\,$\overset{G}{\underset{g=1}{\sum}}\,p_{g} = 1$\, and
	\,$\overset{C}{\underset{c=1}{\sum}}\,\pi_{c\vert g} = 1$.
	Hence,
	\begin{equation*}
	\alpha_{g} \cdot \overset{C}{\underset{c=1}{\sum}}\;N_{cg}
	\;\; = \;\; \overset{C}{\underset{c=1}{\sum}}\;\pi_{c\vert g}
	\;\; = \;\; 1
	\quad\Longrightarrow\quad
	\alpha_{g} \;\;=\;\; \dfrac{1}{\overset{C}{\underset{c=1}{\sum}}\;N_{cg}}.
	\end{equation*}
	And,
	\begin{equation*}
	\beta \cdot \overset{G}{\underset{g=1}{\sum}}\;N_{g}
	\;\; = \;\; \overset{G}{\underset{g=1}{\sum}}\;N_{g}
	\;\; = \;\; 1
	\quad\Longrightarrow\quad
	\beta \;\;=\;\; \dfrac{1}{\overset{G}{\underset{g=1}{\sum}}\;N_{g}}.
	\end{equation*}
	We may now conclude that the maximum likelihood estimators of the parameters
	\,$\left(\,\pi_{1\vert g},\,\pi_{2\vert g}, \,\ldots\,, \,\pi_{C\vert g}\,\right)$\,
	and\\
	$\left(\,p_{1},\,p_{2}, \,\ldots\,, \,p_{G}\,\right)$\,
	are respectively given by:
	\begin{equation*}
		\begin{array}{ccccc}
		\left(\,\widehat{\pi}_{1\vert g}\,,\,\widehat{\pi}_{2\vert g}\,, \,\ldots\,, \,\widehat{\pi}_{C\vert g}\,\right)
		& = & \dfrac{1}{\overset{C}{\underset{c=1}{\sum}}\,N_{cg}} \cdot \left(\,N_{1g},\,N_{2g}, \,\ldots\,,\, N_{Cg}\,\right)
		& = & \dfrac{1}{N_{g}} \cdot \left(\,N_{1g},\,N_{2g}, \,\ldots\,,\, N_{Cg}\,\right),
		\\ \\
		\left(\,\widehat{p}_{1}\,,\,\widehat{p}_{2}\,, \,\ldots\,, \,\widehat{p}_{G}\,\right)
		& = & \dfrac{1}{\overset{G}{\underset{g=1}{\sum}}\,N_{g}} \cdot \left(\,N_{1},\,N_{2}, \,\ldots\,,\, N_{G}\,\right)
		& = & \dfrac{1}{n} \cdot \left(\,N_{1},\,N_{2}, \,\ldots\,,\, N_{G}\,\right).
		\end{array}
	\end{equation*}
\end{enumerate}
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{MLE for regression with categorical response and predictor variables,
with adjustments for observation errors via clerical review on a subsample of observations}

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space.
\item
	$n, G, C \in \N$ are natural numbers.
\item
	$X^{(1)}, \,\ldots\,,\, X^{(n)} : \Omega \longrightarrow \left[\,G\,\right] := \left\{1,2,\ldots,G\right\}$
	are categorical random variables.
\item
	$Y^{(1)}, \,\ldots\,,\, Y^{(n)} : \Omega \longrightarrow \left[\,C\,\right] := \left\{1,2,\ldots,C\right\}$
	are categorical random variables.
\item
	$\widetilde{Y}^{(1)}, \,\ldots\,,\, \widetilde{Y}^{(n)} : \Omega \longrightarrow \left[\,C\,\right] := \left\{1,2,\ldots,C\right\}$
	are categorical random variables.
\item
	$M^{(1)}, \,\ldots\,,\, M^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
	are Bernoulli random variables.
\item
	$J^{(1)}, \,\ldots\,,\, J^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
	are Bernoulli random variables.
\end{itemize}
Assume:
\begin{enumerate}
\item
	The $\left(\,\left[\,C\,\right] \times \left[\,G\,\right] \times \left[\,C\,\right] \times \left[\,1\,\right]\,\right)$-valued
	random variables
	\,$Z^{(i)} := \left(\,Y^{(i)},X^{(i)},\widetilde{Y}^{(i)},M^{(i)}\,\right)$,
	\,$i \in \{1, \ldots, n\}$,
	\,are independent and identically distributed.
\item
	The Bernoulli random variables \,$J^{(i)}$, \,$i \in \{1, \ldots, n\}$, \,are independent and identically distributed.
\item\label{Jindependence}
	The collections of random variables
	\,$\left\{\,Z^{(i)} := \left(\,Y^{(i)},X^{(i)},\widetilde{Y}^{(i)},M^{(i)}\right)\,\right\}_{i=1}^{n}$\,
	and
	\,$\left\{\,J^{(i)}\,\right\}_{i=1}^{n}$\,
	are independent, in the sense that, for all
	\,$z_{1}, \ldots, z_{n} \in \left[\,C\,\right] \times \left[\,G\,\right] \times \left[\,C\,\right] \times \left[\,1\,\right]$\,
	and
	\,$j_{1}, \ldots, j_{n} \in \{0,1\}$,\,
	\begin{eqnarray*}
	&&
		P\!\left(\,Z^{(1)}=z_{1},\,\ldots\,,Z^{(n)}=z_{n},\;J^{(1)}=j_{1},\,\ldots\,,J^{(n)}=j_{n}\,\right)
	\\
	& = &
		P\!\left(\,Z^{(1)}=z_{1},\,\ldots\,,Z^{(n)}=z_{n}\,\right)
		\cdot
		P\!\left(\,J^{(1)}=j_{1},\,\ldots\,,J^{(n)}=j_{n}\,\right)
	\end{eqnarray*}
\item\label{MzeroImplies}
	For each $i\in\{1,2,\ldots,n\}$, we have:
	\begin{equation*}
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=0\,\right)
	\;\; = \;\;
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,X^{(i)}=g\,\right)
	\end{equation*}
\item\label{MoneImpliesCEqualsY}
	For each $i\in\{1,2,\ldots,n\}$, we have:
	\begin{equation*}
	M^{(i)} \; = \; 1
	\quad\Longrightarrow\quad
	Y^{(i)} \; = \; \widetilde{Y}^{(i)},
	\end{equation*}
	which, in particular, implies $P\!\left(\,\left.Y^{(i)} = \widetilde{Y}^{(i)}\;\right\vert\;M^{(i)}=1\,\right)\,=\,1$.
\end{enumerate}
Define, for each $i \in \{1,2,\ldots,n\}$, the random variable
\;$K^{(i)} \;:\; \Omega \; \longrightarrow \; \{0,1,2\}$\; by
\begin{equation*}
K^{(i)} \;\;=\;\;
	\left\{\begin{array}{cl}
	0, & \textnormal{if}\;\; J^{(i)} = 0,
	\\
	1, & \textnormal{if}\;\; J^{(i)} = 1 \;\;\textnormal{and}\;\; M^{(i)} = 0,
	\\
	2, & \textnormal{if}\;\; J^{(i)} = 1 \;\;\textnormal{and}\;\; M^{(i)} = 1
	\end{array}\right.
\end{equation*}
Then, the logarithm of the joint probability distribution of
\;$Y^{(i)}$,\,
$X^{(i)}$,\,
$\widetilde{Y}^{(i)}$,\,
$K^{(i)}$,\,
for \;$i \in \{1,2,\ldots,n\}$,\;
is given by:
\begin{eqnarray*}
\log\,L
	&=&
	\log\,P\!\left(\;
		\overset{n}{\underset{i=1}{\bigcap}}\;
		\left\{
			Y^{(i)}=c_{i},X^{(i)}=x_{i},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}
		\right\}
	\;\right)
%\\
%&=&
%	\log\left(\,
%		\overset{n}{\underset{i=1}{\prod}} \,
%		P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x_{i},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}\,\right)
%	\right)
\\
&=&
	{\color{white}+} \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}0}}\right)
	\cdot
	\left(\,
		\log\left(\pi_{c \vert g}\,\mu_{0 \vert gy} + \delta_{cy}\,\mu_{1\vert gy}\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{0}
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert g}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\mu_{0\vert gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{1}
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert gy{\color{red}2}}\right)
	\cdot
	\left(\,
		\log\,\mu_{1 \vert gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{1}
	\,\right),
\end{eqnarray*}
where, \, for
\;$c \in \{1,\ldots,C\}$,
\;$g \in \{1,\ldots,G\}$,
\;$y \in \{1,\ldots,C\}$,
\;$m \in \{0,1\}$,
\;$j \in \{0,1\}$, \;and
\;$k \in \{0,1\}$,
\begin{equation*}
\begin{array}{ccll}
	W^{(i)}_{c \vert gyk}
		&:=&
		I\!\left\{\,Y^{(i)}=c,X^{(i)}=g,\widetilde{Y}^{(i)}=y,K^{(i)}=k\,\right\}\,,
	\\
	\pi_{c \vert g}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\left.\overset{{\color{white}.}}{Y}=c\;\right\vert X=g\,\right)\,,
	\\
	\mu_{m \vert gy}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\left.\overset{{\color{white}.}}{M}=m\;\right\vert X=g, \widetilde{Y}=y\,\right)\,,
	\\
	\nu_{gy}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\overset{{\color{white}.}}{X}=g, \widetilde{Y}=y\,\right)\,,
		\quad\textnormal{and}
	\\
	\omega_{j}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\overset{{\color{white}.}}{J}=j\,\right)\,,
	\\
	\overset{\textnormal{\large\color{white}1}}{\delta}_{cy}
		&\overset{{\color{white}\vert}}{:=}&
		\left\{\begin{array}{cl}
			1, & \textnormal{if}\;\; c = y\,,
			\\
			0, & \textnormal{otherwise}\,.
		\end{array}\right.
\end{array}
\end{equation*}
\end{proposition}
\proof
The joint probability distribution function of
\,$Y^{(i)}$, $X^{(i)}$, $\widetilde{Y}^{(i)}$, $K^{(i)}$,
\,for \,$i \in \{1,\ldots,n\}$,
\,is given by:
\begin{eqnarray*}
L &=&
	P\!\left(\;
		\overset{n}{\underset{i=1}{\bigcap}}\;
		\left\{
			Y^{(i)}=c_{i},X^{(i)}=x_{i},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}
		\right\}
	\;\right)
\\
&=&
	\overset{n}{\underset{i=1}{\prod}} \;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x_{i},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}\,\right)
\\
&=&
	\overset{n}{\underset{i=1}{\prod}} \;
	\left\{\;
		\overset{C}{\underset{c=1}{\prod}}\;\;
		\overset{G}{\underset{g=1}{\prod}}\;\;
		\overset{C}{\underset{y=1}{\prod}}\;\;
		\overset{2}{\underset{k=0}{\prod}}\;
		P\!\left(\,Y=c,X=g,\widetilde{Y}=y,K=k\,\right)^{W^{(i)}_{c \vert gyk}}
	\;\right\}
\\
&=&
	\overset{C}{\underset{c=1}{\prod}}\;\;
	\overset{G}{\underset{g=1}{\prod}}\;\;
	\overset{C}{\underset{y=1}{\prod}}\;\;
	\overset{2}{\underset{k=0}{\prod}}\;
	\left\{\;
		\overset{n}{\underset{i=1}{\prod}} \;
		P\!\left(\,Y=c,X=g,\widetilde{Y}=y,K=k\,\right)^{W^{(i)}_{c \vert gyk}}
	\;\right\}
\\
&=&
	\overset{C}{\underset{c=1}{\prod}}\;\;
	\overset{G}{\underset{g=1}{\prod}}\;\;
	\overset{C}{\underset{y=1}{\prod}}\;\;
	\overset{2}{\underset{k=0}{\prod}}\;
	\left\{\;
		P\!\left(\,Y=c,X=g,\widetilde{Y}=y,K=k\,\right)
		^{\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert gyk}}
	\;\right\}\,.
\end{eqnarray*}
The logarithm of this joint probability distribution function is thus:
\begin{eqnarray*}
\log\,L &=&
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{2}{\underset{k=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gyk}\right)
	\cdot
	\log\,P\!\left(\,Y=c,X=g,\widetilde{Y}=y,K=k\,\right)
\\
&=&
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{2}{\underset{k=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gyk}\right)
	\cdot
	\log\left[\,
		P\!\left(\,Y=c\;\left\vert\;X=g,\widetilde{Y}=y,K=k\right.\,\right)
		P\!\left(\,X=g,\widetilde{Y}=y,K=k\,\right)
	\,\right]
\\
&=&
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{2}{\underset{k=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gyk}\right)
	\cdot
	\log\left[\,
		\pi_{c \vert gyk} \overset{{\color{white}\vert}}{\cdot} p_{gyk}
	\,\right]
\\
&=&
	{\color{white}+} \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}0}}\right)
	\cdot
	\log\left[\,
		\pi_{c \vert gy\color{red}0} \cdot p_{gy\color{red}0}
	\,\right]
\\
&&
	+ \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}1}}\right)
	\cdot
	\log\left[\,
		\pi_{c \vert gy\color{red}1} \cdot p_{gy\color{red}1}
	\,\right]
\\
&&
	+ \;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert gy{\color{red}2}}\right)
	\cdot
	\log\left[\,
		\pi_{y \vert gy\color{red}2} \cdot p_{gy\color{red}2}
	\,\right]\,,
\end{eqnarray*}
where
\;$\pi_{c \vert gyk} \, := \, P\!\left(\,Y=c\;\left\vert\;X=g,\widetilde{Y}=y,K=k\right.\,\right)$,
\;$p_{gyk} \, := \, P\!\left(\,X=g,\widetilde{Y}=y,K=k\,\right)$,
\;for
\;$g \in \left\{1,\ldots,G\right\}$,
\;$y \in \left\{1,\ldots,C\right\}$,
\,$k \in \left\{0,1,2\right\}$.
\;Note that the last equality above follows from
\begin{eqnarray*}
W^{(i)}_{c \vert gy{\color{red}2}}
&:=&
	I\!\left\{\,Y^{(i)}=c,X^{(i)}=g,\widetilde{Y}^{(i)}=y,K^{(i)}={\color{red}2}\,\right\}
\\
&=&
	I\!\left\{\,Y^{(i)}=c,X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=1,M^{(i)}=1\,\right\}
\\
&=&
	\left\{\begin{array}{cl}
		1, & \textnormal{if}\;\; c = y
		\\
		0, & \textnormal{otherwise}
	\end{array}\right.\,,
\end{eqnarray*}
which in turn follows from the definition of $K^{(i)}$ and
the hypothesis \eqref{MoneImpliesCEqualsY}.
Next, in order to establish the desired expression for the log likelihood,
we derive expressions for
\,$\pi_{c \vert gy0}$,
\,$\pi_{c \vert gy1}$,
\,$\pi_{c \vert gy2}$,
\,$p_{gy0}$,
\,$p_{gy1}$,
\,and
\,$p_{gy2}$,
in terms of
\,$\pi_{c \vert g}$,
\,$\mu_{0 \vert gy}$,
\,$\mu_{1 \vert gy}$,
\,$\nu_{gy}$,
\,$\omega_{0}$,
\,and
\,$\omega_{1}$.
\begin{eqnarray*}
\pi_{c \vert gy0}
&=&
	P\!\left(\;Y=c\;\left\vert\;X=g,\widetilde{Y}=y,K=0\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X=g,\widetilde{Y}=y,J=0\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right),
	\quad\quad
	\textnormal{by \eqref{Jindependence}}
\\
&=&
	P\!\left(\;Y=c,M=0\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right)
	\;+\;
	P\!\left(\;Y=c,M=1\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right)
\\
&=&
	{\color{white}+}\;\;
	P\!\left(\;Y=c\;\left\vert\;M=0,X=g,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\;M=0\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right)
\\
&&
	+\;\;
	P\!\left(\;Y=c\;\left\vert\;M=1,X=g,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\;M=1\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X\overset{{\color{white}-}}{=}g\right.\,\right)
	\cdot
	\mu_{0 \vert gy}
	\;\;+\;\;
	\delta_{cy}
	\cdot
	\mu_{1 \vert gy}\,,
	\quad\;
	\textnormal{by \,\eqref{MzeroImplies}, \eqref{MoneImpliesCEqualsY},\, and definition of \,$\mu_{m \vert gy}$}
\\
&=&
	\pi_{c \vert g}
	\cdot
	\mu_{0 \vert gy}
	\;\;+\;\;
	\delta_{cy}
	\cdot
	\mu_{1 \vert gy}\,,
	\quad\quad
	\textnormal{by definition of \,$\pi_{c \vert g}$}
\end{eqnarray*}
\begin{eqnarray*}
\pi_{c \vert gy1}
&=&
	P\!\left(\;Y=c\;\left\vert\;X=g,\widetilde{Y}=y,K=1\right.\,\right)
\;\;=\;\;
	P\!\left(\;Y=c\;\left\vert\;X=g,\widetilde{Y}=y,J=1,M=0\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X=g,\widetilde{Y}=y,M=0\right.\,\right),
	\quad\quad
	\textnormal{by \eqref{Jindependence}}
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X\overset{{\color{white}-}}{=}g\right.\,\right),
	\quad\quad
	\textnormal{by \,\eqref{MzeroImplies}}
\\
&=&
	\pi_{c \vert g}\,,
	\quad\quad
	\textnormal{by definition of \,$\pi_{c \vert g}$}
\end{eqnarray*}
\begin{eqnarray*}
\pi_{y \vert gy2}
&=&
	P\!\left(\;Y=y\;\left\vert\;X=g,\widetilde{Y}=y,K=2\right.\,\right)
\;\;=\;\;
	P\!\left(\;Y=y\;\left\vert\;X=g,\widetilde{Y}=y,J=1,M=1\right.\,\right)
\\
&=&
	1\,,
	\quad\quad
	\textnormal{by \eqref{MoneImpliesCEqualsY}.}
\end{eqnarray*}
\begin{eqnarray*}
p_{gy0}
&=&
	P\!\left(\,X=g,\widetilde{Y}=y,K=0\,\right)
\;\;=\;\;
	P\!\left(\,X=g,\widetilde{Y}=y,J=0\,\right)
\\
&=&
	P\!\left(\,X=g,\widetilde{Y}=y\,\right)
	\cdot
	P\!\left(\,J=0\,\right)\,,
	\quad\quad
	\textnormal{by \eqref{Jindependence}}
\\
&=&
	\nu_{gy}
	\cdot
	\omega_{0}\,,
	\quad\quad
	\textnormal{by definition of \,$\nu_{gy}$\, and \,$\omega_{0}$}
\end{eqnarray*}
\begin{eqnarray*}
p_{gy1}
&=&
	P\!\left(\,X=g,\widetilde{Y}=y,K=1\,\right)
\;\;=\;\;
	P\!\left(\,X=g,\widetilde{Y}=y,J=1,M=0\,\right)
\\
&=&
	P\!\left(\,X=g,\widetilde{Y}=y,M=0\,\right)
	\cdot
	P\!\left(\,J=1\,\right)\,,
	\quad\quad
	\textnormal{by \eqref{Jindependence}}
\\
&=&
	P\!\left(\,M=0\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\,X=g,\widetilde{Y}=y\,\right)
	\cdot
	P\!\left(\,J=1\,\right)\,,
\\
&=&
	\mu_{0 \vert gy}
	\cdot
	\nu_{gy}
	\cdot
	\omega_{1}\,,
	\quad\quad
	\textnormal{by definition of \,$\mu_{0 \vert gy}$, \,$\nu_{gy}$\, and \,$\omega_{1}$}
\end{eqnarray*}
\begin{eqnarray*}
p_{gy2}
&=&
	P\!\left(\,X=g,\widetilde{Y}=y,K=2\,\right)
\;\;=\;\;
	P\!\left(\,X=g,\widetilde{Y}=y,J=1,M=1\,\right)
\\
&=&
	P\!\left(\,X=g,\widetilde{Y}=y,M=1\,\right)
	\cdot
	P\!\left(\,J=1\,\right)\,,
	\quad\quad
	\textnormal{by \eqref{Jindependence}}
\\
&=&
	P\!\left(\,M=1\;\left\vert\;X=g,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\,X=g,\widetilde{Y}=y\,\right)
	\cdot
	P\!\left(\,J=1\,\right)\,,
\\
&=&
	\mu_{1 \vert gy}
	\cdot
	\nu_{gy}
	\cdot
	\omega_{1}\,,
	\quad\quad
	\textnormal{by definition of \,$\mu_{1 \vert gy}$, \,$\nu_{gy}$\, and \,$\omega_{1}$}
\end{eqnarray*}
Substituting the above expressions for
\,$\pi_{c \vert gy0}$,
\,$\pi_{c \vert gy1}$,
\,$\pi_{c \vert gy2}$,
\,$p_{gy0}$,
\,$p_{gy1}$,
\,and
\,$p_{gy2}$
\,into that of the log likelihood yields
\begin{eqnarray*}
\log\,L
&=&
	{\color{white}+} \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}0}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert gy\color{red}0} \,\overset{{\color{white}.}}{+}\, \log\,p_{gy\color{red}0}
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert gy\color{red}1} \,\overset{{\color{white}.}}{+}\, \log\,p_{gy\color{red}1}
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert gy{\color{red}2}}\right)
	\cdot
	\left(\,
		\log\,\pi_{y \vert gy\color{red}2} \,\overset{{\color{white}.}}{+}\, \log\,p_{gy\color{red}2}
	\,\right)
\\
&=&
	{\color{white}+} \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}0}}\right)
	\cdot
	\left(\,
		\log\left(\pi_{c \vert g}\,\mu_{0 \vert gy} + \delta_{cy}\,\mu_{1\vert gy}\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\left(\nu_{gy}\,\omega_{0}\right)
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert g}
		\,\overset{{\color{white}.}}{+}\,
		\log\left(\mu_{0\vert gy}\,\nu_{gy}\,\omega_{1}\right)
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert gy{\color{red}2}}\right)
	\cdot
	\left(\,
		\log\left(\,1\,\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\left(\mu_{1 \vert gy}\,\nu_{gy}\,\omega_{1}\right)
	\,\right)
\\
&=&
	{\color{white}+} \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}0}}\right)
	\cdot
	\left(\,
		\log\left(\pi_{c \vert g}\,\mu_{0 \vert gy} + \delta_{cy}\,\mu_{1\vert gy}\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{0}
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{c=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;\;
	\overset{C}{\underset{y=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert g}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\mu_{0\vert gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{1}
	\,\right)
\\
&&
	+ \;\;
	\overset{C}{\underset{y=1}{\sum}}\;\;
	\overset{G}{\underset{g=1}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert gy{\color{red}2}}\right)
	\cdot
	\left(\,
		\log\,\mu_{1 \vert gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{gy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{1}
	\,\right)
\end{eqnarray*}
This completes the proof of the Proposition.
\qed

$\rho_{gc} \;:=\; P\!\left(M=1\;\vert\;X=g,\widetilde{Y}=c\right)$

\vskip 0.3cm
\noindent
Define the following Bernoulli random variables (indicator variables):
\begin{equation*}
\begin{array}{ccccl}
	W^{(i)}_{c \vert g} &:& \Omega \longrightarrow \{0,1\} &:& \omega\;\longmapsto\,
	\left\{
		\begin{array}{cl}
		1, & \textnormal{if}\;\; X^{(i)}(\omega) = g \,\;\textnormal{and}\;\, Y^{(i)}(\omega) = c,
		\\
		\overset{{\color{white}-}}{0}, & \textnormal{otherwise}
		\end{array}
	\right.
\\ \\
	\widetilde{W}^{(i)}_{c \vert g} &:& \Omega \longrightarrow \{0,1\} &:& \omega\;\longmapsto\,
	\left\{
		\begin{array}{cl}
		1, & \textnormal{if}\;\; X^{(i)}(\omega) = g \,\;\textnormal{and}\;\, \widetilde{Y}^{(i)}(\omega) = c,
		\\
		\overset{{\color{white}-}}{0}, & \textnormal{otherwise}
		\end{array}
	\right.
\end{array}
\end{equation*}
\qed

\vskip 0.3cm
\noindent
\begin{equation*}
\begin{array}{lcl}
E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=0\;\right]
&=& 
\\ \\
E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=1,M^{(i)}=1\;\right]
&=& \widetilde{W}^{(i)}_{c \vert g}
\\ \\
E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=1,M^{(i)}=0\;\right]
&=& \pi_{c \vert g}
\end{array}
\end{equation*}

\begin{eqnarray*}
&&
	E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=0\;\right]
\;\;=\;\;
	E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y\;\right]
\\
&=&
	1 \cdot P\!\left(\left.W^{(i)}_{c \vert g} = 1\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
	\;\;+\;\;
	0 \cdot P\!\left(\left.W^{(i)}_{c \vert g} = 0\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
&=&
	P\!\left(\left.W^{(i)}_{c \vert g} = 1\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\;\; = \;\;
	P\!\left(\left.X^{(i)} = g\,,Y^{(i)} = c\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
& = &
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
&=&
	P\!\left(\left.Y^{(i)} = c\,,M^{(i)}=0\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
	\; + \; P\!\left(\left.Y^{(i)} = c\,,M^{(i)}=1\,\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
&=&
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,M^{(i)}=0,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
	\,\cdot\,
	P\!\left(\left.M^{(i)}=0\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
&&
	+ \;\;
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,M^{(i)}=1,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
	\,\cdot\,
	P\!\left(\left.M^{(i)}=1\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
&=&
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,X^{(i)}=g\,\right)
	\,\cdot\,
	P\!\left(\left.M^{(i)}=0\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\\
&&
	+ \;\;
	\delta_{cy}
	\,\cdot\,
	P\!\left(\left.M^{(i)}=1\;\right\vert\,X^{(i)}=g,\widetilde{Y}^{(i)}=y\,\right)
\end{eqnarray*}

\begin{eqnarray*}
&&
	E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=1,M^{(i)}=1\;\right]
\;\;=\;\;
	E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=1\;\right]
\\
&=&
	P\!\left(\left.W^{(i)}_{c \vert g}=1\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=1\,\right)
\;\;=\;\;
	P\!\left(\left.X^{(i)} = g\,,Y^{(i)} = c\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=1\,\right)
\\
&=&
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=1\,\right)
	\;\; = \;\; \delta_{cy}
\end{eqnarray*}

\begin{eqnarray*}
&&
	E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,J^{(i)}=1,M^{(i)}=0\;\right]
\;\;=\;\;
	E\!\left[\;\left.W^{(i)}_{c \vert g}\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=0\;\right]
\\
&=&
	P\!\left(\left.W^{(i)}_{c \vert g}=1\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=0\,\right)
\;\;=\;\;
	P\!\left(\left.X^{(i)} = g\,,Y^{(i)} = c\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=0\,\right)
\\
&=&
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\;X^{(i)}=g,\widetilde{Y}^{(i)}=y,M^{(i)}=0\,\right)
\\
&=&
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert\;X^{(i)}=g\,\right)
\end{eqnarray*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
