
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Chipperfield \textit{et al.}, 2011, \cite{Chipperfield2011}}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{Contingency tables}
Suppose
$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space and
$n, p, G, C \in \N$ are natural numbers.
Suppose
\begin{itemize}
\item
	$X^{(1)}, \ldots, X^{(n)} : \Omega \longrightarrow \left[\,G\,\right] := \left\{1,2,\ldots,G\right\}$
	are categorical random variables,
\item
	$Y^{(1)}, \ldots, Y^{(n)} : \Omega \longrightarrow \left[\,C\,\right] := \left\{1,2,\ldots,C\right\}$
	are categorical random variables, and
\item
	the $\left(\,\left[\,G\,\right] \times \left[\,C\,\right]\,\right)$-valued random variables
	$Z^{(i)} := \left(\,Y^{(i)},X^{(i)}\,\right)$, $i = 1, 2, \ldots, n$, are independent and identically distributed.
\end{itemize}
%Then, the joint probability distribution of $X,Y$ can be expressed as follows:
%\begin{equation*}
%P\!\left(X = x,Y=y\right) \;\;=\;\; P_{Y \vert X}\!\left(\,Y = y \,\vert X = x\right) \cdot P_{X}\!\left(X = x\right).
%\end{equation*}
%We seek to estimate $P_{Y \vert X}\!\left(\,Y = y \,\vert X = x\right)$,
%for each $(x,y) \in \left[\,G\,\right] \times \left[\,C\,\right]$,
%based on the observed data
%$\left(X^{(1)},Y^{(1)}\right)$, $\left(X^{(2)},Y^{(2)}\right)$, $\ldots$\,, $(X^{(n)},Y^{(n)})$.
Then, the joint probability distribution of
$\left(X^{(1)},Y^{(1)}\right)$, $\left(X^{(2)},Y^{(2)}\right)$, $\ldots$\,, $(X^{(n)},Y^{(n)})$
can be expressed as follows:
\begin{eqnarray*}
&&
	P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
\\
&=&
	P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}\right) \times \cdots \times P\!\left(X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
\\
&=&
	P_{Y \vert X}\!\left(\left.Y^{(1)} = y_{1} \,\right\vert X^{(1)} = x_{1}\right) \cdot P_{X}\!\left(X^{(1)} = x_{1}\right)
	\times \cdots \times
	P_{Y \vert X}\!\left(\left.Y^{(n)} = y_{n} \,\right\vert X^{(n)} = x_{n}\right) \cdot P_{X}\!\left(X^{(n)} = x_{n}\right)
\\
&=&
	\overset{G}{\underset{g=1}{\prod}} \;\;\; \overset{C}{\underset{c=1}{\prod}} \;
	\left(\,\pi_{c\,\vert\,g} \overset{{\color{white}\vert}}{\cdot} p_{g}\,\right)
	^{\overset{n}{\underset{i=1}{\sum}}\,I\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}}
%\;\; = \;\;
%	\overset{G}{\underset{g=1}{\prod}}
%		\left(\,\overset{{\color{white}-}}{p}_{g}\,\right)^{\overset{n}{\underset{i=1}{\sum}}\,I\left\{X^{(i)}=g\right\}}
%	\,\cdot\,
%	\overset{C}{\underset{c=1}{\prod}}
%	\left(\,\overset{{\color{white}\vert}}{\pi}_{c\,\vert\,g}\,\right)
%	^{\overset{n}{\underset{i=1}{\sum}}\,I\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}},
\end{eqnarray*}
where
$\pi_{c\,\vert\,g} \,:=\, P_{Y \vert X}\!\left(\left.Y = c \,\right\vert X = g\right)$
and
$p_{g} \,:=\, P_{X}\!\left(X = g\right)$.
First, note that we have the identity:
\begin{equation*}
\overset{C}{\underset{c=1}{\sum}}\;\pi_{c\vert g} \;\; = \;\; 1.
\end{equation*}
We seek to estimate $\pi_{c\,\vert\,g}$\,,
%$P_{Y \vert X}\!\left(\,Y = c \,\vert X = g\right)$,
for each $(g,c) \in \left[\,G\,\right] \times \left[\,C\,\right]$,
based on the observed data.
Now, the logarithm of the above joint probability is:
\begin{eqnarray*}
&&
	\log\,P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
\\
&=&
	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
	\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
	\cdot
	\left(\,\log\,\pi_{c\,\vert\,g} \,\overset{{\color{white}\vert}}{+}\, \log\,p_{g}\,\right)
\\
&=&
	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
	\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
	\cdot
	\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
	\;\; + \;\;
	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
	\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
	\cdot
	\left(\,\log\,p_{g}\,\right)
\\
&=&
	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
	\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
	\cdot
	\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
	\;\; + \;\;
	\overset{G}{\underset{g=1}{\sum}} \, \left(\,\log\,p_{g}\,\right)
	\cdot
	\left(\,
		\overset{n}{\underset{i=1}{\sum}} \;
		\overset{C}{\underset{c=1}{\sum}} \,
		I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}
	\right)
\\
&=&
	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
	\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
	\cdot
	\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
	\;\; + \;\;
	\overset{G}{\underset{g=1}{\sum}} \, \left(\,\log\,p_{g}\,\right)
	\cdot
	\left(\, \overset{n}{\underset{i=1}{\sum}} \; I\!\left\{X^{(i)}=g\right\} \right)
\\
&=&
	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;\,
	n_{gc} \cdot \left(\,\log\,\pi_{c\,\vert\,g}\,\right)
	\;\; + \;\;
	\overset{G}{\underset{g=1}{\sum}} \;\, n_{g} \cdot \left(\,\log\,p_{g}\,\right)
\\
&=&
	\overset{G}{\underset{g=1}{\sum}} \;
	\left\{\,
		n_{gC}
		\cdot
		\log\left(\,1 - \overset{C-1}{\underset{c=1}{\sum}}\pi_{c\,\vert\,g}\,\right)
		\; + \;
		\overset{C-1}{\underset{c=1}{\sum}} \; n_{gc}
		\cdot
		\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
	\,\right\}
	\; + \;
	\left\{\,
		n_{G}
		\cdot
		\log\left(\,1 - \overset{G-1}{\underset{g=1}{\sum}}\;p_{g}\,\right)
		\, + \,
		\overset{G-1}{\underset{g=1}{\sum}} \; n_{g}
		\cdot
		\left(\,\log\,p_{g}\,\right)
	\,\right\}
\end{eqnarray*}
%Similarly, the joint probability distribution of $X^{(1)}$, $X^{(2)}$, $\ldots$\,, $X^{(n)}$
%can be expressed as:
%\begin{eqnarray*}
%P\!\left(X^{(1)} = x_{1}, X^{(2)} = x_{2}, \;\ldots\; ,X^{(n)} = x_{n}\right)
%&=&
%	P\!\left(X^{(1)} = x_{1}\right) \times P\!\left(X^{(2)} = x_{2}\right) \times \cdots \times P\!\left(X^{(n)} = x_{n}\right)
%\\
%&=&
%	\overset{G}{\underset{g=1}{\prod}}
%	\left(\,\overset{{\color{white}\vert}}{p}_{g}\,\right)
%	^{\overset{n}{\underset{i=1}{\sum}}\,I\left\{X^{(i)}=g\right\}},
%\end{eqnarray*}
%with logarithm:
%\begin{eqnarray*}
%\log\, P\!\left(X^{(1)} = x_{1}, X^{(2)} = x_{2}, \;\ldots\; ,X^{(n)} = x_{n}\right)
%&=&
%	\overset{G}{\underset{g=1}{\sum}}
%	\left(\overset{n}{\underset{i=1}{\sum}}\,I\left\{X^{(i)}=g\right\}\right)
%	\cdot
%	\left(\,\log\,\overset{{\color{white}\vert}}{p}_{g}\,\right)
%\end{eqnarray*}
%We therefore obtain this expression for the following logarithm of conditional probability:
%\begin{eqnarray*}
%&&
%	\log\,P\!\left(\,\left.Y^{(1)}=y_{1},Y^{(2)}=y_{2},\,\ldots\,,Y^{(n)}=y_{n}\;\right\vert\,X^{(1)}=x_{1},X^{(2)}=x_{2},\ldots,X^{(n)}=x_{n}\right)
%\\
%&=&
%	\overset{G}{\underset{g=1}{\sum}} \;\; \overset{C}{\underset{c=1}{\sum}} \;
%	\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
%	\cdot
%	\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
%\\
%&=&
%	\overset{G}{\underset{g=1}{\sum}} \;
%	\left\{\,
%		\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=C\right\}\right)
%		\cdot
%		\log\left(\,1 - \overset{C-1}{\underset{c=1}{\sum}}\pi_{c\,\vert\,g}\,\right)
%		\; + \;
%		\overset{C-1}{\underset{c=1}{\sum}} \;
%		\left(\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{X^{(i)}=g,\,Y^{(i)}=c\right\}\right)
%		\cdot
%		\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
%	\,\right\}
%\\
%&=&
%	\overset{G}{\underset{g=1}{\sum}} \;
%	\left\{\,
%		n_{gC}
%		\cdot
%		\log\left(\,1 - \overset{C-1}{\underset{c=1}{\sum}}\pi_{c\,\vert\,g}\,\right)
%		\; + \;
%		\overset{C-1}{\underset{c=1}{\sum}} \; n_{gc}
%		\cdot
%		\left(\,\log\,\pi_{c\,\vert\,g}\,\right)
%	\,\right\}
%\end{eqnarray*}
For each $g \in \{1,2,\ldots,G\}$ and $c \in \{1,2,\ldots,C-1\}$, differentiating with respect to \,$\pi_{c \vert g}$\, yields:
\begin{eqnarray*}
\dfrac{\partial}{\partial\,\pi_{c \vert g}}\,
\log\,P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
&=&
	\dfrac{n_{gC}\cdot(\,-1\,)}{1 - \overset{C-1}{\underset{y=1}{\sum}}\pi_{y\,\vert\,g}}
	\; + \;
	\dfrac{n_{gc}}{\pi_{c\,\vert\,g}}
\\
&=&
	\dfrac{n_{gc}}{\pi_{c\,\vert\,g}}
	\; - \;
	\dfrac{n_{gC}}{\pi_{C\,\vert\,g}}
\end{eqnarray*}
For each $g \in \{1,2,\ldots,G-1\}$, differentiating with respect to \,$p_{g}$\, yields:
\begin{eqnarray*}
\dfrac{\partial}{\partial\,p_{g}}\,
\log\,P\!\left(X^{(1)} = x_{1},Y^{(1)}=y_{1}, \;\ldots\; ,X^{(n)} = x_{n},Y^{(n)}=y_{n}\right)
&=&
	\dfrac{n_{G}\cdot(\,-1\,)}{1 - \overset{G-1}{\underset{x=1}{\sum}}\,p_{x}}
	\; + \;
	\dfrac{n_{g}}{p_{g}}
\\
&=&
	\dfrac{n_{g}}{p_{g}}
	\; - \;
	\dfrac{n_{G}}{p_{G}}
\end{eqnarray*}
Setting the L.H.S.'s of the above two equations to zero yields:
There exists $\beta \in \Re$ and, 
for each $g \in \{1,2,\ldots,G\}$, there exists $\alpha_{g} \in \Re$ such that
\begin{equation*}
	\begin{array}{ccl}
	\left(\,\pi_{1\vert g},\,\pi_{2\vert g}, \,\ldots\,, \,\pi_{C\vert g}\,\right)
	& = & \alpha_{g} \cdot \left(\,n_{g1},\,n_{g2}, \,\ldots\,,\, n_{gC}\,\right)
	\\
	\left(\,p_{1},\,p_{2}, \,\ldots\,, \,p_{G}\,\right)
	& \overset{{\color{white}\vert}}{=} & \beta \cdot \left(\,n_{1},\,n_{2}, \,\ldots\,,\, n_{G}\,\right)
	\end{array}
\end{equation*}
Recall now that we also have
\,$\overset{G}{\underset{g=1}{\sum}}\,p_{g} = 1$\, and
\,$\overset{C}{\underset{c=1}{\sum}}\,\pi_{c\vert g} = 1$.
Hence,
\begin{equation*}
\alpha_{g} \cdot \overset{C}{\underset{c=1}{\sum}}\;n_{gc}
\;\; = \;\; \overset{C}{\underset{c=1}{\sum}}\;\pi_{c\vert g}
\;\; = \;\; 1
\quad\Longrightarrow\quad
\alpha_{g} \;\;=\;\; \dfrac{1}{\overset{C}{\underset{c=1}{\sum}}\;n_{gc}}.
\end{equation*}
And,
\begin{equation*}
\beta \cdot \overset{G}{\underset{g=1}{\sum}}\;n_{g}
\;\; = \;\; \overset{G}{\underset{g=1}{\sum}}\;p_{g}
\;\; = \;\; 1
\quad\Longrightarrow\quad
\beta \;\;=\;\; \dfrac{1}{\overset{G}{\underset{g=1}{\sum}}\;n_{g}}.
\end{equation*}
We may now conclude that the maximum likelihood estimators of the parameters
\,$\left(\,\pi_{1\vert g},\,\pi_{2\vert g}, \,\ldots\,, \,\pi_{C\vert g}\,\right)$\,
and\\
$\left(\,p_{1},\,p_{2}, \,\ldots\,, \,p_{G}\,\right)$\,
are given by:
\begin{equation*}
	\begin{array}{cclcl}
	\left(\,\widehat{\pi}_{1\vert g}\,,\,\widehat{\pi}_{2\vert g}\,, \,\ldots\,, \,\widehat{\pi}_{C\vert g}\,\right)
	& = & \dfrac{1}{\sum^{C}_{c=1}n_{gc}} \cdot \left(\,n_{g1},\,n_{g2}, \,\ldots\,,\, n_{gC}\,\right)
	& = & \dfrac{1}{n_{g}} \cdot \left(\,n_{g1},\,n_{g2}, \,\ldots\,,\, n_{gC}\,\right),
	\\ \\
	\left(\,\widehat{p}_{1}\,,\,\widehat{p}_{2}\,, \,\ldots\,, \,\widehat{p}_{G}\,\right)
	& = & \dfrac{1}{\sum^{G}_{g=1}n_{g}} \cdot \left(\,n_{1},\,n_{2}, \,\ldots\,,\, n_{G}\,\right)
	& = & \dfrac{1}{n} \cdot \left(\,n_{1},\,n_{2}, \,\ldots\,,\, n_{G}\,\right).
	\end{array}
\end{equation*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{DELETEME}

Suppose $n, p \in \N$ are natural numbers and
$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space.
Suppose $M^{(1)}, \ldots, M^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
are Bernoulli random variables defined on $\Omega$, and
$\Gamma^{(1)}, \ldots, \Gamma^{(n)} : \Omega \longrightarrow \left\{0,1\right\}^{p}$
are $\left\{0,1\right\}^{p}$-valued random variables defined on $\Omega$.

\vskip 0.5cm
\noindent
\textbf{The Record Linkage Problem}
\vskip 0.05cm
\noindent
Suppose the values of $\Gamma^{(k)} \in \{0,1\}^{p}$ are known (have been observed) for each $k \in \{1,2,\ldots,n\}$,
but those of the $M^{(k)}$'s are not known.
We would like to ``predict'' accurately $M^{(k)} \in \{0,1\}$, for each $k \in \{1,2,\ldots,n\}$,
based on the known values of the $\Gamma^{(k)}$'s.

\vskip 0.5cm
\begin{remark}[The Fellegi-Sunter decision rule via the log odds]
\mbox{}\vskip 0.05cm
\noindent
In order to ``predict'' $M^{(k)}$ based on the observations of $\Gamma^{(k)}$,
we must tacitly assume that the $M^{(k)}$'s and the $\Gamma^{(k)}$'s must be
``suitably'' dependent.
For example, one such assumption of ``suitable'' dependence among
the $M^{(k)}$'s and the $\Gamma^{(k)}$'s might be:\,
For each $k = 1,2,\ldots,n$,
\begin{equation}\label{suitableDependence}
\rho\!\left(\,\gamma^{(k)}\,\right) \;\,\textnormal{tends to be}
\;\;
\left\{\begin{array}{cl}
\textnormal{large}\,, & \textnormal{if \;$M^{(k)} = 1$}
\\
\textnormal{small}\,, & \textnormal{if \;$\overset{{\color{white}.}}{M^{(k)}} = 0$}
\end{array}\right.\,,
\end{equation}
where
\begin{equation*}
\rho\!\left(\,\gamma^{(k)}\,\right)
\;\; := \;\;
\log\,
\dfrac{P\!\left(\,\Gamma^{(k)}=\gamma^{(k)}\,\left\vert\;M^{(k)} = \overset{{\color{white}.}}{1}\,\right.\right)}
{P\!\left(\,\Gamma^{(k)}=\gamma^{(k)}\,\left\vert\;M^{(k)} = \overset{{\color{white}.}}{0}\right.\,\right)}\,,
\end{equation*}
and $\gamma^{(k)} \in \{0,1\}^{p}$ denotes the observed value of \,$\Gamma^{(k)}$.
Thus, under assumption \eqref{suitableDependence},
%for $\omega \in \Omega$ with $M(\omega) = 1$, it is probable that
%$\Gamma_{i}(\omega) = 1$, for most $i=1,\ldots,p$, and consequently
%$\rho(\omega)$ will probably be large.
%Conversely, $\rho(\omega)$ will probably be small if $M(\omega) = 0$.
%Thus,
one is led to the following intuitive approach for predicting $M^{(k)}$:
for each $k\in\{1,\ldots,n\}$,
if we ``observe'' that $\rho(\gamma^{(k)})$ is sufficiently large, then we will ``predict'' that $M^{(k)} = 1$,
and conversely,
if we ``observe'' that $\rho(\gamma^{(k)})$ is sufficiently small, then we will predict that $M^{(k)} = 0$.
However, note that
{\color{red}the logarithm of odd ratios in the definition for $\rho(\gamma^{(k)})$ is unknown},
%\begin{equation*}
%\log\,\dfrac{P(\,\Gamma_{i}=1\,\vert\,M = 1)}{P(\,\Gamma_{i}=1\,\vert\,M = 0)}
%\quad\textnormal{and}\quad
%\log\,\dfrac{P(\,\Gamma_{i}=0\,\vert\,M = 1)}{P(\,\Gamma_{i}=0\,\vert\,M = 0)},
%\end{equation*}
and $\rho(\gamma^{(k)})$ must therefore be estimated based on the observations of $\Gamma^{(k)}$'s somehow.
The Fellegi-Sunter framework for record linkage is roughly the following:
\begin{enumerate}
\item
	Choose ``prediction thresholds'': \;$-\,\infty < \rho_{0} < \rho_{1} < \infty$.
\item
	{\color{red}Estimate} the values of
	\begin{equation*}
	P\!\left(\,
		\Gamma^{(k)}=\gamma^{(k)}\,
		\left\vert\;M^{(k)} = \overset{{\color{white}.}}{1}\right.
		\,\right)
	\quad\textnormal{and}\quad
	P\!\left(\,
		\Gamma^{(k)}=\gamma^{(k)}\,
		\left\vert\;M^{(k)} = \overset{{\color{white}.}}{0}\right.
		\,\right)
	\end{equation*}
	based on the observed values $\gamma^{(k)} \in \{0,1\}^{p}$, for $k = 1,\ldots,n$.
\item
	Subsequently, compute the estimates $\widehat{\rho}\left(\,\gamma^{(k)}\,\right)$
	of $\rho\!\left(\,\gamma^{(k)}\,\right)$, for $k = 1,\ldots,n$.
\item
	For each $k = 1,\ldots, n$, we predict $M^{(k)}$ as follows:
	\begin{equation*}
	\widehat{M}^{(k)}
	\;\;=\;\;
	\left\{\begin{array}{cl}
	0\,, & \textnormal{if \;$\widehat{\rho}\left(\,\gamma^{(k)}\,\right) \;<\; \rho_{0}$},
	\\
	\overset{{\color{white}|}}{1}\,, & \textnormal{if \;$\rho_{1} \;<\; \widehat{\rho}\left(\,\gamma^{(k)}\,\right)$}
	\\
	\overset{{\color{white}|}}{\textnormal{undecided}}\,, & \textnormal{otherwise.}
	\end{array}\right.
	\end{equation*}
\end{enumerate}
The core operational component of the Fellegi-Sunter framework is therefore the estimation of
	\begin{equation*}
	P\!\left(\,
		\Gamma^{(k)}=\gamma^{(k)}\,
		\left\vert\;M^{(k)} = \overset{{\color{white}.}}{1}\right.
		\,\right)
	\quad\textnormal{and}\quad
	P\!\left(\,
		\Gamma^{(k)}=\gamma^{(k)}\,
		\left\vert\;M^{(k)} = \overset{{\color{white}.}}{0}\right.
		\,\right),
	\end{equation*}
based on the observed values $\gamma^{(k)}$'s.
\end{remark}

\vskip 0.5cm
\begin{remark}[The Fellegi-Sunter inference method for $P\!\left(\,\Gamma^{(k)}\,\vert\,M^{(k)}\,\right)$]
\mbox{}\vskip 0.05cm
\noindent
Recall that the observations at hand are the $n \in \N$ vectors $\gamma^{(k)} \in \, \mathcal{G} := \{0,1\}^{p}$, and
we need to estimate the following set of conditional probabilities:
\begin{equation*}
P\!\left(\,
	\Gamma^{(k)}=\gamma^{(k)}\,
	\left\vert\;M^{(k)} = \overset{{\color{white}.}}{1}\right.
	\,\right)
\quad\textnormal{and}\quad
P\!\left(\,
	\Gamma^{(k)}=\gamma^{(k)}\,
	\left\vert\;M^{(k)} = \overset{{\color{white}.}}{0}\right.
	\,\right),
\quad
\textnormal{for $k = 1,2,\ldots,n$}.
\end{equation*}
The Fellegi-Sunter inference method makes the following \textbf{two assumptions}:
\begin{enumerate}
\item
	Independence and identical distribution of sampled pairs:\,
	The collection \,$\left\{\,X^{(k)}\,\right\}_{k=1}^{n}$\, is an
	independent and identically distributed set of $\{0,1\}^{p+1}$-valued
	random variables defined on $\Omega$, where $X^{(k)}$ is defined as follows:
	\begin{equation*}
	X^{(k)} \;=\; \left(\,M^{(k)},\Gamma^{(k)}\,\right) \;:\; \Omega \;\longrightarrow \; \{0,1\}^{p+1}\,,
	\quad
	\textnormal{for \;$k = 1,2,\ldots,n$}.
	\end{equation*}
\item
	Conditional independence of the components of \,$\Gamma^{(k)}$ given $M$:\,
	For each $m = 0,1$ and $k = 1,2,\ldots,n$, we have
	\begin{equation*}
	P\!\left(\,\left.\Gamma^{(k)} = \gamma^{(k)} \,\right\vert\, M = m\,\right)
	\;\; = \;\;
	P\!\left(\,\left.\Gamma^{(k)}_{1} = \gamma^{(k)}_{1} \,\right\vert\, M = m\,\right)
	\times \cdots \times
	P\!\left(\,\left.\Gamma^{(k)}_{p} = \gamma^{(k)}_{p} \,\right\vert\, M = m\,\right),
	\end{equation*}
	where \,$\Gamma^{(k)} = \left(\,\Gamma^{(k)}_{1},\ldots,\Gamma^{(k)}_{p}\,\right)$
	is the representation of \,$\Gamma^{(k)} : \Omega \longrightarrow \{0,1\}^{p}$
	in terms of its component $\{0,1\}$-valued random variables, and
	\,$\gamma^{(k)} = \left(\,\gamma^{(k)}_{1},\ldots,\gamma^{(k)}_{p}\,\right)$
	is the representation of $\gamma^{(k)} \in \{0,1\}^{p}$ in terms of its components.
\end{enumerate}
Under the I.I.D. assumption on $X^{(k)} = \left(\,M^{(k)},\Gamma^{(k)}\,\right)$,
the likelihood of the observed data \,$\left\{\,\Gamma^{(k)} = \gamma^{(k)}\,\right\}_{k=1}^{n}$\,
can be written as:
\begin{eqnarray*}
P\!\left(\;\bigcap_{k=1}^{n} \left\{\,\Gamma^{(k)} = \gamma^{(k)}\right\}\,\right)
& = & \prod_{k=1}^{n} \, P\!\left(\,\Gamma^{(k)} = \gamma^{(k)}\,\right)
\;\; = \;\;
	\prod_{\gamma\in\mathcal{G}} \left(
		\overset{{\color{white}-}}{P}\!\left(\,\Gamma = \gamma\,\right)
	\,\right)^{\color{red}\overset{n}{\underset{k=1}{\textnormal{\normalsize$\sum$}}}\,I\left\{\,\Gamma^{(k)}\,=\,\gamma\,\right\}}
\\
& = &
	\prod_{\gamma\in\mathcal{G}} \left(\;
		\sum_{m=0}^{1}\,P\!\left(\,\Gamma = \gamma\;\vert\,M = m\,\right)\cdot P\!\left(\,M=m\,\right)
	\,\right)^{\overset{n}{\underset{k=1}{\textnormal{\normalsize$\sum$}}}\,I\left\{\,\Gamma^{(k)}\,=\,\gamma\,\right\}},
\end{eqnarray*}
from which we observe that a \textbf{minimal sufficient statistic} is given by:
\begin{equation*}
Y : \Omega \longrightarrow \{0,1,\ldots,n\}^{\mathcal{G}},
\quad\textnormal{where}\;\;\,
Y_{\gamma}(\omega) \; := \; \overset{n}{\underset{k=1}{\sum}}\;I_{\left\{\,\Gamma^{(k)}(\omega) \,=\, \gamma\,\right\}},
\;\;\,\textnormal{for each \,$\gamma\in\mathcal{G} \,:=\, \{0,1\}^{p}$}.
\end{equation*}
Recall:
\begin{itemize}
\item
	$\mathcal{G} \,:=\, \{0,1\}^{p}$\, is the collection of all finite sequences of zeros or ones of length $p$,
	and $\textnormal{Card}\!\left(\mathcal{G}\right) = 2^{p}$.
\item
	$\{0,1,\ldots,n\}^{\mathcal{G}}$ denotes the set of all functions from $\mathcal{G} \longrightarrow \{0,1,\ldots,n\}$;
	in other words, $\{0,1,\ldots,n\}^{\mathcal{G}}$ denotes the collection of all $n^{\textnormal{th}}$-order arrays
	with entries in $\{0,1,2,\ldots,n\}$ and each of whose factors has dimension $2$.
\end{itemize}
The I.I.D. assumption on $X^{(k)} \,=\, \left(\,M^{(k)},\Gamma^{(k)}\,\right)$ furthermore implies that
\,$Y \sim \textnormal{Multinomial}\!\left(\,n\,;\,\left\{\pi_{\gamma}\right\}_{\gamma\in\mathcal{G}}\,\right)$,
where
\begin{equation*}
\pi_{\gamma} \;\; := \;\; P\!\left(\,\Gamma = \gamma\,\right),
\quad\textnormal{for each \,$\gamma\in\mathcal{G}\,:=\,\{0,1\}^{p}$}.
\end{equation*}
In other words, we have
\begin{equation*}
\sum_{\gamma\,\in\,\mathcal{G}}\,Y_{\gamma}(\omega) \; = \; n\,,
\;\,\textnormal{for each \;$\omega\in\Omega$},
\quad\textnormal{and}\quad
\sum_{\gamma\,\in\,\mathcal{G}}\,\pi_{\gamma} \; = \; 1\,,
\end{equation*}
and, for $y = \left\{\,y_{\gamma}\,\right\}_{\gamma\in\mathcal{G}} \in \left\{0,1,\ldots,n\right\}^{\mathcal{G}}$,
\begin{equation*}
P\!\left(\,Y = y\,\right)
\;\; = \;\;
\dfrac{n!}{\overset{{\color{white}.}}{\underset{\gamma\,\in\,\mathcal{G}}{\prod}}\,y_{\gamma}!}
\cdot
\underset{\gamma\,\in\,\mathcal{G}}{\prod}\, \pi_{\gamma}^{y_{\gamma}}
\end{equation*}
Taking natural logarithm of both sides yields:
\begin{equation*}
\log P\!\left(\,Y = y\,\right)
\;\; = \;\;
\underset{\gamma\,\in\,\mathcal{G}}{\textnormal{\large$\sum$}}\;\, y_{\gamma}\cdot\log\!\left(\pi_{\gamma}\right)
\; + \;
\log\!\left(n!\left/\overset{{\color{white}.}}{\underset{\gamma\,\in\,\mathcal{G}}{\prod}}\,y_{\gamma}!\right.\right)
\end{equation*}
On the other hand, the assumption of conditional independence of the component functions of $\Gamma$ given $M$ implies
\begin{eqnarray*}
\pi_{\gamma}
& = & P\!\left(\,\Gamma = \gamma\,\right)
\;\; = \;\;
	P\!\left(\,\Gamma = \gamma\;\vert\;M=1\,\right)\cdot P\!\left(\,M=1\,\right)
	\; + \;
	P\!\left(\,\Gamma = \gamma\;\vert\;M=0\,\right)\cdot P\!\left(\,M=0\,\right)
\\
& = &
	\overset{1}{\underset{m=0}{\sum}}\;
	P\!\left(\,M=m\,\right) \cdot
	\overset{p}{\underset{i=1}{\prod}}\;P\!\left(\,\Gamma_{i} = \gamma_{i}\;\vert\,M=m\,\right)
\end{eqnarray*}
Thus, the full likelihood function given the observed data
\,$y = \left\{\,y_{\gamma}\,\right\}_{\gamma\in\mathcal{G}} \in \{0,1,\ldots,n\}^{\mathcal{G}}$\,
is
\begin{equation*}
\log P\!\left(\,Y = y\,\right)
\;\; = \;\;
\underset{\gamma\,\in\,\mathcal{G}}{\textnormal{\large$\sum$}}\;\, y_{\gamma}\cdot
	\log\!\left[\;
		\overset{1}{\underset{m=0}{\sum}}\;
		{\color{red}P\!\left(\,M=m\,\right)} \cdot
		\overset{p}{\underset{i=1}{\prod}}\,{\color{red}P\!\left(\,\Gamma_{i} = \gamma_{i}\;\vert\,M=m\,\right)}
	\;\right]
\; + \;
\log\!\left(n!\left/\overset{{\color{white}.}}{\underset{\gamma\,\in\,\mathcal{G}}{\prod}}\,y_{\gamma}!\right.\right),
\end{equation*}
where \,$P\!\left(\,M=m\,\right)$\, and \,$P\!\left(\,\Gamma_{i} = \gamma_{i}\;\vert\,M=m\,\right)$,\,
$m \in \{0,1\}$, $\gamma_{i} \in \{0,1\}$, $i\in\{1,\ldots,p\}$,
are model parameters to be estimated.
\end{remark}

%\begin{equation*}
%\rho(\omega)
%\;\; = \;\;
%\rho\!\left(\,\Gamma_{1}(\omega),\ldots,\Gamma_{p}(\omega)\,\right)
%\;\; := \;\;
%\overset{p}{\underset{i=1}{\sum}}\;
%\left(\log\,\dfrac{P(\,\Gamma_{i}=1\,\vert\,M = 1)}{P(\,\Gamma_{i}=1\,\vert\,M = 0)}\,\right)^{\Gamma_{i}(\omega)}
%\cdot
%\left(\log\,\dfrac{P(\,\Gamma_{i}=0\,\vert\,M = 1)}{P(\,\Gamma_{i}=0\,\vert\,M = 0)}\,\right)^{1\,-\,\Gamma_{i}(\omega)}
%\end{equation*}

\begin{equation}
\textnormal{For each $i \in\left\{\,1,\ldots,p\,\right\}$}\,,
\quad
P(\,\Gamma_{i}=1\,\vert\,M = 1)\;\;\,\textnormal{and}\;\;\,P(\,\Gamma_{i}=0\,\vert\,M = 0)\;\;\textnormal{are large}.
\end{equation}
Under the assumption \eqref{suitableDependence}, we would have:
\begin{equation*}
P(\,\Gamma_{i}=1\,\vert\,M = 0)\;\;\,\textnormal{and}\;\;\,P(\,\Gamma_{i}=0\,\vert\,M = 1)\;\;\textnormal{are small},
\quad
\textnormal{for $i \in\left\{\,1,\ldots,p\,\right\}$},
\end{equation*}
hence, we would also have:
\begin{equation*}
\textnormal{for each $i \in\left\{\,1,\ldots,p\,\right\}$},
\quad
\dfrac{P(\,\Gamma_{i}=1\,\vert\,M = 1)}{P(\,\Gamma_{i}=1\,\vert\,M = 0)}\;\;\textnormal{is large}
\quad\textnormal{and}\quad
\dfrac{P(\,\Gamma_{i}=0\,\vert\,M = 1)}{P(\,\Gamma_{i}=0\,\vert\,M = 0)}\;\;\textnormal{is small}.
\end{equation*}

\begin{equation*}
P\!\left(\Gamma_{i}=1\,\vert\,M=1\right)
\;\; := \;\;
	\dfrac{P\!\left(\Gamma_{i}=1,\,M=1\right)}{P\!\left(\,M=1\,\right)}
\;\; = \;\;
	\dfrac{\mu\!\left(\left\{\;\left.\omega\overset{{\color{white}.}}{\in}\Omega\,\;\right\vert\;\Gamma_{i}(\omega)=1,\,M(\omega)=1\,\right\}\right)}
	{\mu\!\left(\left\{\;\left.\omega\overset{{\color{white}.}}{\in}\Omega\;\,\right\vert\;M(\omega)=1\,\right\}\right)}
\end{equation*}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
