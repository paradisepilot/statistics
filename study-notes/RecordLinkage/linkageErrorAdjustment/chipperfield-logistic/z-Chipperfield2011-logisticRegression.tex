
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Chipperfield \textit{et al.}, 2011, \cite{Chipperfield2011}}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%%
           ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{MLE for logistic regression with multivariate Bernoulli predictor variables}

The precise mathematical formulation of the discussion in
Section 2.2 (entitled ``Logistic regression'') in \cite{Chipperfield2011}
is the following proposition.

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space\,,
	\,and
	\,$n \in \N$\, is a natural number.
\item
	$Y^{(1)}, \ldots, Y^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$\,
	are binary random variables.
\end{itemize}
Suppose:
\begin{itemize}
\item
	The binary random variables \,$Y^{(i)}$, $i = 1, 2, \ldots, n$,\,
	are independent (but NOT necessarily identically distributed).
\item
	There exist
	\,$\beta \in \Re^{p}$\,
	and
	\,$x^{(1)}, x^{(2)}, \ldots, x^{(n)} \in \Re^{p}$\,,
	such that
	\begin{equation*}
	\pi_{i}
	\;\; := \;\;
		P\!\left(\; Y^{(i)} = 1 \;\right)
	\;\; = \;\;
		E\!\left[\; Y^{(i)}  = 1 \;\right]
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\,\beta^{T} \cdot x^{(i)} \,\right)
			}{
			1 \,+\, \exp\!\left(\,\beta^{T} \cdot x^{(i)} \,\right)
			}\,,
	\quad
	\textnormal{for each \,$i = 1,2,\ldots,n$}\,.
	\end{equation*}
\end{itemize}
Then, the following statements are true:
\begin{enumerate}
\item
	The logarithm of the joint probability distribution of
	\;$Y^{(1)},Y^{(2)},\ldots,Y^{(n)}$\, is given by:
	\begin{eqnarray*}
	\log\,L
		&=&
		\log\,P\!\left(\; Y^{(1)}=y_{1},\,Y^{(2)}=y_{2},\,\ldots,\;Y^{(n)}=y_{n} \;\right)
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot(\,\beta^{T} \cdot x^{(i)}\,)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp(\,\beta^{T} \cdot x^{(i)}\,)\,\right)
			\;\right\}
	\end{eqnarray*}
\item
	The (vectorial) score equation is given by:
	\begin{equation*}
	X \cdot \left(\, \pi \,-\, y\,\right) \;\; = \;\; 0_{p} \;\; \in \;\; \Re^{p}\,,
	\end{equation*}
	where $X \in \Re^{p \times n}$, $\pi \in \Re^{n}$, and $y \in \Re^{n}$ are respectively defined by
	\begin{eqnarray*}
	X
	&=&
		\left(\;\;\overset{{\color{white}.}}{X}_{ki}\;\;\right)
	\;\;:=\;\;
		\left(\;\; x^{(i)}_{k} \;\;\right)
	\;\;=\;\;
		\left(\;\; x^{(1)} \quad x^{(2)} \quad \cdots \quad x^{(n)} \;\;\right) \in \Re^{p \times n}\,,
	\end{eqnarray*}
	\begin{equation*}
	y \,:=\, \left(\;y_{1},\ldots,y_{n}\;\right)^{T} \in \Re^{n \times 1}\,, 
	\quad\quad
	\textnormal{and}	
	\quad\quad
	\pi \,:=\, \left(\;\pi_{1},\ldots,\pi_{n}\;\right)^{T} \in \Re^{n \times 1}\,.
	\end{equation*}
	The score equation can be given equivalently in component form by:
	\begin{equation*}
	\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \left(\,\pi_{i} - y_{i}\,\right)
	\;\; = \;\; 0\,,
	\quad
	\textnormal{for each \,$k = 1, 2,\ldots,p$}\,;
	\end{equation*}
	or equivalently but more explicitly,
	\begin{equation*}
	\overset{n}{\underset{i=1}{\sum}}\;\,
	\dfrac{x^{(i)}_{k}}{1 \,+\, \exp\!\left({\color{red}-}\;\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)}
	\;\; = \;\;
	\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}\,,
	\quad
	\textnormal{for each \,$k = 1, 2,\ldots,p$}\,.
	\end{equation*}
\end{enumerate}
\end{proposition}
\proof
\begin{enumerate}
\item
	First, note that
	\begin{equation*}
	\dfrac{\pi_{i}}{1 - \pi_{i}}
	\;\; = \;\;
		\dfrac{\exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}{1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}
		\cdot
		\dfrac{1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)}{1}
	\;\; = \;\;
		\exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)
	\end{equation*}
	Hence,
	\begin{eqnarray*}
	\log\,L
	&=&
		\log\,P\!\left(\;Y^{(1)} = y_{1},\,Y^{(2)} = y_{2},\,\ldots\,,\,Y^{(n)} = y_{n}\;\right)
	\\
	&=&
		\log\!\left(\;\,
			\overset{n}{\underset{i=1}{\prod}}\;\, \pi_{i}^{y_{i}} \cdot \left(1 - \pi_{i}\right)^{1-y_{i}}
			\,\right)
	\;\;=\;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\log\,\pi_{i}
			\, \overset{{\color{white}\vert}}{+} \,
			(1-y_{i})\cdot\log\!\left(1 - \pi_{i}\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\log\left(\,\dfrac{\pi_{i}}{1 - \pi_{i}}\,\right)
			\, \overset{{\color{white}\vert}}{+} \,
			\log\!\left(1 - \pi_{i}\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left(\;
			y_{i}\cdot\left(\,\beta^{T} \cdot x^{(i)}\,\right)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp\!\left(\,\beta^{T} \cdot x^{(i)}\,\right)\,\right)
			\;\right)	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
			\, \overset{{\color{white}\vert}}{-} \,
			\log\!\left(\,1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)\,\right)
			\;\right\}	
	\end{eqnarray*}
\item
	The partial derivative of $\log\,L$ with respect to $\beta_{k}$ is:
	\begin{eqnarray*}
	\dfrac{\partial\,\log L}{\partial\,\beta_{k}}
	&=&
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\,x^{(i)}_{k}
			\; \overset{{\color{white}\vert}}{-} \;
			\dfrac{
				\exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}{
				1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
				}
			\cdot
			x^{(i)}_{k}
			\;\right\}	
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\,
		\left\{\;
			y_{i}\cdot\,x^{(i)}_{k}
			\; \overset{{\color{white}\vert}}{-} \;
			\pi_{i} \cdot x^{(i)}_{k}
			\;\right\}	
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}}\;\;
		x^{(i)}_{k}	
		\cdot
		\left(\; y_{i} \; \overset{{\color{white}.}}{-} \; \pi_{i} \;\right)
	\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
		\; - \;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \pi_{i}
	\end{eqnarray*}
	Thus, the score equations are
	\begin{eqnarray*}
	\dfrac{\partial\,\log L}{\partial\,\beta_{k}} \;=\; 0
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot \pi_{i}
		\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
	\\
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot
		\dfrac{
			\exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
			}{
			1 + \exp\!\left(\,\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)
			}
		\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
	\\
	&\quad\Longleftrightarrow\quad&
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot
		\dfrac{1}{1 \,+\, \exp\!\left({\color{red}-}\;\overset{n}{\underset{l=1}{\sum}}\;\beta_{l}\,x^{(i)}_{l}\,\right)}
		\;\; = \;\;
		\overset{n}{\underset{i=1}{\sum}}\;\, x^{(i)}_{k} \cdot y_{i}
	\end{eqnarray*}
	for each $k = 1,2,\ldots,p$.
\end{enumerate}
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\subsection{MLE for logistic regression with binary predictor variables,
in the presence of measurement errors,
with adjustments for measurement errors via clerical review on a subsample of collected data}

\begin{comment}
\vskip 0.3cm
In this subsection, we attempt to give the probability framework underlying the discussion
in Section 3.2, \cite{Chipperfield2011}.
In particular, we attempt to give expressions for 
\begin{itemize}
\item
	the complete-data log likelihood involved in the application
	of the Expection-Maximization (EM) algorithm discussed in
	Section 3.2, \cite{Chipperfield2011}, as well as
\item
	the conditional expectations that need to be computed during the Expectation step therein.
\end{itemize}
Note that \cite{Chipperfield2011} does NOT give an explicit
expression for the likelihood function being maximized, and simply
asserts that the solution to the Maximization step can be given by
Equation (7) in \cite{Chipperfield2011}.
We remark, however, that the expression for the complete-data log likelihood
contained in Theorem \ref{ThmLogisticRegressionWithIncorrectData} (i) below
suggests that the solution to the Maximization step of the EM algorithm
in Section 3.1, \cite{Chipperfield2011} might be significantly more complicated than
Equation (7) in \cite{Chipperfield2011}.
\end{comment}

\vskip 0.3cm
\noindent
In Theorem \ref{ThmLogisticRegressionWithIncorrectData} below,
$n \in \N$ denotes the sample size.
For $i \in \{1,2,\ldots,n\}$, the interpretations of the random variables
$X^{(i)}$, $Y^{(i)}$, $\widetilde{Y}^{(i)}$, $M^{(i)}$ and $J^{(i)}$
are explained in the following table:
\begin{center}
\begin{tabular}{|c|c|l|}
\hline
$\overset{{\color{white}.}}{x^{(i)}}$ & observed & multivariate predictor variable with binary component variables
\\ \hline
$\overset{{\color{white}.}}{Y^{(i)}}$ & unobserved & (true) binary response variable
\\ \hline
$\overset{{\color{white}.}}{\widetilde{Y}^{(i)}}$ & observed & observed binary response variable (may contain measurement errors)
\\ \hline
$M^{(i)}$ & & binary variable indicating whether clerical review confirms correctness of $\overset{{\color{white}.}}{\widetilde{Y}^{(i)}}$
\\ \hline
$\overset{{\color{white}.}}{J^{(i)}}$ & & binary variable indicating whether observation $i$ is selected for clerical review
\\ \hline
&& \\
$K^{(i)}$ & observed & $K^{(i)} \;:=\;
	\left\{\begin{array}{cl}
		0, & \textnormal{if}\;\; J^{(i)} = 0,
		\\
		1, & \textnormal{if}\;\; J^{(i)} = 1, \;\;\textnormal{and}\;\; M^{(i)} = 0,
		\\
		2, & \textnormal{if}\;\; J^{(i)} = 1, \;\;\textnormal{and}\;\; M^{(i)} = 1
	\end{array}\right.$
\\
&& \\
\hline
\end{tabular}
\end{center}

\vskip0.5cm
\begin{remark}
\mbox{}\vskip 0.1cm\noindent
In Theorem \ref{ThmLogisticRegressionWithIncorrectData} below,
we assume that the observations are independent, and
we make certain independence assumptions on the random variables.
Apart from these, the main non-trivial assumption we make is the following:
\begin{equation*}
P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,X^{(i)}=x,\widetilde{Y}^{(i)}=y,M^{(i)}=0\,\right)
\;\; = \;\;
P\!\left(\left.Y^{(i)} = c\,\;\right\vert\,X^{(i)}=x\,\right)\,,
\;\;
\textnormal{for each \,$i\in\{1,\ldots,n\}$}\,,
\end{equation*}
which amounts to the assumption that,
\textbf{\color{red}if clerical review fails to confirm correctness of $\widetilde{Y}^{(i)}$,
then the conditional distribution of $Y^{(i)}$ (given $X^{(i)}$, $\widetilde{Y}^{(i)}$ and $M^{(i)}=0$)
simply reverts back to that of the no-measurement-error scenario,
namely $P\!\left(\,\left.Y^{(i)}=c\,\right\vert\,X^{(i)}=x\,\right)$}.
\end{remark}

\vskip 0.5cm
\begin{theorem}\label{ThmLogisticRegressionWithIncorrectData}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\left(\Omega,\mathcal{A},\mu\right)$ is a probability space.
	\,
	$n \in \N$ is a natural number.
\item
	$Y^{(1)}, \,\ldots\,,\, Y^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
	are binary random variables.
\item
	$\widetilde{Y}^{(1)}, \,\ldots\,,\, \widetilde{Y}^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
	are binary random variables.
\item
	$X^{(1)}, \,\ldots\,,\, X^{(n)} : \Omega \longrightarrow \{0,1\}^{p}$
	are ${\color{red}\{0,1\}^{p}}$-valued random variables.
\item
	$M^{(1)}, \,\ldots\,,\, M^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
	are Bernoulli random variables.
\item
	$J^{(1)}, \,\ldots\,,\, J^{(n)} : \Omega \longrightarrow \left\{0,1\right\}$
	are Bernoulli random variables.
\end{itemize}
Assume:
\renewcommand{\theenumi}{\alph{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\begin{enumerate}
\item
	There exists $\beta \in \Re^{p}$ such that
	\begin{equation*}
	%\pi_{i \vert x}
	%\;\; := \;\;
		P\!\left(\; Y^{(i)} = 1 \,\left\vert\, X^{(i)} = x \right.\,\right)
	\;\; = \;\;
		E\!\left[\; Y^{(i)}  = 1 \,\left\vert\, X^{(i)} = x \right.\,\right]
	\;\; = \;\;
		\dfrac{
			\exp\!\left(\,\beta^{T} \cdot x \,\right)
			}{
			1 \,+\, \exp\!\left(\,\beta^{T} \cdot x \,\right)
			}
	\;\; = \;\;
		\logisticBetaX\,,
	\quad
	\end{equation*}
	for each $x \in \{0,1\}^{p}$ and for each \,$i = 1,2,\ldots,n$.
\item\label{iIndependence}
	The $\left(\left\{0,1\right\} \overset{{\color{white}.}}{\times} \left\{0,1\right\}^{p} \times \left\{0,1\right\} \times \left\{0,1\right\}\right)$-valued
	random variables
	\,$Z^{(i)} := \left(\,Y^{(i)},X^{(i)},\widetilde{Y}^{(i)},M^{(i)}\,\right)$,
	\,$i \in \{1, \ldots, n\}$,
	\,are independent and identically distributed.
\item\label{Jindependence}
	The Bernoulli random variables \,$J^{(i)}$, \,$i \in \{1, \ldots, n\}$, \,are independent and identically distributed.
\item\label{ZJindependence}
	The two collections of random variables
	\,$\left\{\,Z^{(i)} := \left(\,Y^{(i)},X^{(i)},\widetilde{Y}^{(i)},M^{(i)}\right)\,\right\}_{i=1}^{n}$\,
	and
	\,$\left\{\,J^{(i)}\,\right\}_{i=1}^{n}$\,
	are independent, in the sense that, for all
	\,$z_{1}, \ldots, z_{n} \in \left\{0,1\right\}^{p+3}$\, and \,$j_{1}, \ldots, j_{n} \in \{0,1\}$,\,
	\begin{eqnarray*}
	&&
		P\!\left(\,Z^{(1)}=z_{1},\,\ldots\,,Z^{(n)}=z_{n},\;J^{(1)}=j_{1},\,\ldots\,,J^{(n)}=j_{n}\,\right)
	\\
	& = &
		P\!\left(\,Z^{(1)}=z_{1},\,\ldots\,,Z^{(n)}=z_{n}\,\right)
		\cdot
		P\!\left(\,J^{(1)}=j_{1},\,\ldots\,,J^{(n)}=j_{n}\,\right)
	\end{eqnarray*}
\item\label{MzeroImplies}
	For each \,$i\in\{1,2,\ldots,n\}$, \,$c\in\{0,1\}$, \,$x \in \{0,1\}^{p+3}$, and \,$y \in \{0,1\}$, \,we have:
	\begin{equation*}
	P\!\left(\left.Y^{(i)} = c\,\;\right\vert X^{(i)}=x,\widetilde{Y}^{(i)}=y,M^{(i)}=0\,\right)
	\;\; = \;\;
	P\!\left(\;Y^{(i)} = c\;\left\vert\,X^{(i)}=x\right.\,\right)\,.
	\end{equation*}
\item\label{MoneImpliesCEqualsY}
	For each $i\in\{1,2,\ldots,n\}$, we have:
	\begin{equation*}
	M^{(i)} \; = \; 1
	\quad\Longrightarrow\quad
	Y^{(i)} \; = \; \widetilde{Y}^{(i)},
	\end{equation*}
	which, in particular, implies $P\!\left(\,\left.Y^{(i)} = \widetilde{Y}^{(i)}\;\right\vert\;M^{(i)}=1\,\right)\,=\,1$.
\end{enumerate}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
%Define, for each $i \in \{1,2,\ldots,n\}$, the random variable
%\;$K^{(i)} \;:\; \Omega \; \longrightarrow \; \{0,1,2\}$\; by
%\begin{equation*}
%K^{(i)} \;\;=\;\;
%	\left\{\begin{array}{cl}
%	0, & \textnormal{if}\;\; J^{(i)} = 0,
%	\\
%	1, & \textnormal{if}\;\; J^{(i)} = 1 \;\;\textnormal{and}\;\; M^{(i)} = 0,
%	\\
%	2, & \textnormal{if}\;\; J^{(i)} = 1 \;\;\textnormal{and}\;\; M^{(i)} = 1
%	\end{array}\right.
%\end{equation*}
Define, for each
\;$i \in \{1,2,\ldots,n\}$,
\;$c,y \in \{0,1\}$,
\;$x \in \{0,1\}^{p}$,
\;and
\;$m,j,k \in \{0,1\}$,
\;the following:
\begin{equation*}
\begin{array}{ccll}
	K^{(i)}
		&:=&
		\left\{\begin{array}{cl}
		0, & \textnormal{if}\;\; J^{(i)} = 0,
		\\
		1, & \textnormal{if}\;\; J^{(i)} = 1 \;\;\textnormal{and}\;\; M^{(i)} = 0,
		\\
		2, & \textnormal{if}\;\; J^{(i)} = 1 \;\;\textnormal{and}\;\; M^{(i)} = 1
		\end{array}\right.
	\\
	W^{(i)}_{c \vert xyk}
		&:=&
		\overset{\textnormal{\large\color{white}1}}{I}\!\left\{\,Y^{(i)}=c,X^{(i)}=x,\widetilde{Y}^{(i)}=y,K^{(i)}=k\,\right\}\,,
	\\
	\pi_{c \vert x}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\left.\overset{{\color{white}.}}{Y}=c\;\right\vert X=x\,\right)\,,
	\\
	\mu_{m \vert xy}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\left.\overset{{\color{white}.}}{M}=m\;\right\vert X=x, \widetilde{Y}=y\,\right)\,,
	\\
	\nu_{xy}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\overset{{\color{white}.}}{X}=x, \widetilde{Y}=y\,\right)\,,
		\quad\textnormal{and}
	\\
	\omega_{j}
		&\overset{{\color{white}\vert}}{:=}&
		P\!\left(\,\overset{{\color{white}.}}{J}=j\,\right)\,,
	\\
	\overset{\textnormal{\Large\color{white}1}}{\delta}_{cy}
		&\overset{{\color{white}\vert}}{:=}&
		\left\{\begin{array}{cl}
			1, & \textnormal{if}\;\; c = y\,,
			\\
			0, & \textnormal{otherwise}\,.
		\end{array}\right.
\end{array}
\end{equation*}
Note that the $K^{(i)}$'s and the $W^{(i)}_{c \vert xyk}$'s
are categorical random variables defined on the underlying probability space $\Omega$,
while $\pi_{c \vert x}$, $\mu_{m \vert xy}$, $\nu_{xy}$, $\omega_{j}$ are parameters,
whose well-definition follows from the independence hypotheses
\eqref{iIndependence}, \eqref{Jindependence} and \eqref{ZJindependence}.
\vskip 0.3cm
\noindent
Then, the following statements are true:
\begin{enumerate}
\item
	The logarithm of the joint probability distribution of
	\;$Y^{(i)}$,\,
	$X^{(i)}$,\,
	$\widetilde{Y}^{(i)}$,\,
	$K^{(i)}$,\,
	for \;$i \in \{1,2,\ldots,n\}$,\;
	is given by:
	\begin{eqnarray*}
	\log\,L
		&=&
		\log\,P\!\left(\;
			\overset{n}{\underset{i=1}{\bigcap}}\;
			\left\{
				Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}
			\right\}
		\;\right)
	\\
	&=&
		{\color{white}+} \;\;
		\overset{1}{\underset{c=0}{\sum}}\;\;
		\underset{x\in\{0,1\}^{p}}{\sum}\;\;
		\overset{1}{\underset{y=0}{\sum}}\;
		\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}0}}\right)
		\cdot
		\left(\,
			\log\left(\pi_{c \vert x}\,\mu_{0 \vert xy} + \delta_{cy}\,\mu_{1\vert xy}\right)
			\,\overset{{\color{white}.}}{+}\,
			\log\,\nu_{xy}
			\,\overset{{\color{white}.}}{+}\,
			\log\,\omega_{0}
		\,\right)
	\\
	&&
		+ \;\;
		\overset{1}{\underset{c=0}{\sum}}\;\;
		\underset{x\in\{0,1\}^{p}}{\sum}\;\;
		\overset{1}{\underset{y=0}{\sum}}\;
		\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}1}}\right)
		\cdot
		\left(\,
			\log\,\pi_{c \vert x}
			\,\overset{{\color{white}.}}{+}\,
			\log\,\mu_{0\vert xy}
			\,\overset{{\color{white}.}}{+}\,
			\log\,\nu_{xy}
			\,\overset{{\color{white}.}}{+}\,
			\log\,\omega_{1}
		\,\right)
	\\
	&&
		+ \;\;
		\overset{1}{\underset{y=0}{\sum}}\;\;
		\underset{x\in\{0,1\}^{p}}{\sum}\,
		\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert xy{\color{red}2}}\right)
		\cdot
		\left(\,
			\log\,\mu_{1 \vert xy}
			\,\overset{{\color{white}.}}{+}\,
			\log\,\nu_{xy}
			\,\overset{{\color{white}.}}{+}\,
			\log\,\omega_{1}
		\,\right)
	\end{eqnarray*}
\item
	The conditional expectation values of
	\;$\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert gy{\color{red}0}}$\,,
	\;$\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert gy{\color{red}1}}$\,,
	\;and
	\;$\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert gy{\color{red}2}}$\,,
	given $X^{(a)}$, $\widetilde{Y}^{(a)}$, $K^{(a)}$, for $a \in \{1,\ldots,n\}$,
	with respect to the parameters
	\;$\pi_{c \vert g}$\,, \,$\mu_{m \vert gy}$\,, \,$\nu_{gy}$\,, \,$\omega_{j}$\,,
	\;for
	\;$c,y\in[\,C\,], \; g\in[\,G\,], \; m,j\in\{0,1\}$\,,
	\;can be expressed as follows:
	\begin{eqnarray*}
	E\!\left[\;\,
		\left.
		\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}0}}
		\,\;\right\vert
		\EMConditions
	\right]
		&=&
		N_{gy{\color{red}0}}\cdot\left(\,\pi_{c \vert g}\,\mu_{0 \vert gy} \,+\, \delta_{cy}\,\mu_{1 \vert gy}\,\right)\,,
	\\ \\
	E\!\left[\;\,
		\left.
		\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}1}}
		\,\;\right\vert
		\EMConditions
	\right]
		&=&
		N_{gy{\color{red}1}}\cdot\pi_{c \vert g}\,,
	\\ \\
	E\!\left[\;\,
		\left.
		\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert gy{\color{red}2}}
		\,\;\right\vert
		\EMConditions
	\right]
		&=&
		N_{gy{\color{red}2}}\,,
	\end{eqnarray*}
	where \;$N_{gy{\color{red}k}}$ $:=$
	$\overset{n}{\underset{i=1}{\sum}}\,I\!\left\{\,X^{(i)}=g,\,\widetilde{Y}^{(i)}=y,\,K^{(i)}={\color{red}k}\,\right\}$,
	for $k \in \{0,1,2\}$.
\end{enumerate}
\end{theorem}
\proof
\begin{enumerate}
\item
The joint probability distribution function of
\,$Y^{(i)}$, $X^{(i)}$, $\widetilde{Y}^{(i)}$, $K^{(i)}$,
\,for \,$i \in \{1,\ldots,n\}$,
\,is given by:
\begin{comment}
\begin{eqnarray*}
L &=&
	P\!\left(\;
		\overset{n}{\underset{i=1}{\bigcap}}\;
		\left\{
			Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}
		\right\}
	\;\right)
\\
&=&
	\overset{n}{\underset{i=1}{\prod}} \;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}\,\right)
\\
&=&
	{\color{white}\times}\;
	\underset{i\,\in\,K_{\color{red}0}}{\prod} \;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}={\color{red}0}\,\right)
\\
&&
	\times\;
	\underset{i\,\in\,K_{\color{red}1}}{\prod} \;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}={\color{red}1}\,\right)
\\
&&
	\times\;
	\underset{i\,\in\,K_{\color{red}2}}{\prod} \;
	P\!\left(\,Y^{(i)}={\color{red}y_{i}},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}={\color{red}y_{i}},K^{(i)}={\color{red}2}\,\right)
\end{eqnarray*}

\begin{eqnarray*}
\log\,L
&=&
	{\color{white}+}\;
	\underset{i\,\in\,K_{\color{red}0}}{\sum} \;
	\log \;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}={\color{red}0}\,\right)
\\
&&
	+\;
	\underset{i\,\in\,K_{\color{red}1}}{\sum} \;
	\log\;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}={\color{red}1}\,\right)
\\
&&
	+\;
	\underset{i\,\in\,K_{\color{red}2}}{\sum} \;
	\log\;
	P\!\left(\,Y^{(i)}={\color{red}y_{i}},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}={\color{red}y_{i}},K^{(i)}={\color{red}2}\,\right)
\end{eqnarray*}

\begin{eqnarray*}
P\!\left(\,Y=c,X=x,\widetilde{Y}=y,K={\color{red}0}\,\right)
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y,K=0\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y,K=0\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y,J=0\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y,J=0\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	\nu_{xy}%P\!\left(\,X=x,\widetilde{Y}=y\,\right)
\end{eqnarray*}

\begin{eqnarray*}
P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
&=&
	P\!\left(\, Y=c,M=1 \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
	\;+\;
	P\!\left(\, Y=c,M=0 \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
\\
&=&
	{\color{white}+}\;
	P\!\left(\, Y=c \;\left\vert\; M=1, X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\, M=1 \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
\\
&&
	+\;
	P\!\left(\, Y=c \;\left\vert\; M=0,X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\, M=0 \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
\\
&=&
	{\color{white}+}\;
	\delta_{cy}
	\cdot
	\mu_{1 \vert x y} %P\!\left(\, M=1 \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
	\;+\;
	P\!\left(\, Y=c \;\left\vert\; \overset{{\color{white}.}}{X}=x \right.\,\right)
	\cdot
	\mu_{0 \vert x y} %P\!\left(\, M=0 \;\left\vert\; X=x,\widetilde{Y}=y\right.\,\right)
\end{eqnarray*}

\begin{eqnarray*}
P\!\left(\,Y=c,X=x,\widetilde{Y}=y,K={\color{red}1}\,\right)
&=&
	P\!\left(\,Y=c,X=x,\widetilde{Y}=y,J=1,M=0\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y,J=1,M=0\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y,J=1,M=0\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x,\widetilde{Y}=y,M=0\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y,M=0\,\right)
	\cdot
	P\!\left(\,J=1\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x\right.\,\right)
	\cdot
	P\!\left(\,M=0 \,\left\vert\, X=x,\widetilde{Y}=y \right.\,\right) 
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y\,\right) 
	\cdot
	P\!\left(\,J=1\,\right)
\\
&=&
	P\!\left(\,Y=c \;\left\vert\; X=x\right.\,\right)
	\cdot
	\mu_{0 \vert xy} %P\!\left(\,M=0 \;\left\vert\; X=x,\widetilde{Y}=y \right.\,\right) 
	\cdot
	\nu_{xy} %P\!\left(\,X=x,\widetilde{Y}=y\,\right) 
	\cdot
	\omega_{1} %P\!\left(\,J=1\,\right)
\end{eqnarray*}

\begin{eqnarray*}
P\!\left(\,Y=y,X=x,\widetilde{Y}=y,K={\color{red}2}\,\right)
&=&
	P\!\left(\,Y=y,X=x,\widetilde{Y}=y,J=1,M=1\,\right)
\\
&=&
	P\!\left(\,Y=y \;\left\vert\; X=x,\widetilde{Y}=y,J=1,M=1\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y,J=1,M=1\,\right)
\\
&=&
	P\!\left(\,Y=y \;\left\vert\; X=x,\widetilde{Y}=y,M=1\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y,M=1\,\right)
	\cdot
	P\!\left(\,J=1\,\right)
\\
&=&
	1
	\cdot
	P\!\left(\,M=1 \,\left\vert\, X=x,\widetilde{Y}=y \right.\,\right) 
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y\,\right) 
	\cdot
	P\!\left(\,J=1\,\right)
\\
&=&
	\mu_{1 \vert xy} %P\!\left(\,M=0 \;\left\vert\; X=x,\widetilde{Y}=y \right.\,\right) 
	\cdot
	\nu_{xy} %P\!\left(\,X=x,\widetilde{Y}=y\,\right) 
	\cdot
	\omega_{1} %P\!\left(\,J=1\,\right)
\end{eqnarray*}
\end{comment}

\begin{eqnarray*}
L &=&
	P\!\left(\;
		\overset{n}{\underset{i=1}{\bigcap}}\;
		\left\{
			Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}
		\right\}
	\;\right)
\\
&=&
	\overset{n}{\underset{i=1}{\prod}} \;
	P\!\left(\,Y^{(i)}=c_{i},X^{(i)}=x^{(i)},\widetilde{Y}^{(i)}=y_{i},K^{(i)}=k_{i}\,\right)
\\
&=&
	\overset{n}{\underset{i=1}{\prod}} \;
	\left\{\;
		\overset{1}{\underset{c=0}{\prod}}
		\,\;\underset{x\in\{0,1\}^{p}}{\prod}\;\,
		\overset{1}{\underset{y=0}{\prod}}\;\;
		\overset{2}{\underset{k=0}{\prod}}\;
		P\!\left(\,Y=c,X=x,\widetilde{Y}=y,K=k\,\right)^{W^{(i)}_{c \vert xyk}}
	\;\right\}
\\
&=&
	\overset{1}{\underset{c=0}{\prod}}
	\,\;\underset{x\in\{0,1\}^{p}}{\prod}\;\,
	\overset{1}{\underset{y=0}{\prod}}\;\;
	\overset{2}{\underset{k=0}{\prod}}\;
	\left\{\;
		\overset{n}{\underset{i=1}{\prod}} \;
		P\!\left(\,Y=c,X=x,\widetilde{Y}=y,K=k\,\right)^{W^{(i)}_{c \vert xyk}}
	\;\right\}
\\
&=&
	\overset{1}{\underset{c=0}{\prod}}
	\,\;\underset{x\in\{0,1\}^{p}}{\prod}\;\,
	\overset{1}{\underset{y=0}{\prod}}\;\;
	\overset{2}{\underset{k=0}{\prod}}\;
	\left\{\;
		P\!\left(\,Y=c,X=x,\widetilde{Y}=y,K=k\,\right)
		^{\overset{n}{\underset{i=1}{\sum}}\,W^{(i)}_{c \vert xyk}}
	\;\right\}\,.
\end{eqnarray*}
The logarithm of this joint probability distribution function is thus:
\begin{eqnarray*}
\log\,L &=&
	\overset{1}{\underset{c=0}{\sum}}
	\,\;\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;\;
	\overset{2}{\underset{k=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xyk}\right)
	\cdot
	\log\,P\!\left(\,Y=c,X=x,\widetilde{Y}=y,K=k\,\right)
\\
&=&
	\overset{1}{\underset{c=0}{\sum}}
	\,\;\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;\;
	\overset{2}{\underset{k=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xyk}\right)
	\cdot
	\log\left[\,
		P\!\left(\,Y=c\;\left\vert\;X=x,\widetilde{Y}=y,K=k\right.\,\right)
		P\!\left(\,X=x,\widetilde{Y}=y,K=k\,\right)
	\,\right]
\\
&=&
	\overset{1}{\underset{c=0}{\sum}}
	\,\;\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;\;
	\overset{2}{\underset{k=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xyk}\right)
	\cdot
	\log\left[\,
		\pi_{c \vert xyk} \overset{{\color{white}\vert}}{\cdot} p_{xyk}
	\,\right]
\\
&=&
	{\color{white}+} \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\,\;\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}0}}\right)
	\cdot
	\log\left[\,
		\pi_{c \vert xy\color{red}0} \cdot p_{xy\color{red}0}
	\,\right]
\\
&&
	+ \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\,\;\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}1}}\right)
	\cdot
	\log\left[\,
		\pi_{c \vert xy\color{red}1} \cdot p_{xy\color{red}1}
	\,\right]
\\
&&
	+ \;\;
	\overset{1}{\underset{y=0}{\sum}}\;
	\,\;\underset{x\in\{0,1\}^{p}}{\sum}\,
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert xy{\color{red}2}}\right)
	\cdot
	\log\left[\,
		\pi_{y \vert xy\color{red}2} \cdot p_{xy\color{red}2}
	\,\right]\,,
\end{eqnarray*}
where
\;$\pi_{c \vert xyk} \, := \, P\!\left(\,Y=c\;\left\vert\;X=x,\widetilde{Y}=y,K=k\right.\,\right)$,
\;$p_{xyk} \, := \, P\!\left(\,X=x,\widetilde{Y}=y,K=k\,\right)$,
\;for
\;$x \in \left\{0,1\right\}^{p}$,
\;$y \in \left\{0,1\right\}$,
\,$k \in \left\{0,1,2\right\}$.
\;Note that the last equality above follows from
\begin{eqnarray*}
W^{(i)}_{c \vert xy{\color{red}2}}
&:=&
	I\!\left\{\,Y^{(i)}=c,X^{(i)}=x,\widetilde{Y}^{(i)}=y,K^{(i)}={\color{red}2}\,\right\}
\\
&=&
	I\!\left\{\,Y^{(i)}=c,X^{(i)}=x,\widetilde{Y}^{(i)}=y,J^{(i)}=1,M^{(i)}=1\,\right\}
\\
&=&
	\left\{\begin{array}{cl}
		1, & \textnormal{if}\;\; c = y
		\\
		0, & \textnormal{otherwise}
	\end{array}\right.\,,
\end{eqnarray*}
which in turn follows from the definition of $K^{(i)}$ and
the hypothesis \eqref{MoneImpliesCEqualsY}.
Next, in order to establish the desired expression for the log likelihood,
we derive expressions for
\,$\pi_{c \vert xy0}$,
\,$\pi_{c \vert xy1}$,
\,$\pi_{c \vert xy2}$,
\,$p_{xy0}$,
\,$p_{xy1}$,
\,and
\,$p_{xy2}$,
in terms of
\,$\pi_{c \vert x}$,
\,$\mu_{0 \vert xy}$,
\,$\mu_{1 \vert xy}$,
\,$\nu_{xy}$,
\,$\omega_{0}$,
\,and
\,$\omega_{1}$.
\begin{eqnarray*}
\pi_{c \vert xy0}
&=&
	P\!\left(\;Y=c\;\left\vert\;X=x,\widetilde{Y}=y,K=0\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X=x,\widetilde{Y}=y,J=0\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right),
	\quad\quad
	\textnormal{by \eqref{ZJindependence}}
\\
&=&
	P\!\left(\;Y=c,M=0\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right)
	\;+\;
	P\!\left(\;Y=c,M=1\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right)
\\
&=&
	{\color{white}+}\;\;
	P\!\left(\;Y=c\;\left\vert\;M=0,X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\;M=0\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right)
\\
&&
	+\;\;
	P\!\left(\;Y=c\;\left\vert\;M=1,X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\;M=1\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right)
\\
&=&
	P\!\left(\;\left.Y\overset{{\color{white}-}}{=}c\;\right\vert X=x\;\right)
	\cdot
	\mu_{0 \vert xy}
	\;\;+\;\;
	\delta_{cy}
	\cdot
	\mu_{1 \vert xy}\,,
	\quad\;
	\textnormal{by \,\eqref{MzeroImplies}, \eqref{MoneImpliesCEqualsY},\, and definition of \,$\mu_{m \vert xy}$}
\\
&=&
	\pi_{c \vert x}
	\cdot
	\mu_{0 \vert xy}
	\;\;+\;\;
	\delta_{cy}
	\cdot
	\mu_{1 \vert xy}\,,
	\quad\quad
	\textnormal{by definition of \,$\pi_{c \vert x}$}
\end{eqnarray*}
\begin{eqnarray*}
\pi_{c \vert xy1}
&=&
	P\!\left(\;Y=c\;\left\vert\;X=x,\widetilde{Y}=y,K=1\right.\,\right)
\;\;=\;\;
	P\!\left(\;Y=c\;\left\vert\;X=x,\widetilde{Y}=y,J=1,M=0\right.\,\right)
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X=x,\widetilde{Y}=y,M=0\right.\,\right),
	\quad\quad
	\textnormal{by \eqref{ZJindependence}}
\\
&=&
	P\!\left(\;Y=c\;\left\vert\;X\overset{{\color{white}-}}{=}x\right.\,\right),
	\quad\quad
	\textnormal{by \,\eqref{MzeroImplies}}
\\
&=&
	\pi_{c \vert x}\,,
	\quad\quad
	\textnormal{by definition of \,$\pi_{c \vert x}$}
\end{eqnarray*}
\begin{eqnarray*}
\pi_{y \vert xy2}
&=&
	P\!\left(\;Y=y\;\left\vert\;X=x,\widetilde{Y}=y,K=2\right.\,\right)
\;\;=\;\;
	P\!\left(\;Y=y\;\left\vert\;X=x,\widetilde{Y}=y,J=1,M=1\right.\,\right)
\\
&=&
	1\,,
	\quad\quad
	\textnormal{by \eqref{MoneImpliesCEqualsY}.}
\end{eqnarray*}
\begin{eqnarray*}
p_{xy0}
&=&
	P\!\left(\,X=x,\widetilde{Y}=y,K=0\,\right)
\;\;=\;\;
	P\!\left(\,X=x,\widetilde{Y}=y,J=0\,\right)
\\
&=&
	P\!\left(\,X=x,\widetilde{Y}=y\,\right)
	\cdot
	P\!\left(\,J=0\,\right)\,,
	\quad\quad
	\textnormal{by \eqref{ZJindependence}}
\\
&=&
	\nu_{xy}
	\cdot
	\omega_{0}\,,
	\quad\quad
	\textnormal{by definition of \,$\nu_{xy}$\, and \,$\omega_{0}$}
\end{eqnarray*}
\begin{eqnarray*}
p_{xy1}
&=&
	P\!\left(\,X=x,\widetilde{Y}=y,K=1\,\right)
\;\;=\;\;
	P\!\left(\,X=x,\widetilde{Y}=y,J=1,M=0\,\right)
\\
&=&
	P\!\left(\,X=x,\widetilde{Y}=y,M=0\,\right)
	\cdot
	P\!\left(\,J=1\,\right)\,,
	\quad\quad
	\textnormal{by \eqref{ZJindependence}}
\\
&=&
	P\!\left(\,M=0\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y\,\right)
	\cdot
	P\!\left(\,J=1\,\right)
\\
&=&
	\mu_{0 \vert xy}
	\cdot
	\nu_{xy}
	\cdot
	\omega_{1}\,,
	\quad\quad
	\textnormal{by definition of \,$\mu_{0 \vert xy}$, \,$\nu_{xy}$\, and \,$\omega_{1}$}
\end{eqnarray*}
\begin{eqnarray*}
p_{xy2}
&=&
	P\!\left(\,X=x,\widetilde{Y}=y,K=2\,\right)
\;\;=\;\;
	P\!\left(\,X=x,\widetilde{Y}=y,J=1,M=1\,\right)
\\
&=&
	P\!\left(\,X=x,\widetilde{Y}=y,M=1\,\right)
	\cdot
	P\!\left(\,J=1\,\right)\,,
	\quad\quad
	\textnormal{by \eqref{ZJindependence}}
\\
&=&
	P\!\left(\,M=1\;\left\vert\;X=x,\widetilde{Y}=y\right.\,\right)
	\cdot
	P\!\left(\,X=x,\widetilde{Y}=y\,\right)
	\cdot
	P\!\left(\,J=1\,\right)
\\
&=&
	\mu_{1 \vert xy}
	\cdot
	\nu_{xy}
	\cdot
	\omega_{1}\,,
	\quad\quad
	\textnormal{by definition of \,$\mu_{1 \vert xy}$, \,$\nu_{xy}$\, and \,$\omega_{1}$}
\end{eqnarray*}
Substituting the above expressions for
\,$\pi_{c \vert xy0}$,
\,$\pi_{c \vert xy1}$,
\,$\pi_{c \vert xy2}$,
\,$p_{xy0}$,
\,$p_{xy1}$,
\,and
\,$p_{xy2}$
\,into that of the log likelihood yields
\begin{eqnarray*}
\log\,L
&=&
	{\color{white}+} \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}0}}\right)
	\cdot
	\left(\,
		\log\left(\pi_{c \vert x}\,\mu_{0 \vert xy} + \delta_{cy}\,\mu_{1\vert xy}\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\left(\nu_{xy}\,\omega_{0}\right)
	\,\right)
\\
&&
	+ \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert x}
		\,\overset{{\color{white}.}}{+}\,
		\log\left(\mu_{0\vert xy}\,\nu_{xy}\,\omega_{1}\right)
	\,\right)
\\
&&
	+ \;\;
	\overset{1}{\underset{y=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\,
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert xy{\color{red}2}}\right)
	\cdot
	\left(\,
		\log\left(\,1\,\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\left(\mu_{1 \vert xy}\,\nu_{xy}\,\omega_{1}\right)
	\,\right)
\\
&=&
	{\color{white}+} \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}0}}\right)
	\cdot
	\left(\,
		\log\left(\pi_{c \vert x}\,\mu_{0 \vert xy} + \delta_{cy}\,\mu_{1\vert xy}\right)
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{xy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{0}
	\,\right)
\\
&&
	+ \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert x}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\mu_{0\vert xy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{xy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{1}
	\,\right)
\\
&&
	+ \;\;
	\overset{1}{\underset{y=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\,
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert xy{\color{red}2}}\right)
	\cdot
	\left(\,
		\log\,\mu_{1 \vert xy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\nu_{xy}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\omega_{1}
	\,\right)
\\
&=&
	{\color{white}+} \;\;
	\left(\,\log\,\overset{{\color{white}+}}{\omega}_{0}\,\right)\cdot\overset{n}{\underset{i=1}{\sum}}\; I\!\left\{\,J^{(i)}=0\,\right\}
	\;\; + \;\;
	\left(\,\log\,\overset{{\color{white}+}}{\omega}_{1}\,\right)\cdot\overset{n}{\underset{i=1}{\sum}}\; I\!\left\{\,J^{(i)}=1\,\right\}
\\
&&
	+ \;\;
	\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
		\left(\,\log\,\overset{{\color{white}+}}{\nu}_{xy}\,\right)
		\cdot
		\overset{n}{\underset{i=1}{\sum}}\; I\!\left\{\,X^{(i)}=,\widetilde{Y}^{(i)}=y\,\right\}
\\
&&
	+ \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}0}}\right)
	\cdot
	\log\left(\,
		\pi_{c \vert x}\,\mu_{0 \vert xy}
		\,\overset{{\color{white}.}}{+}\,
		\delta_{cy}\,\mu_{1\vert xy}
		\,\right)
\\
&&
	+ \;\;
	\overset{1}{\underset{c=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\;\,
	\overset{1}{\underset{y=0}{\sum}}\;
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{c \vert xy{\color{red}1}}\right)
	\cdot
	\left(\,
		\log\,\pi_{c \vert x}
		\,\overset{{\color{white}.}}{+}\,
		\log\,\mu_{0\vert xy}
	\,\right)
\\
&&
	+ \;\;
	\overset{1}{\underset{y=0}{\sum}}
	\;\,\underset{x\in\{0,1\}^{p}}{\sum}\,
	\left(\overset{n}{\underset{i=1}{\sum}}\;W^{(i)}_{y \vert xy{\color{red}2}}\right)
	\cdot
	\left(\,\log\,\overset{{\color{white}.}}{\mu}_{1 \vert xy}\,\right)
\end{eqnarray*}
\item
	First, note that
	\begin{eqnarray*}
	&&
		E\!\left[\;
			\left.
			W^{(i)}_{c \vert gy{\color{red}0}}
			\,\;\right\vert
			\EMConditionsWithValues
		\right]
	\\
	&=&
		E\!\left[\;
			\left.
			W^{(i)}_{c \vert gy{\color{red}0}}
			\,\;\right\vert
			\ithEMConditionsWithValues
		\right]\,,
		\quad
		\textnormal{by \; \eqref{iIndependence}, \eqref{Jindependence} and \eqref{ZJindependence}.}
	\\
	&=&
		P\!\left(\,
			\left.
			W^{(i)}_{c \vert gy{\color{red}0}} = 1
			\,\;\right\vert
			\ithEMConditionsWithValues
		\right)
	\\
	&=&
		P\!\left(\,
			\left.
			Y^{(i)} = c, X^{(i)} = g, \widetilde{Y}^{(i)} = y, K^{(i)} = 0
			\,\;\right\vert
			\!\!
			\ithEMConditionsWithValues
		\!\!\right)
	\\
	&=&
		I\!\left\{\,X^{(i)}\,=g , \widetilde{Y}^{(i)}=y , K^{(i)}\,=0\,\right\}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = c
			\,\;\right\vert
			\!\!
			\ithGYZeroEMConditionsWithValues
		\!\!\right)
	\\
	&=&
		I^{(i)}_{gy0}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = c
			\,\;\right\vert
			\!\!
			\begin{array}{c}
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y,\, J^{(i)} = 0
			\end{array}
			\textnormal{\large;}
			\modelParameters
		\!\!\right)
	\\
	&=&
		I^{(i)}_{gy0}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = c
			\,\;\right\vert
			\!\!
			\begin{array}{c}
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\end{array}
			\textnormal{\large;}
			\;\;\cdots\;\;
		\right)\,,
		\quad
		\textnormal{by \eqref{ZJindependence} and suppressing parameters for brevity}
	\\
	&=&
		I^{(i)}_{gy0}
		\cdot
		\left\{\;
			P\!\left(\,
				\left.
				Y^{(i)} = c,\,M^{(i)}=0
				\;\right\vert
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\;\right)
			\;+\;
			P\!\left(\,
				\left.
				Y^{(i)} = c,\,M^{(i)}=1
				\;\right\vert
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\;\right)
		\;\right\}
	\\
	&=&
		I^{(i)}_{gy0}
		\cdot
		\left\{\;
		\begin{array}{cc}
			&
			P\!\left(\,
				\left.
				Y^{(i)} = c
				\;\right\vert
				M^{(i)}=0,\,X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\;\right)
			\cdot
			P\!\left(\,
				\left.
				M^{(i)}=0
				\;\right\vert
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\;\right)
		\\
			\overset{\textnormal{\large{\color{white}1}}}{+}&
			P\!\left(\,
				\left.
				Y^{(i)} = c
				\;\right\vert
				M^{(i)}=1,\, X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\;\right)
			\cdot
			P\!\left(\,
				\left.
				M^{(i)}=1
				\;\right\vert
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y
			\;\right)
		\end{array}
		\;\right\}
	\\
	&=&
		I^{(i)}_{gy0}
		\cdot
		\left(\;
			\pi_{c \vert gy}
			\cdot
			\mu_{0 \vert gy}
			\;\overset{{\color{white}.}}{+}\;
			\delta_{cy}
			\cdot
			\mu_{1 \vert gy}
		\;\right)\,,
		\quad
		\textnormal{by \,\eqref{MzeroImplies}, \,\eqref{MoneImpliesCEqualsY}, and definition of \,$\mu_{m \vert gy}$}
	\end{eqnarray*}
	where \,$I^{(i)}_{gy0} \,:=\, I\!\left\{\,X^{(i)}\,=g , \widetilde{Y}^{(i)}=y , K^{(i)}\,=0\,\right\}$.
	Thus, we have:
	\begin{eqnarray*}
	&&
		E\!\left[\;\;
			\left.
			\overset{n}{\underset{i=1}{\sum}} \; W^{(i)}_{c \vert gy{\color{red}0}}
			\,\;\right\vert
			\!\!
			\EMConditionsWithValues
		\right]
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}} \;\,
		E\!\left[\;
			\left.
			W^{(i)}_{c \vert gy{\color{red}0}}
			\,\;\right\vert
			\!\!
			\EMConditionsWithValues
		\right]
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}} \;\,
		I^{(i)}_{gy0}
		\cdot
		\left(\;
			\pi_{c \vert gy}
			\cdot
			\mu_{0 \vert gy}
			\;\overset{{\color{white}.}}{+}\;
			\delta_{cy}
			\cdot
			\mu_{1 \vert gy}
		\;\right)
	\\
	&=&
		\left(\;
			\pi_{c \vert gy}
			\cdot
			\mu_{0 \vert gy}
			\;\overset{{\color{white}.}}{+}\;
			\delta_{cy}
			\cdot
			\mu_{1 \vert gy}
		\;\right)
		\cdot
		\left(\; \overset{n}{\underset{i=1}{\sum}} \; I^{(i)}_{gy0} \;\right)
	\\
	&=&
		N_{gy0}
		\cdot
		\left(\;
			\pi_{c \vert gy}
			\cdot
			\mu_{0 \vert gy}
			\;\overset{{\color{white}.}}{+}\;
			\delta_{cy}
			\cdot
			\mu_{1 \vert gy}
		\;\right)\,,
	\end{eqnarray*}
	where \,$N_{gy0} \,:=\, \overset{n}{\underset{i=1}{\sum}} \, I^{(i)}_{gy0}$.
	This establishes the desired expression for the first conditional expectations in question.
	Next, note similarly that
	\begin{eqnarray*}
	&&
		E\!\left[\;
			\left.
			W^{(i)}_{c \vert gy{\color{red}1}}
			\,\;\right\vert
			\EMConditionsWithValues
		\right]
		\;\; = \;\; \cdots \;\; =
	\\
	&=&
		P\!\left(\,
			\left.
			Y^{(i)} = c, X^{(i)} = g, \widetilde{Y}^{(i)} = y, K^{(i)} = {\color{red}1}
			\,\;\right\vert
			\!\!
			\ithEMConditionsWithValues
		\!\!\right)
	\\
	&=&
		I\!\left\{\,X^{(i)}\,=g , \widetilde{Y}^{(i)}=y , K^{(i)}\,=1\,\right\}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = c
			\,\;\right\vert
			\!\!
			\ithGYOneEMConditionsWithValues
		\!\!\right)
	\\
	&=&
		I^{(i)}_{gy1}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = c
			\,\;\right\vert
			\!\!
			\begin{array}{c}
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y,\, J^{(i)} = 1,\, M^{(i)} = 0
			\end{array}
			\textnormal{\large;}
			\modelParameters
		\!\!\right)
	\\
	&=&
		I^{(i)}_{gy1}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = c
			\,\;\right\vert
			\!\!
			\begin{array}{c}
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y,\, M^{(i)} = 0
			\end{array}
			\textnormal{\large;}
			\;\;\cdots\;\;
		\right)\,,
		\;\;
		\textnormal{by \eqref{ZJindependence} and suppressing parameters for brevity}
	\\
	&=&
		I^{(i)}_{gy1}
		\cdot
		P\!\left(\,
			\left.
			Y^{(i)} = c
			\;\right\vert
			X^{(i)} = g
		\;\right)
	\;\;=\;\;
		I^{(i)}_{gy1} \cdot \pi_{c \vert g}\,,
		\quad
		\textnormal{by \,\eqref{MzeroImplies}, and definition of \,$\pi_{c \vert g}$}\,,
	\end{eqnarray*}
	where \,$I^{(i)}_{gy1} \,:=\, I\!\left\{\,X^{(i)}\,=g , \widetilde{Y}^{(i)}=y , K^{(i)}\,=1\,\right\}$.
	Thus, we have:
	\begin{eqnarray*}
	&&
		E\!\left[\;\;
			\left.
			\overset{n}{\underset{i=1}{\sum}} \; W^{(i)}_{c \vert gy{\color{red}1}}
			\,\;\right\vert
			\!\!
			\EMConditionsWithValues
		\right]
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}} \;\,
		E\!\left[\;
			\left.
			W^{(i)}_{c \vert gy{\color{red}1}}
			\,\;\right\vert
			\!\!
			\EMConditionsWithValues
		\right]
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}} \;
		I^{(i)}_{gy1} \cdot \pi_{c \vert gy}
	\;\; = \;\; 
		\pi_{c \vert gy}
		\cdot
		\left(\; \overset{n}{\underset{i=1}{\sum}} \; I^{(i)}_{gy1} \;\right)
	\\
	&=&
		N_{gy1} \cdot \pi_{c \vert gy}\,,
	\end{eqnarray*}
	where \,$N_{gy1} \,:=\, \overset{n}{\underset{i=1}{\sum}} \, I^{(i)}_{gy1}$.
	Lastly, note that
	\begin{eqnarray*}
	&&
		E\!\left[\;
			\left.
			W^{(i)}_{y \vert gy{\color{red}2}}
			\,\;\right\vert
			\EMConditionsWithValues
		\right]
		\;\; = \;\; \cdots \;\; =
	\\
	&=&
		P\!\left(\,
			\left.
			Y^{(i)} = y, X^{(i)} = g, \widetilde{Y}^{(i)} = y, K^{(i)} = {\color{red}2}
			\,\;\right\vert
			\!\!
			\ithEMConditionsWithValues
		\!\!\right)
	\\
	&=&
		I\!\left\{\,X^{(i)}\,=g , \widetilde{Y}^{(i)}=y , K^{(i)}\,=2\,\right\}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = y
			\,\;\right\vert
			\!\!
			\ithGYTwoEMConditionsWithValues
		\!\!\right)
	\\
	&=&
		I^{(i)}_{gy2}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = y
			\,\;\right\vert
			\!\!
			\begin{array}{c}
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y,\, J^{(i)} = 1,\, M^{(i)} = 1
			\end{array}
			\textnormal{\large;}
			\modelParameters
		\!\!\right)
	\\
	&=&
		I^{(i)}_{gy2}
		\cdot
		P\!\left(
			\left.
			Y^{(i)} = y
			\,\;\right\vert
			\!\!
			\begin{array}{c}
				X^{(i)} = g,\, \widetilde{Y}^{(i)} = y,\, M^{(i)} = 1
			\end{array}
			\textnormal{\large;}
			\;\;\cdots\;\;
		\right)\,,
		\;\;
		\textnormal{by \eqref{ZJindependence} and suppressing parameters for brevity}
	\\
	&=&
		I^{(i)}_{gy2} \cdot 1
		\;\; = \;\;
		I^{(i)}_{gy2}\,,
		\quad
		\textnormal{by \,\eqref{MoneImpliesCEqualsY}}\,,
	\end{eqnarray*}
	where \,$I^{(i)}_{gy2} \,:=\, I\!\left\{\,X^{(i)}\,=g , \widetilde{Y}^{(i)}=y , K^{(i)}\,=2\,\right\}$.
	Thus, we have:
	\begin{eqnarray*}
	&&
		E\!\left[\;\;
			\left.
			\overset{n}{\underset{i=1}{\sum}} \; W^{(i)}_{c \vert gy{\color{red}2}}
			\,\;\right\vert
			\!\!
			\EMConditionsWithValues
		\right]
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}} \;\,
		E\!\left[\;
			\left.
			W^{(i)}_{c \vert gy{\color{red}2}}
			\,\;\right\vert
			\!\!
			\EMConditionsWithValues
		\right]
	\\
	&=&
		\overset{n}{\underset{i=1}{\sum}} \;
		I^{(i)}_{gy2}
	\;\; =: \;\; 
		N_{gy2}\,.
	\end{eqnarray*}
\end{enumerate}
This completes the proof of the Theorem. \qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
