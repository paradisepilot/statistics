
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{$X$-estimability \,$\Longrightarrow$\, pointwise least squares estimates yield well-defined random variable}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[$X$-estimability]
\mbox{}\vskip 0.1cm\noindent
Suppose \,$X \in \Re^{n \times p}$.
A linear map $\Lambda : \Re^{p} \longrightarrow \Re^{q}$ is said to be \textbf{$X$-estimable}
if there exists $\Gamma \in \Re^{n \times q}$ such that
\begin{equation*}
\Lambda(\beta) \;\; =\;\; \Gamma^{T} \cdot X \cdot \beta\,,
\quad
\textnormal{for each \,$\beta \in \Re^{p}$}.
\end{equation*}
\end{definition}

\begin{proposition}[$X$-estimability $\Longleftrightarrow$ unique least squares estimate, vector case]
\label{VectorEstimabilityImpliesUniqueLSE}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\lambda \in \Re^{1 \times p}$,\,
	and
	\,$\mathcal{L}_{\lambda} : \Re^{p} \longrightarrow \Re^{1}$ is left-multiplication by $\lambda$,
	i.e. $\mathcal{L}_{\lambda}(\beta) = \lambda \cdot \beta$.
\item
	$X \in \Re^{n \times p}$.
\item
	$y \in \Re^{n}$.
\end{itemize}
Then,
	$\mathcal{L}_{\lambda}$ is $X$-estimable
	\,if and only if\,
	$\lse\!\left(\,\mathcal{L}_{\lambda} \,;\, y\,\vert X\,\right)$\,
	is a singleton (i.e. $\mathcal{L}_{\lambda}$ admits a unique $(\,y\,\vert\,X)$-least square estimate).
\end{proposition}
\proof
\vskip 0.2cm\noindent
\underline{\textbf{(\,$\Longrightarrow$\,)}}\quad
Suppose $\mathcal{L}_{\lambda}$ is $X$-estimable, i.e. $\lambda = \gamma^{T} \cdot X$,
for some $\gamma \in \Re^{n \times 1}$.
Then,
\begin{equation*}
\lambda\cdot\Null(X) = \gamma^{T} \cdot X \cdot \Null(X) = \gamma^{T} \cdot \{\,0\,\} = \{\,0\,\}.
\end{equation*}
Hence,
\begin{eqnarray*}
\lse\!\left(\,\mathcal{L}_{\lambda} \,;\, y\,\vert X\,\right)
&=&
	\lambda \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y \;\, + \;\, \lambda \cdot \Null(X)
\;\;=\;\;
	\gamma^{T} \cdot \left(\,\overset{{\color{white}.}}{X} \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T}\,\right) \cdot y \;\, + \;\, \{\,0\,\}
\\
&=&
	\left\{\; \overset{{\color{white}.}}{\gamma}^{T} \cdot \Pi_{X} \cdot y \;\right\}\,,
\end{eqnarray*}
i.e. $\lse\!\left(\,\mathcal{L}_{\lambda} \,;\, y\,\vert X\,\right)$ is indeed a singleton, as desired.

\vskip 0.5cm\noindent
\underline{\textbf{(\,$\Longleftarrow$\,)}}\quad
Conversely, suppose $\lse\!\left(\,\mathcal{L}_{\lambda} \,;\, y\,\vert X\,\right)$ is a singleton.
Let \,$\Pi_{X^{T}} \in \Re^{p \times p}$\, be the orthogonal projection from $\Re^{p}$ onto
the subspace $\Col(X^{T}) \subset \Re^{p}$.
Note that \,$(I_{p} - \Pi_{X^{T}})$\, is the orthogonal projection from $\Re^{p}$ onto $\Col(X^{T})^{\perp}$.
Write
\begin{equation*}
\lambda^{T}
\;\;=\;\;
	X^{T} \cdot \rho_{1} \;+\; (I_{p} - \Pi_{X^{T}}) \cdot \rho_{2}
\;\;\in\;\;
	\Col(X^{T}) \,\oplus\, \Col(X^{T})^{\perp}
\;\;=\;\;
	\Re^{p \times 1}\,,
\end{equation*}
where $\rho_{1} \in \Re^{n \times 1}$, $\rho_{2} \in \Re^{p \times 1}$.
We therefore see that it suffices to establish that $(I_{p} - \Pi_{X^{T}}) \cdot \rho_{2} \,=\, 0$.

\vskip 0.5cm
\noindent
\textbf{Claim 1:}\quad $\rho_{2}^{T} \cdot (I_{p} - \Pi_{X^{T}}) \cdot v \,=\, 0$,
for each $v \in \Col(X^{T})$.
\vskip 0.2cm
\noindent
Proof of Claim 1:\quad
Immediate since left-multiplication by \,$I_{p} - \Pi_{X^{T}}$\, is the orthogonal projection
from $\Re^{p}$ onto $\Col(X^{T})^{\perp}$. This proves Claim 1.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
$\lambda\cdot\beta_{1} = \lambda\cdot\beta_{2}$,\,
for each \,$\beta_{1}, \beta_{2} \in \left\{\;\left.\overset{{\color{white}.}}{\beta} \in \Re^{p} \;\,\right\vert\; X\beta=\Pi_{X}(y) \;\right\}$.
\vskip 0.2cm
\noindent
Proof of Claim 2:\quad
Recall that \,$\lse\!\left(\,\mathcal{L}_{\lambda}\,;\,y\,\vert X\,\right)$ $=$
$\left\{\;\left.\overset{{\color{white}.}}{\lambda}\cdot\beta\in\Re^{1}\;\right\vert\;X\cdot\beta=\Pi_{X}\cdot y\;\right\}$.
Hence, Claim 2 is true because it is simply a restatement of the hypothesis that
$\lse\!\left(\,\mathcal{L}_{\lambda}\,;\,y\,\vert X\,\right)$ is a singleton.
This proves Claim 2.

\vskip 0.5cm
\noindent
\textbf{Claim 3:}\quad
$\rho_{2}^{T} \cdot (I_{p} - \Pi_{X^{T}}) \cdot w \,=\, 0$,
for each $w \in \Col(X^{T})^{\perp}$.
\vskip 0.2cm
\noindent
Proof of Claim 3:\quad
First, by the finite-dimensional Fredholm Alternative (see, for example, Theorem 7.7, p.207, \cite{Axler2015}),
we have: \,$\Col(X^{T})^{\perp} = \Null(X)$.
Next, let $w \in \Col(X^{T})^{\perp} = \Null(X)$ be an arbitrary element.
Fix a generalized inverse $(X^{T} \cdot X)^{\dagger} \in \Re^{p \times p}$
of the symmetric matrix $X^{T} \cdot X \in \Re^{p \times p}$.
Observe that
\begin{equation*}
\lse\!\left(\,\mathcal{L}_{\lambda}\,;\,y\,\vert X\,\right)
\;\; = \;\;
	\lambda \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y \;\, + \;\, \lambda \cdot \Null(X)
\;\; = \;\;
	\lambda \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y \;\, + \;\, \lambda \cdot \Col(X^{T})^{\perp}
\end{equation*}
Now, let $\beta_{1} := (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y$.
Thus, $X\cdot\beta_{1} = X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y = \Pi_{X}\cdot y$.
Next, let $\beta_{2} := \beta_{1} + w$\,; hence, $w = \beta_{2} - \beta_{1}$.
Then, $X\cdot\beta_{2} = X\cdot\beta_{1} + X\cdot w = X\cdot\beta_{1} + 0 = X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y = \Pi_{X}\cdot y$.
Thus, $\beta_{1}, \beta_{2} \in \left\{\;\left.\overset{{\color{white}.}}{\beta} \in \Re^{p} \;\,\right\vert\; X\cdot\beta=\Pi_{X}(y) \;\right\}$.
By Claim 2, we have: $\lambda\cdot\beta_{1} = \lambda\cdot\beta_{2}$.
Now, we see that
\begin{eqnarray*}
\rho_{2}^{T} \cdot (I_{p} - \Pi_{X^{T}}) \cdot w
&=&
	\left(\, \lambda - \rho_{1}^{T} \cdot X \,\right) \cdot w
\;\; = \;\;
	\left(\, \lambda - \rho_{1}^{T} \cdot X \,\right) \cdot (\beta_{2} - \beta_{1})
\\
&=&
	\lambda\cdot\beta_{2} - \lambda\cdot\beta_{1} - \rho_{1}^{T} \cdot (X \cdot \beta_{2} - X \cdot \beta_{1})
\;\; = \;\;
	\lambda\cdot\beta_{2} - \lambda\cdot\beta_{1} - \rho_{1}^{T} \cdot (\Pi_{X}\cdot y - \Pi_{X}\cdot y)
\\
&=&
	\lambda\cdot\beta_{2} - \lambda\cdot\beta_{1}
\\
&=&
	0
\end{eqnarray*}
This proves Claim 3.

\vskip 0.5cm
\noindent
\textbf{Claim 4:}\quad
$\rho_{2}^{T} \cdot (I_{p} - \Pi_{X^{T}}) \cdot u \,=\, 0$,\, for each $u \in \Re^{p}$.
\vskip 0.2cm
\noindent
Proof of Claim 4:\quad
This follows trivially from Claim 1, Claim 3 and the fact that
$\Col(X^{T}) \oplus \Col(X^{T})^{\perp} = \Re^{p}$.
This proves Claim 4.

\vskip 0.5cm
\noindent
To complete the proof, simply note now that
\begin{equation*}
\textnormal{Claim 4}
\quad\Longleftrightarrow\quad
	\rho_{2}^{T} \cdot (I_{p} - \Pi_{X^{T}}) = 0
\quad\Longleftrightarrow\quad
	(I_{p} - \Pi_{X^{T}}) \cdot \rho_{2} = 0
\end{equation*}
This completes the proof of the Proposition.
\qed

\begin{theorem}[$X$-estimability $\Longleftrightarrow$ unique least squares estimate, matrix case]
\label{MatrixEstimabilityImpliesUniqueLSE}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$\Lambda \in \Re^{q \times p}$,\,
	and
	\,$\mathcal{L}_{\Lambda} : \Re^{p} \longrightarrow \Re^{q}$ is left-multiplication by $\Lambda$,
	i.e. $\mathcal{L}_{\Lambda}(\beta) = \Lambda \cdot \beta$.
\item
	$X \in \Re^{n \times p}$.
\item
	$y \in \Re^{n}$.
\end{itemize}
Then,
	$\mathcal{L}_{\Lambda}$ is $X$-estimable
	\,if and only if\,
	$\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$\,
	is a singleton (i.e. $\mathcal{L}_{\Lambda}$ admits a unique $(\,y\,\vert\,X)$-least square estimate).
\end{theorem}
\proof
\vskip 0.2cm\noindent
\underline{\textbf{(\,$\Longrightarrow$\,)}}\quad
Suppose $\mathcal{L}_{\Lambda}$ is $X$-estimable, i.e. $\Lambda = \Gamma^{T} \cdot X$,
for some $\Gamma \in \Re^{n \times q}$.
Then,
\begin{equation*}
\lambda\cdot\Null(X) \;\;=\;\; \Gamma^{T} \cdot X \cdot \Null(X) \;\;=\;\; \Gamma^{T} \cdot \{\,0\,\} \;\;=\;\; \{\,0\,\}.
\end{equation*}
Hence,
\begin{eqnarray*}
\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)
&=&
	\Lambda \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y \;\, + \;\, \Lambda \cdot \Null(X)
\;\;=\;\;
	\Gamma^{T} \cdot \left(\,\overset{{\color{white}.}}{X} \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T}\,\right) \cdot y \;\, + \;\, \{\,0\,\}
\\
&=&
	\left\{\; \overset{{\color{white}.}}{\Gamma}^{T} \cdot \Pi_{X} \cdot y \;\right\}\,,
\end{eqnarray*}
i.e. $\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$ is indeed a singleton, as desired.

\vskip 0.5cm\noindent
\underline{\textbf{(\,$\Longleftarrow$\,)}}\quad
Conversely, suppose $\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$ is a singleton.
Write
\begin{equation*}
\Lambda
\;\; = \;\;
	\left[\;\;\begin{array}{c}
	\lambda_{1} \\ \lambda_{2} \\ \vdots \\ \lambda_{q}
	\end{array}\;\;\right]
	\in\Re^{q \times p}\,,
	\quad\textnormal{where \,$\lambda_{i} \in \Re^{1 \times p}$,\, for each $i = 1,2,\ldots,p$}\,.
\end{equation*}

\vskip 0.5cm
\noindent
\textbf{Claim 1:}\quad
$\lse\!\left(\,\mathcal{L}_{\lambda_{i}} \,;\, y\,\vert X\,\right)$\, is a singleton, for each $i = 1,2,\ldots,q$.
\vskip 0.2cm
\noindent
Proof of Claim 1:\quad
Fix a generalized inverse \,$(X^{T} \cdot X)^{\dagger} \in \Re^{p \times p}$\, of the symmetric
matrix \,$X^{T} \cdot X \in \Re^{p \times p}$.\,
Then, by Definition \ref{LSE},
\begin{eqnarray*}
\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)
&=&
	\Lambda\cdot
	\left\{\;
		\left.
		\overset{{\color{white}.}}{\beta}
		\in\Re^{p \times 1}
		\;\;\right\vert\;
		X \cdot \beta \,=\, \Pi_{X}\cdot y
	\;\right\}
\;\;=\;\;
	\Lambda \cdot \left(\;(X^{T} \cdot \overset{{\color{white}.}}{X})^{\dagger} \cdot X^{T} \cdot y \; + \; \Null(X) \;\right)
\\
&=&
	\left\{\;
		\left.
		\overset{{\color{white}\vert}}{\Lambda}
		\cdot
		\left(\,(X^{T} \cdot X)^{\dagger} \overset{{\color{white}+}}{\cdot} X^{T} \cdot y \, + \, v \,\right)
		\in\Re^{q \times 1}
		\;\;\right\vert\;
		v \,\in\, \Null(X)
	\;\right\}
\\
&=&
	\left\{\;
		\left.
		\left[\;\;\begin{array}{c}
			\lambda_{1} \cdot \left(\,(X^{T} \cdot X)^{\dagger} \overset{{\color{white}+}}{\cdot} X^{T} \cdot y \, + \, v \,\right)
			\\
			\overset{{\color{white}\vert}}{\lambda}_{2} \cdot \left(\,
				(X^{T} \cdot X)^{\dagger} \overset{{\color{white}+}}{\cdot} X^{T} \cdot y \, + \, v
				\,\right)
			\\
			\vdots
			\\
			\overset{{\color{white}\vert}}{\lambda}_{q} \cdot \left(\,
				(X^{T} \cdot X)^{\dagger} \overset{{\color{white}+}}{\cdot} X^{T} \cdot y \, + \, v
				\,\right)
		\end{array}\;\;\right]
		\in\Re^{q \times 1}
		\;\;\right\vert\;
		v \,\in\, \Null(X)
	\;\right\}
\end{eqnarray*}
Hence, if \,$\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$\, is a singleton,
it immediately follows that, for each \,$i = 1,2,\ldots,q$,
\begin{equation*}
\left\{\;
	\left.
	\lambda_{i} \cdot \left(\,(X^{T} \cdot X)^{\dagger} \overset{{\color{white}+}}{\cdot} X^{T} \cdot y \, + \, v \,\right)
	\in\Re^{q \times 1}
	\;\;\right\vert\;
	v \,\in\, \Null(X)
\;\right\}
\end{equation*}
is also a singleton.
However, by Definition \ref{LSE} again, we also have
\begin{equation*}
\lse\!\left(\,\mathcal{L}_{\lambda_{i}} \,;\, y\,\vert X\,\right)
\;\;=\;\;
	\left\{\;
		\left.
		\lambda_{i} \cdot \left(\,(X^{T} \cdot X)^{\dagger} \overset{{\color{white}+}}{\cdot} X^{T} \cdot y \, + \, v \,\right)
		\in\Re^{q \times 1}
		\;\;\right\vert\;
		v \,\in\, \Null(X)
	\;\right\}\,,
	\quad
	\textnormal{for each \,$i = 1,2,\ldots,q$}\,.
\end{equation*}
Thus, we see that
if \,$\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$\, is a singleton,
then \,$\lse\!\left(\,\mathcal{L}_{\lambda_{i}} \,;\, y\,\vert X\,\right)$\, is a singleton,
for each $i = 1, 2, \ldots, q$.
This proves Claim 1.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
For each $i = 1,2,\ldots,q$, \,$\mathcal{L}_{\lambda_{i}}$ is $X$-estimable, 
i.e. $\lambda_{i} \,=\, \gamma_{i}^{T} \cdot X \,\in\, \Re^{1 \times p}$, for some $\gamma_{i} \in \Re^{n \times 1}$.
\vskip 0.2cm
\noindent
Proof of Claim 2:\quad
Immediate by Proposition \ref{VectorEstimabilityImpliesUniqueLSE}.

\vskip 0.5cm
\noindent
\textbf{Claim 3:}\quad
$\mathcal{L}_{\Lambda}$ is $X$-estimable, 
i.e. $\Lambda \,=\, \Gamma^{T} \cdot X \,\in\, \Re^{q \times p}$, for some $\Gamma \in \Re^{n \times q}$.
\vskip 0.2cm
\noindent
Proof of Claim 3:\quad
\begin{equation*}
\Lambda
\;\; = \;\;
	\left[\;\;\begin{array}{c}
	\lambda_{1} \\ \lambda_{2} \\ \vdots \\ \lambda_{q}
	\end{array}\;\;\right]
\;\; = \;\;
	\left[\;\;\begin{array}{c}
	\gamma_{1}^{T} \cdot X \\ \gamma_{2}^{T} \cdot X \\ \vdots \\ \gamma_{q}^{T} \cdot X
	\end{array}\;\;\right]
\;\; = \;\;
	\left[\;\begin{array}{cccc}
	\\
	\gamma_{1} & \gamma_{2} & \cdots & \gamma_{q}
	\\ \\
	\end{array}\;\right]^{T}
	\cdot \; X
\;\; = \;\;
	\Gamma^{T} \cdot X
\;\; \in\;\;
	\Re^{q \times p}\,,
\end{equation*}
where
\begin{equation*}
\Gamma
\;\; := \;\;
	\left[\;\begin{array}{cccc}
	\\
	\gamma_{1} & \gamma_{2} & \cdots & \gamma_{q}
	\\ \\
	\end{array}\;\right]
\;\; \in\;\;
	\Re^{n \times q}\,.
\end{equation*}
This proves Claim 3, as well as completes the proof of the Theorem. \qed

\vskip 0.5cm
\begin{theorem}[$X$-estimability $\Longrightarrow$ pointwise least squares estimates yield well-defined random variable]
\label{LSEofEstimableFunctionsAreRandomVariables}
\noindent
Suppose:
\begin{itemize}
\item
	$\Lambda \in \Re^{q \times p}$,\,
	and
	\,$\mathcal{L}_{\Lambda} : \Re^{p} \longrightarrow \Re^{q}$ is left-multiplication by $\Lambda$,
	i.e. $\mathcal{L}_{\Lambda}(\beta) = \Lambda \cdot \beta$.
\item
	$X \in \Re^{n \times p}$\, and \,$\Pi_{X} = X\cdot(X^{T}\cdot X)^{\dagger}\cdot X^{T} \in \Re^{n \times n}$,
	where $(X^{T}\cdot X)^{\dagger} \in \Re^{p \times p}$ is any generalized inverse of
	$X^{T}\cdot X \in \Re^{p \times p}$.
\item
	$Y : (\Omega,\mathcal{A},\mu) \,\longrightarrow\, \Re^{n}$ is an $\Re^{n}$-valued
	random variable defined on the probability space $(\Omega,\mathcal{A},\mu)$.
\end{itemize}
Define \,$\lse\!\left(\,\mathcal{L}_{\Lambda},\,Y\,\vert\,X\,\right) : (\Omega,\mathcal{A},\mu) \longrightarrow \Re^{n}$
as follows:
\begin{equation*}
\lse\!\left(\,\mathcal{L}_{\Lambda},\,Y\,\vert\,X\,\right)(\omega)
\;\; := \;\; \lse\!\left(\,\mathcal{L}_{\Lambda},\,Y(\omega)\,\vert\,X\,\right)\,,
\quad
\textnormal{for each \,$\omega \in \Omega$}\,.
\end{equation*}
Then,
\begin{equation*}
\textnormal{$\mathcal{L}_{\Lambda}$ is $X$-estimable}
\quad\Longrightarrow\quad
\textnormal{$\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, Y\,\vert X\,\right)$\,
is a well-defined $\Re^{q}$-valued random variable}
\end{equation*}
\end{theorem}
\proof
By Theorem \ref{MatrixEstimabilityImpliesUniqueLSE},
if $\mathcal{L}_{\Lambda}$ is $X$-estimable (i.e. $\Lambda = \Gamma^{T} \cdot X$,
for some $\Gamma \in \Re^{n \times q}$), then, for each $\omega \in \Omega$,
$\lse\!\left(\,\mathcal{L}_{\Lambda},\,Y(\omega)\,\vert\,X\,\right) \subset \Re^{q}$
is a singleton and thus can be identified with a unique element in $\Re^{q}$.
Thus,
\begin{equation*}
\lse\!\left(\,\mathcal{L}_{\Lambda},\,Y\,\vert\,X\,\right)
\;:\; (\Omega,\mathcal{A},\mu) \; \longrightarrow \; \Re^{q}
\;:\; \omega \; \longmapsto \; \lse\!\left(\,\mathcal{L}_{\Lambda},\,Y(\omega)\,\vert\,X\,\right)
\;=\; \Gamma^{T} \cdot \Pi_{X} \cdot Y(\omega)
\end{equation*}
indeed defines a set-theoretic map from $\Omega$ into $\Re^{q}$.
Secondly, note that the map
$\lse\!\left(\,\mathcal{L}_{\Lambda},\,Y\,\vert\,X\,\right) : (\Omega,\mathcal{A},\mu) \longrightarrow \Re^{q}$
is the composition of \,$Y : (\Omega,\mathcal{A},\mu) \rightarrow \Re^{n}$ followed by the
left multiplication by $\Gamma^{T} \cdot \Pi_{X} \in \Re^{q \times n}$ from $\Re^{n}$ into $\Re^{q}$.
$Y$ is measurable by hypothesis, while left multiplication by $\Gamma^{T} \cdot \Pi_{X}$ is measurable
since it is continuous (being linear).
Since the composition of two measurable functions is itself measurable,
we see that the map $\lse\!\left(\,\mathcal{L}_{\Lambda},\,Y\,\vert\,X\,\right)$ is measurable,
i.e. it is indeed an $\Re^{q}$-valued random variable.
This completes the proof of the Theorem.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
