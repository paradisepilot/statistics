
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Estimation for the case $\Col(X) \not\subset \Col(V)$}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{theorem}
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$\sigma^{2} > 0$.
	\;
	$X \in \Re^{n \times p}$.
	\;
	$V \in \Re^{n \times n}$ is a symmetric positive semi-definite matrix of rank at least 1.
\item
	$Y = \left\{\;
		\left.
		\overset{{\color{white}.}}{Y}_{\beta} : (\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta}) \longrightarrow \Re^{n}
		\;\;\right\vert\;
		\beta \in \Re^{p}
		\;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$,
	of $\Re^{n}$-valued random variables defined respectively on the
	probability spaces $(\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta})$.
\item
	$Z = \left\{\;
		\left.
		\overset{{\color{white}.}}{Z}_{\beta}
		: (\Omega^{\prime}_{\beta},\mathcal{A}^{\prime}_{\beta},\mu^{\prime}_{\beta}) \longrightarrow \Re^{n}
		\;\;\right\vert\;
		\beta \in \Re^{p}
		\;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$,
	of $\Re^{n}$-valued random variables defined respectively on the
	probability spaces $(\Omega^{\prime}_{\beta},\mathcal{A}^{\prime}_{\beta},\mu^{\prime}_{\beta})$.
\item
	For each $\beta \in \Re^{p}$, the following hold:
	\begin{equation*}
	\begin{array}{rclcl}
	E\!\left[\;Y_{\beta}\,\right]
		& \overset{{\color{white}\vert}}{=} &
		E\!\left[\;Z_{\beta}\,\right] \;\; = \;\; X \cdot \beta & \in & \Re^{n}
	\\
	\Cov\!\left(\,Y_{\beta}\,\right)
		& \overset{{\color{white}\vert}}{=} &
		\sigma^{2} \cdot V & \in & \Re^{n \times n}
	\\
	\Cov\!\left(\,Z_{\beta}\,\right)
		& \overset{{\color{white}\vert}}{=} &
		\sigma^{2} \cdot (\,V+X\cdot X^{T}\,) & \in & \Re^{n \times n}
	\end{array}
	\end{equation*}
\end{itemize}
Then, the following statements hold:
\begin{enumerate}
\item\label{BLUEYiffBLUEZ}
	For any \,$A \in \Re^{n \times n}$,\, we have:
	\begin{equation*}
	\begin{array}{c}
		\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n} : y \longmapsto A \cdot y
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{is \, a \, BLUE \, of}
		\\{\color{white}\overset{.}{\vert}}
		\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n} : \beta \longmapsto X \cdot \beta
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{in terms of \,$Y = \{\,Y_{\beta}\,\}$}
		\end{array}
	\;\;\quad\Longleftrightarrow\;\;\quad
	\begin{array}{c}
		\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n} : z \longmapsto A \cdot z
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{is \, a \, BLUE \, of}
		\\{\color{white}\overset{.}{\vert}}
		\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n} : \beta \longmapsto X \cdot \beta
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{in terms of \,$Z = \{\,Z_{\beta}\,\}$}
		\end{array}
	\end{equation*}
\item\label{formulaColXNotInColV}
	{\color{red}The map
	\begin{equation*}
	\Re^{n} \longrightarrow \Re^{n}
	\; : \; y \; \longmapsto \;
	X \cdot \left(\,X^{T} \cdot W^{\dagger} \cdot X\,\right)^{\dagger} \cdot X^{T} \cdot W^{\dagger} \cdot y
	\end{equation*}
	is a BLUE of
	\begin{equation*}
	\mathcal{L}_{X} \; : \; \Re^{p} \longrightarrow \Re^{n} \; : \; \beta \; \longmapsto \; X \cdot \beta
	\end{equation*}
	in terms of
	\,$Y = \{\,Y_{\beta}\,\}$},
	where
	\,$W^{\dagger} \in \Re^{n \times n}$\, is any generalized inverse of
	\,$W := V + X \cdot X^{T} \in \Re^{n \times n}$,\,
	and
	\,$\left(\,X^{T} \cdot W^{\dagger} \cdot X\,\right)^{\dagger} \in \Re^{p \times p}$\,
	is any generalized inverse of
	\,$X^{T} \cdot W^{\dagger} \cdot X \in \Re^{p \times p}$.
\end{enumerate}
\end{theorem}
\proof
\begin{enumerate}
\item
	We start by stating and proving two claims:

	\vskip 0.3cm
	\noindent
	\textbf{Claim 1:}\quad
	For any matrix \,$M \in \Re^{n \times n}$,\,
	\begin{equation*}
		\begin{array}{c}
		\mathcal{L}_{M} : \Re^{n} \longrightarrow \Re^{n} : y \longmapsto M \cdot y
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{is a linear unbiased estimator of}
		\\{\color{white}\overset{.}{\vert}}
		\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n} : \beta \longmapsto X \cdot \beta
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{in terms of \,$Y = \{\,Y_{\beta}\,\}$}
		\end{array}
	\quad\Longleftrightarrow\quad
		\begin{array}{c}
		\textnormal{$M \in \Re^{n \times n}$}
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{satisfies}
		\\{\color{white}\overset{.}{\vert}}
		M \cdot X \; = \; X
		\end{array}
	\quad\Longleftrightarrow\quad
		\begin{array}{c}
		\mathcal{L}_{M} : \Re^{n} \longrightarrow \Re^{n} : z \longmapsto M \cdot z
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{is a linear unbiased estimator of}
		\\{\color{white}\overset{.}{\vert}}
		\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n} : \beta \longmapsto X \cdot \beta
		\\{\color{white}\overset{.}{\vert}}
		\textnormal{in terms of \,$Z = \{\,Z_{\beta}\,\}$}
		\end{array}
	\end{equation*}
	\vskip 0.0cm
	Proof of Claim 1:
	On the one hand,
	\begin{eqnarray*}
	&&
		\textnormal{$\mathcal{L}_{M}$\, is a linear unbiased estimator of \,$\mathcal{L}_{X}$\,
		in terms of $Y = \{\,Y_{\beta}\,\}$}
	\\
	&\Longleftrightarrow&
		E\!\left[\;\mathcal{L}_{M}(\,Y_{\beta}\,)\;\right] \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		E\!\left[\; M \cdot Y_{\beta} \;\right] \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		M \cdot E\!\left[\; Y_{\beta} \;\right] \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		M \cdot X \cdot \beta \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		\left(\,M \cdot X - X\,\right) \cdot \beta \;\;=\;\; 0\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		M \cdot X \;\; = \;\; X
	\end{eqnarray*}
	On the other hand,
	\begin{eqnarray*}
	&&
		\textnormal{$\mathcal{L}_{M}$\, is a linear unbiased estimator of \,$\mathcal{L}_{X}$\,
		in terms of $Z = \{\,Z_{\beta}\,\}$}
	\\
	&\Longleftrightarrow&
		E\!\left[\;\mathcal{L}_{M}(\,Z_{\beta}\,)\;\right] \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		E\!\left[\; M \cdot Z_{\beta} \;\right] \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		M \cdot E\!\left[\; Z_{\beta} \;\right] \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		M \cdot X \cdot \beta \;\;=\;\; X \cdot \beta\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		\left(\,M \cdot X - X\,\right) \cdot \beta \;\;=\;\; 0\,,
		\quad\textnormal{for each \,$\beta\in\Re^{p}$}
	\\
	&\Longleftrightarrow&
		M \cdot X \;\; = \;\; X
	\end{eqnarray*}
	This proves Claim 1.

	\vskip 0.3cm
	\noindent
	\textbf{Claim 2:}\quad
	$\Var\!\left(\,\xi^{T} \cdot M \cdot Z\,\right)$
	\;$=$\; $\Var\!\left(\,\xi^{T} \cdot M \cdot Y\,\right)$
		\,$+$\, $\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot M \cdot X \;\right\Vert^{2}$\,,
	for any \,$M \in \Re^{n \times n}$, \,$\xi \in \Re^{n}$.
	\vskip 0.0cm
	\noindent
	Proof of Claim 2:\quad
	Simply note that, for any \,$M \in \Re^{n \times n}$, \,$\xi \in \Re^{n}$,\, we have
	\begin{eqnarray*}
	\Var\!\left(\,\xi^{T} \cdot M \cdot Z\,\right)
	&=&
		\xi^{T} \cdot M \cdot \Var\!\left(\,Z\,\right) \cdot M^{T} \cdot \xi
	\;\; = \;\;
		\xi^{T} \cdot M \cdot \sigma^{2} \cdot \left(\, V + X \cdot X^{T} \,\right) \cdot M^{T} \cdot \xi
	\\
	&=&
		\xi^{T} \cdot M \cdot \left(\,\sigma^{2} \cdot V\,\right) \cdot M^{T} \cdot \xi
		\; + \;
		\sigma^{2} \cdot \xi^{T} \cdot M \cdot X \cdot X^{T} \cdot M^{T} \cdot \xi
	\\
	&=&
		\xi^{T} \cdot M \cdot \Var\!\left(\,Y\,\right) \cdot M^{T} \cdot \xi
		\; + \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot M \cdot X \; \right\Vert^{2}
	\\
	&=&
		\Var\!\left(\,\xi^{T} \cdot M \cdot Y\,\right)
		\; + \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot M \cdot X \; \right\Vert^{2}
	\end{eqnarray*}
	This proves Claim 2.

	\vskip 0.5cm
	\noindent
	\underline{\textbf{(\;$\Longrightarrow$\;)}}\quad
	Suppose \,$\mathcal{L}_{A}$\, is a BLUE of \,$\mathcal{L}_{X}$\, in terms of \,$Y = \{\,Y_{\beta}\,\}$.\,
	It follows immediately from Claim 1 that
	\,$\mathcal{L}_{A}$\, is a linear unbiased estimator of
	\,$\mathcal{L}_{X}$\, in terms of \,$Z = \{\,Z_{\beta}\,\}$.
	
	\vskip 0.1cm
	It remains only to show that, for any \,$M \in \Re^{n \times n}$\, such that
	\,$\mathcal{L}_{M} : \Re^{n} \longrightarrow \Re^{n} : z \longmapsto M \cdot z$\,
	is a linear unbiased estimator of
	\,$\mathcal{L}_{X}$\, in terms of \,$Z = \{\,Z_{\beta}\,\}$,
	we have:
	\begin{equation*}
	\Var\!\left(\,\xi^{T} \cdot A \cdot Z\,\right)
	\;\; \leq \;\;
		\Var\!\left(\,\xi^{T} \cdot M \cdot Z\,\right)\,,
	\quad
	\textnormal{for each \,$\xi \in \Re^{n}$}.
	\end{equation*}
	To this end, observe that, for any \,$\xi \in \Re^{n}$,
	\begin{eqnarray*}
	\Var\!\left(\,\xi^{T} \cdot A \cdot Z\,\right)
	& = &
		\Var\!\left(\,\xi^{T} \cdot A \cdot Y\,\right)
		\; + \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot A \cdot X \;\right\Vert^{2}\,,
		\quad\textnormal{by Claim 2}
	\\
	& = &
		\Var\!\left(\,\xi^{T} \cdot A \cdot Y\,\right)
		\; + \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot X \;\right\Vert^{2}\,,
		\quad\textnormal{by Claim 1}
	\\
	& \leq &
		\Var\!\left(\,\xi^{T} \cdot M \cdot Y\,\right)
		\; + \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot X \;\right\Vert^{2}
	\\
	& = &
		\Var\!\left(\,\xi^{T} \cdot M \cdot Y\,\right)
		\; + \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot M \cdot X \;\right\Vert^{2}\,,
		\quad\textnormal{by Claim 1}
	\\
	& = &
		\Var\!\left(\,\xi^{T} \cdot M \cdot Z\,\right)\,,
		\quad\textnormal{by Claim 2}
	\end{eqnarray*}
	where the inequality follows from the fact that
	\,$\mathcal{L}_{M}$\, is a linear unbiased estimator of
	\,$\mathcal{L}_{X}$\, in terms of \,$Y = \{\,Y_{\beta}\,\}$\,
	(by Claim 1 and the assumption that \,$\mathcal{L}_{M}$\,
	is a linear unbiased estimator of \,$\mathcal{L}_{X}$\,
	in terms of $Z = \{\,Z_{\beta}\,\}$)
	and the hypothesis that
	\,$\mathcal{L}_{A}$\, is a BLUE of \,$\mathcal{L}_{X}$\,
	in terms of \,$Y = \{\,Y_{\beta}\,\}$.
	This completes the proof that every BLUE \,$\mathcal{L}_{A}$\,
	of \,$\mathcal{L}_{X}$\, in terms of \,$Y = \{\,Y_{\beta}\,\}$\,
	is also a BLUE of
	of \,$\mathcal{L}_{X}$\, in terms of \,$Z = \{\,Z_{\beta}\,\}$.
	
	\vskip 0.5cm
	\noindent
	\underline{\textbf{(\;$\Longleftarrow$\;)}}\quad
	Conversely, suppose \,$\mathcal{L}_{A}$\, is a BLUE of \,$\mathcal{L}_{X}$\, in terms of \,$Z = \{\,Z_{\beta}\,\}$.
	It follows immediately from\\ Claim 1 that
	\,$\mathcal{L}_{A}$\, is a linear unbiased estimator of
	\,$\mathcal{L}_{X}$\, in terms of \,$Y = \{\,Y_{\beta}\,\}$.
	
	\vskip 0.1cm
	Thus, it remains only to show that, for any \,$M \in \Re^{n \times n}$\, such that
	\,$\mathcal{L}_{M} : \Re^{n} \longrightarrow \Re^{n} : y \longmapsto M \cdot y$\,
	is a linear unbiased estimator of
	\,$\mathcal{L}_{X}$\, in terms of \,$Y = \{\,Y_{\beta}\,\}$,
	we have:
	\begin{equation*}
	\Var\!\left(\,\xi^{T} \cdot A \cdot Y\,\right)
	\;\; \leq \;\;
		\Var\!\left(\,\xi^{T} \cdot M \cdot Y\,\right)\,,
	\quad
	\textnormal{for each \,$\xi \in \Re^{n}$}.
	\end{equation*}
	To this end, observe that, for any \,$\xi \in \Re^{n}$,
	\begin{eqnarray*}
	\Var\!\left(\,\xi^{T} \cdot A \cdot Y\,\right)
	& = &
		\Var\!\left(\,\xi^{T} \cdot A \cdot Z\,\right)
		\; - \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot A \cdot X \;\right\Vert^{2}\,,
		\quad\textnormal{by Claim 2}
	\\
	& = &
		\Var\!\left(\,\xi^{T} \cdot A \cdot Z\,\right)
		\; - \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot X \;\right\Vert^{2}\,,
		\quad\textnormal{by Claim 1}
	\\
	& \leq &
		\Var\!\left(\,\xi^{T} \cdot M \cdot Z\,\right)
		\; - \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot X \;\right\Vert^{2}
	\\
	& = &
		\Var\!\left(\,\xi^{T} \cdot M \cdot Z\,\right)
		\; - \;
		\sigma^{2} \cdot \left\Vert\; \xi^{T} \cdot M \cdot X \;\right\Vert^{2}\,,
		\quad\textnormal{by Claim 1}
	\\
	& = &
		\Var\!\left(\,\xi^{T} \cdot M \cdot Y\,\right)\,,
		\quad\textnormal{by Claim 2}
	\end{eqnarray*}
	where the inequality follows from the fact that
	\,$\mathcal{L}_{M}$\, is a linear unbiased estimator of
	\,$\mathcal{L}_{X}$\, in terms of \,$Z = \{\,Z_{\beta}\,\}$\,
	(by Claim 1 and the assumption that \,$\mathcal{L}_{M}$\,
	is a linear unbiased estimator of \,$\mathcal{L}_{X}$\,
	in terms of $Y = \{\,Y_{\beta}\,\}$)
	and the hypothesis that
	\,$\mathcal{L}_{A}$\, is a BLUE of \,$\mathcal{L}_{X}$\,
	in terms of \,$Z = \{\,Z_{\beta}\,\}$.
	This completes the proof that every BLUE \,$\mathcal{L}_{A}$\,
	of \,$\mathcal{L}_{X}$\, in terms of \,$Z = \{\,Z_{\beta}\,\}$\,
	is also a BLUE of
	of \,$\mathcal{L}_{X}$\, in terms of \,$Y = \{\,Y_{\beta}\,\}$.
\item
	By \eqref{BLUEYiffBLUEZ}, it suffices to show that the map
	\,$\mathcal{L}_{X \cdot \left(\,X^{T} \cdot W^{\dagger} \cdot X\,\right)^{\dagger} \cdot X^{T} \cdot W^{\dagger}} : \Re^{n} \longrightarrow \Re^{n}$\,
	is a BLUE of
	\,$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$\, in terms of
	\,$Z = \{\,Z_{\beta}\,\}$.
	Since \,$E\!\left[\,Z_{\beta}\,\right] \,=\, X \cdot \beta$\, and
	\,$\Var\!\left(\,Z_{\beta}\,\right)$
	\,$=$\, $\sigma^{2} \cdot W$
	\,$=$\, $\sigma^{2}\cdot\left(\,V + X \cdot X^{T}\,\right)$,\,
	by Theorem \ref{BLUEColXsubsetColV}, it suffices to show that
	\,$\Col(X) \,\subset\, \Col(W) \,=\, \Col\!\left(\,V + X \cdot X^{T}\,\right)$,
	which in turns follows immediately from the following two claims.

	\vskip 0.5cm
	\noindent
	\textbf{Claim 1:}\quad
	$\Col(X) \,\subset\, \Col(W)$
	\,\;$\Longleftrightarrow$\,\; $W \cdot W^{\dagger} \cdot X \,=\,X$.
	\vskip 0.0cm
	\noindent
	Proof of Claim 1:\quad
	First, observe that
	\begin{eqnarray*}
	\Col(X) \,\subset\, \Col(W)
	&\Longleftrightarrow&
		X \,=\, W \cdot \Gamma\,,
		\quad\textnormal{for some \,$\Gamma\in\Re^{n \times p}$}
	\\
	&\overset{{\color{white}\vert}}{\Longrightarrow}&
		W \cdot W^{\dagger} \cdot X
		\,=\, W \cdot W^{\dagger} \cdot W \cdot \Gamma
		\,=\, W \cdot \Gamma
		\,=\, X
	\end{eqnarray*}
	Conversely, suppose \,$W \cdot W^{\dagger} \cdot X \,=\, X$.
	By Corollary \ref{RealSpectralTheorem}, there exist
	a matrix \,$E \in \Re^{n \times r}$\, with orthonormal columns,
	and a diagonal matrix \,$D = \diag(d_{1},\ldots,d_{r}) \in \Re^{r \times r}$,\,
	where \,$r := \rank(\,W\,) \geq 1$\, such that
	\begin{eqnarray*}
	W \cdot E & = & E \cdot D\,,
	\\
	Q^{\dagger} & \overset{{\color{white}\textnormal{\large$-$}}}{:=} & D^{-1/2} \cdot E\,,
	\\
	W^{\dagger} & = & (Q^{\dagger})^{T} \cdot Q^{\dagger}
		\;\; = \;\;
			\left(\,D^{-1/2}\cdot E^{T}\,\right)^{T} \cdot \left(\,D^{-1/2} \cdot E^{T}\,\right)
		\;\; = \;\;
			E \cdot D^{-1} \cdot E^{T}\,,
	\\
	\Col(W) &=& \Col(E)\,.
	\end{eqnarray*}
	Next, write
	\begin{equation*}
	X \;=\; \left[\;
		\begin{array}{ccc}
		\\
		\mathbf{x}_{1} & \cdots & \mathbf{x}_{p}
		\\
		\\
		\end{array}
		\;\right]\,,
	\quad\textnormal{where \,$\mathbf{x}_{i} \,\in\, \Re^{n}$,\, for each \,$i \,=\, 1,2,\ldots,p$}\,.
	\end{equation*}
	Due to the orthogonal decomposition of \,$\Re^{n} \,=\, \Col(W) \,\oplus\, \Col(W)^{\perp}$,\,
	we have, for each \,$i \in 1,2,\ldots,p$\,:
	\begin{equation*}
	\mathbf{x}_{i}
	\;\; = \;\;
		W \cdot \gamma_{i} \;+\; \delta_{i}\,,
	\quad
	\textnormal{for some \,$\gamma_{i} \,\in\, \Re^{n}$\, and \,$\delta_{i} \,\in\, \Col(W)^{\perp}$}
	\end{equation*}
	Hence,
	\begin{equation*}
	X
	\;=\; \left[\;
		\begin{array}{ccc}
		\\
		\mathbf{x}_{1} & \cdots & \mathbf{x}_{p}
		\\
		\\
		\end{array}
		\;\right]
	\;=\;
		W \cdot \left[\;
			\begin{array}{ccc}
			\\
			\gamma_{1} & \cdots & \gamma_{p}
			\\
			\\
			\end{array}
			\;\right]
		\; + \;
			\left[\;
			\begin{array}{ccc}
			\\
			\delta_{1} & \cdots & \delta_{p}
			\\
			\\
			\end{array}
			\;\right]
	\;=\;
		W \cdot \Gamma \; + \; \Delta\,,
	\end{equation*}
	and we emphasize that \,$\Col(\Delta) \,\subset\, \Col(W)^{\perp}$.\,
	Consequently, the hypothesis \,$W \cdot W^{\dagger} \cdot X \,=\, X$\, implies
	\begin{eqnarray*}
	W \cdot \Gamma \;+\; \Delta
	&=& X
	\\
	&=& W \cdot W^{\dagger} \cdot X
	\;\;=\;\; W \cdot W^{\dagger} \cdot (\,W \cdot \Gamma + \Delta\,)
	\;\;=\;\; W \cdot W^{\dagger} \cdot W \cdot \Gamma \;+\; W \cdot W^{\dagger} \cdot \Delta
	\\
	&=& W \cdot \Gamma \;+\; W \cdot W^{\dagger} \cdot \Delta\,,
	\end{eqnarray*}
	which in turn implies (by subtracting \,$W \cdot \Gamma$\, on both sides):
	\begin{equation*}
	\Delta
	\;\; = \;\;
		W \cdot W^{\dagger} \cdot \Delta 
	\;\; = \;\;
		W \cdot E \cdot D^{-1} \cdot E^{T} \cdot \Delta 
	\;\; = \;\;
		W \cdot E \cdot D^{-1} \cdot 0_{r \times p}
	\;\; = \;\;
		0_{n \times p}\,,
	\end{equation*}
	where the third equality follows from the fact that
	\,$\Col(E) \,=\, \Col(W)$\, and \,$\Col(\Delta) \,\subset\, \Col(W)^{\perp}$.
	Therefore, we may now conclude
	\begin{equation*}
	X \;\; = \;\; W \cdot \Gamma \; + \; \Delta \;\; = \;\; W \cdot \Gamma \;+\; 0 \;\; = \;\; W \cdot \Gamma\,;
	\end{equation*}
	equivalently, $\Col(X) \,\subset\, \Col(W)$,\, as desired.
	This proves Claim 1.

	\vskip 0.5cm
	\noindent
	\textbf{Claim 2:}\quad
	$W \,=\, V + X \cdot X^{T}$
	\,\;$\Longrightarrow$\,\; $W \cdot W^{\dagger} \cdot X \,=\,X$.
	\vskip 0.0cm
	\noindent
	Proof of Claim 2:\quad
	Observe that
	\begin{eqnarray*}
	0_{n \times n}
	& = &
		\left(\,W - W\,\right) \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
	\;\; = \;\;
		\left(\,W - W \cdot W^{\dagger} \cdot W\,\right) \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
	\\
	& = &
		\left(\,I_{n} - W \cdot W^{\dagger} \,\right) \cdot W \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
	\;\; = \;\;
		\left(\,I_{n} - W \cdot W^{\dagger} \,\right)
		\cdot \left(\,V + X \cdot X^{T}\,\right)
		\cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
	\\
	& = &
		\left(\,I_{n} - W \cdot W^{\dagger} \,\right)
		\cdot V \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
		\; + \;
		\left(\,I_{n} - W \cdot W^{\dagger} \,\right)
		\cdot X \cdot X^{T} \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
	\end{eqnarray*}
	Thus, for each $\xi \in \Re^{n \times 1}$, we have:
	\begin{eqnarray*}
	0
	&=&
		\xi^{T} \cdot 0_{n \times n} \cdot \xi
	\\
	& = &
		\xi^{T}
		\cdot
		\left(\,I_{n} - W \cdot W^{\dagger} \,\right) \cdot V \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
		\cdot \xi
		\; + \;
		\xi^{T}
		\cdot
		\left(\,I_{n} - W \cdot W^{\dagger} \,\right) \cdot X \cdot X^{T} \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T}
		\cdot \xi		
	\\
	& = &
		\left(\,\left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T} \cdot \xi\;\right)^{T}
		\cdot\;
			V
		\;\cdot
			\left(\,\left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T} \cdot \xi\;\right)
		\; + \;
		\left\Vert\; X^{T} \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T} \cdot \xi \;\right\Vert^{2}
	\\
	& \geq &
		\left\Vert\; X^{T} \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right)^{T} \cdot \xi \;\right\Vert^{2}
		\;\; \geq \;\; 0\,,
	\end{eqnarray*}
	where the inequality follows from the hypothesis that \,$V \in \Re^{n \times n}$\, is positive semi-definite.
	The preceding observation implies:
	\begin{equation*}
	\xi^{T} \cdot \left(\,I_{n} - W \cdot W^{\dagger}\,\right) \cdot X
	\;\; = \;\; 0\,,
	\quad
	\textnormal{for each \,$\xi \in \Re^{n}$}\,,
	\end{equation*}
	which in turn implies
	\begin{equation*}
	\left(\,I_{n} - W \cdot W^{\dagger}\,\right) \cdot X \;\; = \;\; 0\,;
	\end{equation*}
	equivalently,
	\begin{equation*}
	W \cdot W^{\dagger} \cdot X \;\; = \;\; X\,,
	\end{equation*}
	as desired. This proves Claim 2 and completes the proof of \eqref{formulaColXNotInColV}.
\end{enumerate}
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
