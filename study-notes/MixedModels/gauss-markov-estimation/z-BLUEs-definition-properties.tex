
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{BLUEs -- definition and properties}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Linear unbiased estimators and BLUEs]
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$.
\item
	$\Lambda : \Re^{p} \longrightarrow \Re^{q}$ is an $X$-estimable linear map,
	i.e. there exists there exists $\Gamma \in \Re^{n \times q}$ such that
	$\Lambda(\beta) \;\; =\;\; \Gamma^{T} \cdot X \cdot \beta$,
	for each $\beta \in \Re^{p}$.
\item
	$Y = \left\{\;
		\left.
		\overset{{\color{white}.}}{Y}_{\beta} : (\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta}) \longrightarrow \Re^{n}
		\;\;\right\vert\;
		\beta \in \Re^{p}
		\;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$,
	of $\Re^{n}$-valued random variables defined respectively on the
	probability spaces $(\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta})$.
\end{itemize}
Then,
\begin{enumerate}
\item
	A linear map $L : \Re^{n} \longrightarrow \Re^{q}$ is called a
	{\color{red}\textbf{linear unbiased estimator of
	$\Lambda : \Re^{p} \longrightarrow \Re^{q}$
	in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$}} if
	\begin{equation*}
	E\!\left[\;L(\,Y_{\beta})\;\right] \;=\; \Lambda(\,\beta\,)\,,
	\quad
	\textnormal{for each \,$\beta \,\in\, \Re^{p}$}.
	\end{equation*}
\item
	A linear unbiased estimator $L : \Re^{n} \longrightarrow \Re^{q}$
	of $\Lambda : \Re^{p} \longrightarrow \Re^{q}$
	in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$
	is called a
	{\color{red}\textbf{best linear unbiased estimator (BLUE) of
	$\Lambda : \Re^{p} \longrightarrow \Re^{q}$ in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$}}
	if, for every linear unbiased estimator $L^{\prime} : \Re^{n} \longrightarrow \Re^{q}$
	in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$, we have:
	\begin{equation*}
	\Var\!\left(\,\xi^{T} \cdot L(Y_{\beta})\,\right)
	\;\; \leq \;\;
		\Var\!\left(\,\xi^{T} \cdot L^{\prime}(Y_{\beta})\,\right)\,,
	\quad
	\textnormal{for every \,$\beta \in \Re^{p}$\, and \,$\xi \in \Re^{q \times 1}$}\,.
	\end{equation*}
\end{enumerate}
\end{definition}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{proposition}
\label{BLUEoneD}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$\, and 
	\,$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$ is left multiplication by $X \in \Re^{n \times p}$.
\item
	$A \in \Re^{n \times n}$\, and 
	\,$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$ is left multiplication by $A \in \Re^{n \times n}$.
\item
	$Y = \left\{\;
		\left.
		\overset{{\color{white}.}}{Y}_{\beta} : (\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta}) \longrightarrow \Re^{n}
		\;\;\right\vert\;
		\beta \in \Re^{p}
		\;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$,
	of $\Re^{n}$-valued random variables defined respectively on the
	probability spaces $(\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta})$.
\item
	$E\!\left[\;Y_{\beta}\,\right] \,=\, X \cdot \beta$, for each $\beta \in \Re^{p}$
\end{itemize}
Then, \vskip -0.5cm
\begin{equation*}
\begin{array}{c}
	\textnormal{$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$ is a BLUE of}
	\\
	\textnormal{$\overset{{\color{white}+}}{\mathcal{L}}_{X} : \Re^{p} \longrightarrow \Re^{n}$}
	\\
	\textnormal{in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$}
		{\color{white}\overset{+}{\Re}}
	\end{array}
\quad\Longleftrightarrow\quad
\begin{array}{c}
	\textnormal{$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{1}$ is a BLUE of}
	\\
	\textnormal{$\overset{{\color{white}+}}{\mathcal{L}}_{\,\gamma^{T}} \circ \mathcal{L}_{X} :
		\Re^{p} \longrightarrow \Re^{1}$}
	\\
	\textnormal{in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$\,,}
		{\color{white}\overset{+}{\Re}}
	\\
	\textnormal{for every $\gamma \in \Re^{n \times 1}$\,.}
	\end{array}
\end{equation*}
\end{proposition}
\proof
\vskip 0.1cm
\noindent
\textbf{\underline{(\,$\Longrightarrow$\,)}}\quad
Suppose
\,$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$\,
is a BLUE of
\,$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$.

\vskip 0.5cm
\noindent
\textbf{Claim 1:}\quad
$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{1}$\,
is a linear unbiased estimator of
\,$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of \,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$.
\vskip 0.1cm
\noindent
Proof of Claim 1:\quad
First, note that, for each $\beta\in\Re^{p}$, we have
$E\!\left[\; \mathcal{L}_{A}(Y_{\beta})\;\right] \,=\, \mathcal{L}_{X}(\,\beta\,) \,=\, X \cdot \beta$.
Hence,
$E\!\left[\;\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{A}(Y_{\beta})\;\right]$
\,$=$\, $E\!\left[\;\gamma^{T} \cdot \mathcal{L}_{A}(Y_{\beta})\;\right]$
\,$=$\, $\gamma^{T} \cdot E\!\left[\; \mathcal{L}_{A}(Y_{\beta})\;\right]$
\,$=$\, $\gamma^{T} \cdot \mathcal{L}_{X}(\,\beta\,)$
\,$=$\, $\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{X}(\,\beta\,)$,\,
for each $\beta\in\Re^{p}$.
This proves Claim 1.

\vskip 0.5cm
\noindent
Next, let
\,$L : \Re^{n} \longrightarrow \Re^{1}$\,
be an arbitrary linear unbiased estimator of
\,$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of \,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$.
The proof of the forward implication will be complete once we show that
\begin{equation*}
\Var\!\left(\,\xi\cdot\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(Y_{\beta})\,\right)
\;\; \leq \;\;
	\Var\!\left(\,\xi \cdot L(Y_{\beta})\,\right),
\quad
\textnormal{for each \,$\xi \in \Re^{1}$, \,$\beta\in\Re^{p}$}\,,
\end{equation*}
which we will establish as Claim 4 below.
Now, since \,$L : \Re^{n} \longrightarrow \Re^{1}$\,
is a linear map, it has the following form:
there exists $v \in \Re^{n \times 1}$ such that
\,$L(y) \,=\, v^{T} \cdot y$,\,
for each \,$y \in \Re^{n}$.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
$v \,=\, \gamma + w$,\, for some \,$w \,\in\, \Col(X)^{\perp}$.
\vskip 0.1cm
\noindent
Proof of Claim 2:\quad
Note that, for each $\beta \in \Re^{p}$, we have
\,$v^{T}\cdot X \cdot \beta$
\,$=$\, $v^{T} \cdot E\!\left[\;Y_{\beta}\,\right]$
\,$=$\, $E\!\left[\,v^{T} \cdot Y_{\beta}\,\right]$
\,$=$\, $E\!\left[\,L(Y_{\beta})\,\right]$
\,$=$\, $\gamma^{T} \cdot X \cdot \beta$.
Hence,
\,$(v - \gamma)^{T} \cdot X \cdot \beta \,=\, 0$,\,
for each $\beta\in\Re^{p}$\,;
equivalently, $v - \gamma \in \Col(X)^{\perp}$.
Claim 2 now follows immediately.

\vskip 0.5cm
\noindent
\textbf{Claim 3:}\quad
$\Var\!\left(\,\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(Y_{\beta})\,\right)
\,\leq\,
	\Var\!\left(\,L(Y_{\beta})\,\right)$,\,
for each \,$\beta\in\Re^{p}$.
\vskip 0.1cm
\noindent
Proof of Claim 3:\quad
First, observe that
\,$E\!\left[\,\mathcal{L}_{I_{n}}(Y_{\beta})\,\right]$
\,$=$\, $E\!\left[\,I_{n} \cdot Y_{\beta}\,\right]$
\,$=$\, $E\!\left[\;Y_{\beta}\,\right]$
\,$=$\, $X \cdot \beta$
\,$=$\, $\mathcal{L}_{X}(\beta)$,\,
i.e. $\mathcal{L}_{I_{n}}$ is a linear unbiased estimator of $\mathcal{L}_{X}$
in terms of $Y = \{\,Y_{\beta}\,\}$.
Thus, by the hypothesis that $\mathcal{L}_{A}$ is a BLUE of $\mathcal{L}_{X}$ in terms of $Y = \{\,Y_{\beta}\,\}$,
we have
$\Var\!\left(\,\xi^{T} \cdot A \cdot Y_{\beta}\,\right)$
\,$\leq$\, $\Var\!\left(\,\xi^{T} \cdot I_{n} \cdot Y_{\beta}\,\right)$
\,$=$\, $\Var\!\left(\,\xi^{T} \cdot Y_{\beta}\,\right)$,\,
for each $\xi \in \Re^{n}$, $\beta\in\Re^{p}$.
Consequently,
\begin{eqnarray*}
\Var\!\left(\; \mathcal{L}_{\gamma^{T}} \circ \mathcal{L}_{A}(Y_{\beta}) \;\right)
&=&
	\Var\!\left(\; \gamma^{T} \cdot \mathcal{L}_{A}(Y_{\beta}) \;\right)
\;\; = \;\;
	\Var\!\left(\; \gamma^{T} \cdot A \cdot Y_{\beta} \;\right)
\;\; \leq \;\;
	\Var\!\left(\; \gamma^{T} \cdot Y_{\beta} \;\right)
\\
&\leq&
	\Var\!\left(\; \gamma^{T} \cdot Y_{\beta} \;\right)
	\; + \;
	\Var\!\left(\; w^{T} \cdot Y_{\beta} \;\right)
\;\;=\;\;
	\Var\!\left(\; (\gamma + w)^{T} \cdot Y_{\beta} \;\right)
\;\; = \;\;
	\Var\!\left(\; v^{T} \cdot Y_{\beta} \;\right)
\\
&=&
	\Var\!\left(\; L(Y_{\beta}) \;\right),
\quad
\textnormal{for each \,$\beta\in\Re^{p}$}\,.
\end{eqnarray*}
This proves Claim 3.

\vskip 0.5cm
\noindent
\textbf{Claim 4:}\quad
$\Var\!\left(\,\xi\cdot\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(Y_{\beta})\,\right)
\,\leq\,
	\Var\!\left(\,\xi \cdot L(Y_{\beta})\,\right)$,\,
for each \,$\xi\in\Re^{1}$, \,$\beta\in\Re^{p}$.
\vskip 0.1cm
\noindent
Proof of Claim 4:\quad
Simply note that
\begin{equation*}
\Var\!\left(\,\xi\cdot\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(Y_{\beta})\,\right)
\;\; = \;\;
	\xi^{2}\cdot\Var\!\left(\,\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(Y_{\beta})\,\right)
\;\; \leq \;\;
	\xi^{2}\cdot\Var\!\left(\,L(Y_{\beta})\,\right)
\;\; = \;\;
	\Var\!\left(\,\xi\cdot L(Y_{\beta})\,\right),
\end{equation*}
where the middle inequality follows immediately by Claim 3.
This proves Claim 4.


\vskip 0.5cm
\noindent
The forward implication now follows from Claim 1 and Claim 4.

\vskip 0.5cm
\noindent
\textbf{\underline{(\,$\Longleftarrow$\,)}}\quad
Suppose
\,$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{1}$\,
is a BLUE of
\,$\overset{{\color{white}+}}{\mathcal{L}}_{\,\gamma^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$,\,
for every $\gamma \in \Re^{n \times 1}$.
The reverse implication is precisely the validity of Claim 1 and Claim 2 below.

\vskip 0.5cm
\noindent
\textbf{Claim 1:}\quad
$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$\,
is a linear unbiased estimator of
\,$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}$, i.e.
\begin{equation*}
E\!\left[\;\mathcal{L}_{A}(\,Y_{\beta})\;\right]
\;=\;
	\mathcal{L}_{X}(\,\beta\,)
\;=\;
	X \cdot \beta\,,
\quad
\textnormal{for each \,$\beta\in\Re^{p}$}.
\end{equation*}
Proof of Claim 1:\quad
Since \,$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{1}$\,
is a BLUE of
\,$\overset{{\color{white}+}}{\mathcal{L}}_{\,\gamma^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$,
we have
\begin{equation*}
\gamma^{T} \cdot A \cdot X \cdot \beta
\;=\;
	\gamma^{T} \cdot A \cdot E\!\left[\;Y_{\beta}\,\right]
\;=\;
	E\!\left[\;\gamma^{T} \cdot A \cdot Y_{\beta}\;\right]
\;=\;
	E\!\left[\;\mathcal{L}_{\gamma^{T}} \circ \mathcal{L}_{A}(Y_{\beta})\;\right]
\;=\;
	\gamma^{T} \cdot X \cdot \beta\,,
\end{equation*}
for each $\gamma \in \Re^{n}$ and each $\beta \in \Re^{p}$.
This in turn implies
$A \cdot X \cdot \beta \;=\; X \cdot \beta$,
for each \,$\beta \in \Re^{p}$.
(In particular, the map
\,$\Re^{n} \longrightarrow \Re^{n} : y \longmapsto A \cdot y$\,
restricts to the identity map on $\Col(X)$.)
Thus,
\begin{equation*}
E\!\left[\;\mathcal{L}_{A}(Y_{\beta})\;\right]
\;\;=\;\;
	E\!\left[\,A \cdot Y_{\beta}\;\right]
\;\;=\;\;
	A \cdot E\!\left[\;Y_{\beta}\,\right]
\;\;=\;\;
	A \cdot X \cdot \beta
\;\;=\;\;
	X \cdot \beta\,,
\quad
\textnormal{for each \,$\beta \in \Re^{p}$}.
\end{equation*}
This proves Claim 1.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
For each linear unbiased estimator
\,$L : \Re^{n} \longrightarrow \Re^{n}$\,
of
\,$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$,\,
we have:
\begin{equation*}
\Var\!\left(\,\gamma^{T}\cdot\mathcal{L}_{A}(\,Y_{\beta}\,)\,\right)
\;\leq\;
	\Var\!\left(\,\gamma^{T} \cdot L(\,Y_{\beta}\,)\,\right),\,
\quad
\textnormal{for each \,$\gamma\in\Re^{n}$, \,$\beta\in\Re^{p}$}.
\end{equation*}
\vskip 0.1cm
\noindent
Proof of Claim 2:\quad
Since $L : \Re^{n} \longrightarrow \Re^{n}$ is a linear unbiased estimator of
$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$ in terms of $Y = \{\,Y_{\beta}\,\}$,
we have, for each $\gamma \in \Re^{n}$ and $\beta \in \Re^{p}$,
\begin{equation*}
E\!\left[\;\mathcal{L}_{\gamma^{T}} \circ L(\,Y_{\beta}\,)\;\right]
\;\; = \;\; E\!\left[\;\gamma^{T} \cdot L(\,Y_{\beta}\,)\;\right]
\;\; = \;\; \gamma^{T} \cdot E\!\left[\;L(\,Y_{\beta}\,)\;\right]
\;\; = \;\; \gamma^{T} \cdot \mathcal{L}_{X}(\,\beta\,)
\;\; = \;\; \gamma^{T} \cdot X \cdot \beta
\;\; = \;\; \mathcal{L}_{\gamma^{T}} \circ \mathcal{L}_{X} (\,\beta\,)\,.
\end{equation*}
Hence, for each $\gamma \in \Re^{n}$,
\,$\mathcal{L}_{\gamma^{T}} \circ L : \Re^{n} \longrightarrow \Re^{1}$\,
is a linear unbiased estimator of 
\,$\mathcal{L}_{\gamma^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of \,$Y = \{\,Y_{\beta}\,\}$.\,
Consequently, since
\,$\mathcal{L}_{\,\gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{1}$\,
is a BLUE of
\,$\overset{{\color{white}+}}{\mathcal{L}}_{\,\gamma^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$, for each $\gamma\in\Re^{n}$,
we have
\begin{equation*}
\Var\!\left(\;\xi\cdot\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(\,Y_{\beta}\,)\;\right)
\;\; \leq \;\;
	\Var\!\left(\;\xi\cdot\mathcal{L}_{\gamma^{T}}\circ L(\,Y_{\beta}\,)\;\right)\,,
\quad
\textnormal{for each \,$\xi \in \Re^{1},\, \gamma\in\Re^{n},\, \beta\in\Re^{p}$}.
\end{equation*}
In particular, the above inequality holds for $\xi = 1$, i.e.
\begin{equation*}
\Var\!\left(\;\mathcal{L}_{\gamma^{T}}\circ\mathcal{L}_{A}(\,Y_{\beta}\,)\;\right)
\;\; \leq \;\;
	\Var\!\left(\;\mathcal{L}_{\gamma^{T}}\circ L(\,Y_{\beta}\,)\;\right)\,,
\quad
\textnormal{for each \,$\gamma\in\Re^{n},\, \beta\in\Re^{p}$}.
\end{equation*}
This proves Claim 2.

\vskip 0.5cm
\noindent
This completes the proof of the present Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\vskip 0.8cm
\begin{proposition}
\label{BLUEArbitraryD}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$\, and 
	\,$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$ is left multiplication by $X \in \Re^{n \times p}$.
\item
	$A \in \Re^{n \times n}$\, and 
	\,$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$ is left multiplication by $A \in \Re^{n \times n}$.
\item
	$Y = \left\{\;
		\left.
		\overset{{\color{white}.}}{Y}_{\beta} : (\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta}) \longrightarrow \Re^{n}
		\;\;\right\vert\;
		\beta \in \Re^{p}
		\;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$,
	of $\Re^{n}$-valued random variables defined respectively on the
	probability spaces $(\Omega_{\beta},\mathcal{A}_{\beta},\mu_{\beta})$.
\item
	$E\!\left[\;Y_{\beta}\,\right] \,=\, X \cdot \beta$, for each $\beta \in \Re^{p}$
\item
	$\Lambda : \Re^{p} \longrightarrow \Re^{q}$ is an $X$-estimable linear map, i.e.
	there exists \,$\Gamma \in \Re^{n \times q}$\, such that
	\begin{equation*}
	\Lambda(\,\beta\,) \;=\; \Gamma^{T} \cdot X \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}.
	\end{equation*}
\end{itemize}
Then, \vskip -0.5cm
\begin{equation*}
\begin{array}{c}
	\textnormal{$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$ is a BLUE of}
	\\
	\textnormal{$\overset{{\color{white}+}}{\mathcal{L}}_{X} : \Re^{p} \longrightarrow \Re^{n}$}
	\\
	\textnormal{in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$}
		{\color{white}\overset{+}{\Re}}
	\end{array}
\quad\Longrightarrow\quad
\begin{array}{c}
	\textnormal{$\mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{q}$ is a BLUE of}
	\\
	\textnormal{$\Lambda \,=\, \overset{{\color{white}+}}{\mathcal{L}}_{\,\Gamma^{T}} \circ \mathcal{L}_{X} \, : \,
		\Re^{p} \longrightarrow \Re^{q}$}
	\\
	\textnormal{in terms of $Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$\,.}
		{\color{white}\overset{+}{\Re}}
	\end{array}
\end{equation*}
\end{proposition}
\proof
The Proposition is equivalent to the validity of Claim 1 and Claim 2 below.
\vskip 0.3cm
\noindent
\textbf{Claim 1:}\quad
$\mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{q}$\,
is a linear unbiased estimator of
\,$\Lambda = \mathcal{L}_{\,\Gamma^{T}}\circ\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{q}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}$, equivalently,
\begin{equation*}
E\!\left[\;\mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{A}(\,Y_{\beta})\;\right]
\;=\;
	\mathcal{L}_{\,\Gamma^{T}}\circ\mathcal{L}_{X}(\,\beta\,)
\;=\;
	\Lambda(\,\beta\,)\,,
\quad
\textnormal{for each \,$\beta\in\Re^{p}$}.
\end{equation*}
Proof of Claim 1:\quad
Since \,$\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$\,
is a BLUE of
\,$\overset{{\color{white}+}}{\mathcal{L}}_{X} : \Re^{p} \longrightarrow \Re^{n}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}_{\beta\in\Re^{p}}$,
we have
\begin{equation*}
E\!\left[\;\mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{A}(\,Y_{\beta}\,)\;\right]
\;\; = \;\;
	\Gamma^{T} \cdot E\!\left[\;\mathcal{L}_{A}(\,Y_{\beta}\,)\,\right]
\;\; = \;\;
	\Gamma^{T} \cdot X \cdot \beta
\;\; = \;\;
	\mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{X}(\,\beta\,)
\;\; = \;\;
	\Lambda(\,\beta\,)\,,
\quad
\textnormal{for each \,$\beta \in \Re^{p}$}.
\end{equation*}
This proves Claim 1.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
For each linear unbiased estimator
\,$L : \Re^{n} \longrightarrow \Re^{q}$\,
of
\,$\Lambda = \mathcal{L}_{\,\Gamma^{T}}\circ\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{q}$\,
in terms of
\,$Y = \{\,Y_{\beta}\,\}$,\,
we have
\begin{equation*}
\Var\!\left[\;\,\xi^{T} \cdot \mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{A}(\,Y_{\beta}\,)\;\,\right]
\;\leq\;
	\Var\!\left[\;\,\xi^{T} \cdot L(\,Y_{\beta}\,)\;\,\right],
\quad
\textnormal{for each \,$\xi\in\Re^{q},\, \beta\in\Re^{p}$}.
\end{equation*}
Proof of Claim 2:\quad
First, note that, for each $\xi \in \Re^{q}$,
\begin{equation*}
E\!\left[\;\mathcal{L}_{\xi^{T}} \circ L(\,Y_{\beta}\,)\;\right]
\; = \;
	E\!\left[\;\xi^{T} \cdot L(\,Y_{\beta}\,)\;\right]
\; = \;
	\xi^{T} \cdot E\!\left[\;L(\,Y_{\beta}\,)\;\right]
\; = \;
	\xi^{T} \cdot \Gamma^{T} \cdot X \cdot \beta
\; = \;
	\mathcal{L}_{\,(\Gamma\cdot\xi)^{T}} \circ \mathcal{L}_{X}(\,\beta\,)\,,
\quad
\textnormal{for each \,$\beta\in\Re^{p}$}.
\end{equation*}
Thus,
\,$\mathcal{L}_{\xi^{T}} \circ L : \Re^{n} \longrightarrow \Re^{1}$\,
is a linear unbiased estimator of
\,$\mathcal{L}_{\,(\Gamma\cdot\xi)^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$\,
in terms of \,$Y = \{\,Y_{\beta}\,\}$.
Next, observe that, for each $\xi \in \Re^{q \times 1}$,
we have $\Gamma \cdot \xi \in \Re^{n \times 1}$.
Since $\mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{n}$
is a BLUE of
$\mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{n}$
in terms of $Y = \{\,Y_{\beta}\,\}$,
by Proposition \ref{BLUEoneD}, we have
$\mathcal{L}_{\,(\Gamma\cdot\xi)^{T}} \circ \mathcal{L}_{A} : \Re^{n} \longrightarrow \Re^{1}$
is a BLUE of 
$\mathcal{L}_{\,(\Gamma\cdot\xi)^{T}} \circ \mathcal{L}_{X} : \Re^{p} \longrightarrow \Re^{1}$
in terms of $Y = \{\,Y_{\beta}\,\}$, for each $\xi \in \Re^{q}$.
Thus, for each $\xi\in\Re^{q}$ and $\beta\in\Re^{p}$, we have
\begin{eqnarray*}
\Var\!\left[\;\,\xi^{T} \cdot \mathcal{L}_{\,\Gamma^{T}} \circ \mathcal{L}_{A}(\,Y_{\beta}\,)\;\,\right]
&=&
	\Var\!\left[\;\,\mathcal{L}_{\,(\Gamma\cdot\xi)^{T}} \circ \mathcal{L}_{A}(\,Y_{\beta}\,)\;\,\right]
\;\; = \;\;
	\Var\!\left[\;\,1 \cdot \mathcal{L}_{\,(\Gamma\cdot\xi)^{T}} \circ \mathcal{L}_{A}(\,Y_{\beta}\,)\;\,\right]
\\
& \leq &
	\Var\!\left[\;\,1 \cdot \mathcal{L}_{\xi^{T}} \circ L(\,Y_{\beta}\,)\;\,\right]
\;\; = \;\;
	\Var\!\left[\;\,\mathcal{L}_{\xi^{T}} \circ L(\,Y_{\beta}\,)\;\,\right]
\\
&=&
	\Var\!\left[\;\,\xi^{T} \cdot L(\,Y_{\beta}\,)\;\,\right]
\end{eqnarray*}
This proves Claim 2.

\vskip 0.5cm
\noindent
This completes the proof of the present Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
