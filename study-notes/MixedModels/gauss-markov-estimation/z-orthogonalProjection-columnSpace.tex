
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Orthogonal projections onto column spaces in $\Re^{n}$}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{proposition}[Theorem B.33, p.427, \cite{Christensen2011}]
\mbox{}\vskip 0.1cm\noindent
Suppose $A \in \Re^{n \times n}$.
Then, left multiplication by $A$, namely
\begin{equation*}
\Re^{n} \longrightarrow \Re^{n} : \xi \longmapsto A\cdot\xi\,,
\end{equation*}
defines the orthogonal projection from $\Re^{n}$ onto the column space
$\Col(A) \subset \Re^{n}$ of $A$
if and only if
\begin{equation*}
A \cdot A \; = \; A
\quad\;\;\textnormal{and}\quad\;\;
A^{T} \; = A\,.
\end{equation*}
\end{proposition}
\proof
\vskip 0.1cm
\noindent
\underline{\textbf{(\,$\Longrightarrow$\,)}}\quad
Suppose left multiplication by $A$ defines the orthogonal projection
from $\Re^{n}$ onto $\Col(A)$.
Let $v, w \in \Re^{n}$ be two arbitrary elements of $\Re^{n}$.
Write
\begin{equation*}
v \;=\; v_{1} \,+\, v_{2}
\quad\;\;\textnormal{and}\quad\;\;
w \;=\; w_{1} \,+\, w_{2}\,,
\end{equation*}
where $v_{1}, w_{1} \in \Col(A)$ and $v_{2}, w_{2} \in \Col(A)^{\perp}$.
Note that
\begin{equation*}
(I_{n} - A) \cdot v
\;\;=\;\;
	(I_{n} - A) \cdot (v_{1} + v_{2})
\;\;=\;\;
	v_{1} + v_{2} - A \cdot v_{1} - A \cdot v_{2}
\;\;=\;\;
	v_{1} + v_{2} - v_{1} - 0
\;\;=\;\;
	v_{2}\,,
\end{equation*}
and
\begin{equation*}
A\cdot w
\;\;=\;\;
	A\cdot(w_{1} + w_{2})
\;\;=\;\;
	A \cdot w_{1} + A \cdot w_{2}
\;\;=\;\;
	w_{1} + 0
\;\;=\;\;
	w_{1}\,.
\end{equation*}
Hence, the matrix $A \in \Re^{n \times n}$ satisfies:
\begin{equation*}
w^{T} \cdot A^{T} \cdot (I_{n} - A) \cdot v
\;\;=\;\;
	w_{1}^{T} \cdot v_{2}
\;\;=\;\;
	0\,,
\quad\textnormal{for each \,$v, w \in \Re^{n \times n}$}\,.
\end{equation*}
This implies \,$A^{T} \cdot (I_{n} - A) \,=\, 0 \,\in\, \Re^{n \times n}$;
equivalently, \,$A^{T} \,=\, A^{T} \cdot A$.
Taking the transpose on both sides, we have
\,$A \,=\, (A^{T} \cdot A)^{T} \,=\, A^{T} \cdot (A^{T})^{T} \,=\, A^{T} \cdot A$.
In particular, we have \,$A \,=\, A^{T} \cdot A \,=\, A^{T}$,\, i.e. $A$ is symmetric.
Furthermore, $A \cdot A \,=\, A^{T} \cdot A \,=\, A$, as desired.

\vskip 0.2cm
\noindent
\underline{\textbf{(\,$\Longleftarrow$\,)}}\quad
Suppose $A \in \Re^{n \times n}$ satisfies \,$A \cdot A \,=\, A$ and $A^{T} \,=\, A$.
Next, note that, for each $v \in \Col(A)$, we may write $v = A\cdot\beta$, for some $\beta\in\Re^{n}$.
Hence,
\begin{equation*}
A \cdot v
\;\;=\;\;
	A \cdot A \cdot \beta 
\;\;=\;\;
	A \cdot \beta 
\;\;=\;\;
	v
\end{equation*}
Furthermore, for each $w \in \Col(A)^{\perp}$, we have
\begin{equation*}
A \cdot w \;\;= \;\;A^{T} \cdot w \;\;=\;\; 0\,,
\end{equation*}
where the last equality from the fact that each column of $A$ lies in $\Col(A)$.
Thus, we see that left multiplication by $A$ restricts to the identity map on $\Col(A)$
and it restricts to the zero map on $\Col(A)^{\perp}$. This proves that left multiplication
by $A$ defines the orthogonal projection from $\Re^{n}$ onto $\Col(A)$.
\qed

\begin{lemma}[Lemma B.43, p.430, \cite{Christensen2011}]
\label{XGXtXeqX}
\mbox{}\vskip 0.1cm\noindent
Suppose $X \in \Re^{n \times p}$.
\begin{enumerate}
\item
	For any generalized inverse $G \in \Re^{p \times p}$ of \,$X^{T} \cdot X \in \Re^{p \times p}$,
	we have
	\begin{equation*}
	X \cdot G \cdot (X^{T} \cdot X) \;\; = \;\; X\,.
	\end{equation*}
\item
	For any two generalized inverse $G, H \in \Re^{p \times p}$ of \,$X^{T} \cdot X \in \Re^{p \times p}$,
	we have
	\begin{equation*}
	X \cdot G \cdot X^{T} \;\; = \;\; X \cdot H \cdot X^{T}\,.
	\end{equation*}
\end{enumerate}
\end{lemma}
\proof
Let $v \in \Re^{n}$, and write $v = v_{1} + v_{2}$, where
$v_{1} \in \Col(X)$ and $v_{2} \in \Col(X)^{\perp}$.
Since $v_{1} \in \Col(X)$, we have $v_{1} = X \cdot \beta$,
for some $\beta \in \Re^{p}$.
\begin{enumerate}
\item
	First, note that
	$X^{T} \cdot v = X^{T} \cdot (v_{1} + v_{2}) = X^{T} \cdot v_{1} = X^{T} \cdot X \cdot \beta$.
	Hence $v^{T} \cdot X = \beta^{T} \cdot X^{T} \cdot X$.
	We thus see that:
	\begin{equation*}
	v^{T}\left(\,X \overset{{\color{white}\vert}}{\cdot} G \cdot (X^{T} \cdot X)\,\right)
	\;\; = \;\;
		\beta^{T} \cdot (X^{T} \cdot X) \cdot G \cdot (X^{T} \cdot X)
	\;\; = \;\;
		\beta^{T} \cdot (X^{T} \cdot X)
	\;\; = \;\;
		v^{T} \cdot X\,.
	\end{equation*}
	Since $v \in \Re^{n \times n}$ is arbitrary, we may conclude that
	$X \cdot G \cdot (X^{T} \cdot X) \,=\, X$,
	as desired.
\item
	Observe that
	\begin{eqnarray*}
	X \cdot G \cdot X^{T} \cdot v
	& = &
		X \cdot G \cdot X^{T} \cdot (v_{1} + v_{2})
	\;\; = \;\;
		X \cdot G \cdot X^{T} \cdot X \cdot \beta
	\\
	& = &
		X \cdot \beta
	\\
	& = &
		X \cdot H \cdot X^{T} \cdot X \cdot \beta
	\;\; = \;\;
		X \cdot H \cdot X^{T} \cdot (v_{1} + v_{2})
	\\
	& = &
		X \cdot H \cdot X^{T} \cdot v
	\end{eqnarray*}
	Since $v \in \Re^{n}$ is arbitrary, we may conclude that
	$X \cdot G \cdot X^{T} \,=\, X \cdot H \cdot X^{T}$,
	as desired.
	\qed
\end{enumerate}

\begin{theorem}
\mbox{}\vskip 0.1cm\noindent
Suppose $X \in \Re^{n \times p}$.
Then, the following statements hold:
\begin{enumerate}
\item
	For each $v \in \Re^{n}$, $X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot v \in \Re^{n}$
	is independent of the choice of the generalized inverse $(X^{T} \cdot X)^{\dagger}$.
	Consequently, the following:
	\begin{equation*}
	\Re^{n} \; \longrightarrow \; \Re^{n}
	\;\; : \;\;
	v \; \longmapsto X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot v\,,
	\end{equation*}
	where $(X^{T} \cdot X)^{\dagger} \in \Re^{n \times n}$
	is any generalized inverse of the symmetric matrix
	$X^{T} \cdot X \in \Re^{n \times n}$, defines a map from $\Re^{n}$ into itself.
\item
	Furthermore, the map above equals the orthogonal projection
	from $\Re^{n}$ onto the column space $\Col(X) \subset \Re^{n}$.
\end{enumerate}
\end{theorem}
\proof
\begin{enumerate}
\item
	The well-definition of indicated map follows immediately from Lemma \ref{XGXtXeqX}(ii).
\item
Let $v \in \Re^{n}$, and decompose $v$ as $v = v_{1} + v_{2}$,
where $v_{1} \in \Col(X)$ and $v_{2} \in \Col(X)^{\perp}$.
Since $v_{1} \in \Col(X)$, we have $v_{1} = X\cdot\beta$,
for some $\beta \in \Re^{p}$.
Hence,
\begin{equation*}
X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot v_{1}
\;\; = \;\;
	X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot (X \cdot \beta)
\;\; = \;\;
	X \cdot (X^{T} \cdot X)^{\dagger} \cdot (X^{T} \cdot X) \cdot \beta
\;\; = \;\;
	X \cdot \beta
\;\; = \;\;
	v_{1}\,,
\end{equation*}
where the last equality follows from Lemma \ref{XGXtXeqX}(i).
On the other hand,
\begin{equation*}
X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot v_{2}
\;\; = \;\;
	0\,.
\end{equation*}
Hence,
\begin{equation*}
X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot v
\;\; = \;\;
	X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot (v_{1} + v_{2})
\;\; = \;\;
	X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot v_{1}
\;\; = \;\;
	v_{1}
\;\; = \;\;
	\proj_{\,\Col(X)}(\,v\,)\,.
\end{equation*}
Since $v \in \Re^{n}$ is arbitrary, we see that the left multiplication map on $\Re^{n}$
by $X \cdot (X^{T} \cdot X)^{\dagger} \cdot X^{T}$ indeed coincides with the
orthogonal projection map from $\Re^{n}$ onto $\Col(X)$.
This completes the proof of the Theorem.
\qed
\end{enumerate}

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$\, and \,$\Re^{n}$ is equipped with the (usual) Euclidean inner product.
\item
	$\proj_{\,\Col(X)} : \Re^{n} \longrightarrow \Col(X) \subset \Re^{n}$
	be the orthogonal projection from $\Re^{n}$ onto the linear subspace $\Col(X) \subset \Re^{n}$,
	where $\Col(X)$ denotes the column space of $X \in \Re^{n \times p}$.
\end{itemize}
Then, for each $y \in \Re^{n}$ and $\beta \in \Re^{p}$, we have
\begin{equation*}
\beta \;\;\in\;\; \underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;y - X \cdot \zeta\,\Vert^{2}\;\right\}
\quad\;\;\textnormal{\it if and only if}\quad\;\;
X\cdot\beta \;\; = \;\; \proj_{\,\Col(X)}\!\left(\,y\,\right)\,;
\end{equation*}
equivalently,
\begin{equation*}
\underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;y - X \cdot \zeta\,\Vert^{2}\;\right\}
\;\; = \;\;
	(X^{T} \cdot X)^{\dagger} \cdot X^{T} \cdot y
	\; + \;
	\Null(X)\,,
\end{equation*}
where \,$(X^{T} \cdot X)^{\dagger}$\, is any generalized inverse of the symmetric matrix
\,$X^{T} \cdot X$\, and $\Null(X) \subset \Re^{p}$ is the null space of $X$. 
\end{proposition}
\proof
First, recall that $\proj_{\,\Col(X)}(\,y\,) \,=\, \Pi_{X} \cdot y \,=\, X\cdot(X^{T}\cdot X)^{\dagger}\cdot X^{T} \cdot y$,
where $(X^{T} \cdot X)^{\dagger}$ is any generalized inverse of $X^{T} \cdot X$.

\vskip 0.5cm
\noindent
\textbf{Claim 1:}
\begin{equation*}
\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\;\; = \;\;
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\end{equation*}
Proof of Claim:
\begin{eqnarray*}
\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)
&=&
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y + \Pi_{X} \cdot y - X\cdot\beta\right)^{T}
	\cdot
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y + \Pi_{X} \cdot y - X\cdot\beta\right)
\\
&=&
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\\
&&
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}
	\cdot
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}
	\cdot
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
\end{eqnarray*}
But,
\begin{eqnarray*}
\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}
\cdot
\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
&=&
	y^{T} \cdot (I_{n} - \Pi_{X}^{T}) \cdot \Pi_{X} \cdot y
	\;-\;
	y^{T} \cdot (I_{n} - \Pi_{X}^{T}) \cdot X\cdot\beta
\\
&=&
	y^{T} \cdot (I_{n} - \Pi_{X}) \cdot \Pi_{X} \cdot y
	\;-\;
	y^{T} \cdot (I_{n} - \Pi_{X}) \cdot X\cdot\beta
\\
&=&
	y^{T} \cdot (\Pi_{X} - \Pi_{X}^{2}) \cdot y
	\;-\;
	y^{T} \cdot (X - \Pi_{X} \cdot X) \cdot\beta
\\
&=&
	y^{T} \cdot 0 \cdot y \;-\; y^{T} \cdot 0 \cdot \beta
\\
&=&
	0
\end{eqnarray*}
Similarly,
\,$\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T} \cdot \left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)$
\,$=$\, $0$.
Thus, we have:
\begin{equation*}
\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\;\; = \;\;
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\end{equation*}
This proves Claim 1.

\vskip 0.5cm
\noindent
Note that Claim 1 immediately implies:
\begin{equation*}
\underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;y - X \cdot \zeta\,\Vert^{2}\;\right\}
\;\; = \;\;
\underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;\Pi_{X} \cdot y - X \cdot \zeta\,\Vert^{2}\;\right\}
\;\; = \;\;
\left\{\;
	\left.
	\zeta \overset{{\color{white}\vert}}{\in} \Re^{p}
	\;\;\right\vert\;
	X \cdot \zeta \,=\, \Pi_{X} \cdot y \,=\, \proj_{\,\Col(X)}(\,y\,)
	\;\right\}
\end{equation*}
This completes the proof of the Proposition.
\qed

\begin{definition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$.
\item
	$y \in \Re^{n}$.
\item
	$\Lambda \in \Re^{q \times p}$,\,
	and
	\,$\mathcal{L}_{\Lambda} : \Re^{p} \longrightarrow \Re^{q}$ is left-multiplication by $\Lambda$,
	i.e. $\mathcal{L}_{\Lambda}(\beta) = \Lambda \cdot \beta$.
\end{itemize}
The set \,$\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$\,
of \textbf{$(y\,\vert X)$-least squares estimates} of
\,$\mathcal{L}_{\Lambda} : \Re^{p} \longrightarrow \Re^{q}$
is, by definition, the following:
\begin{eqnarray*}
\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)
&:=&
	\mathcal{L}_{\Lambda}\!\left(\;
		\underset{\zeta\,\in\,\Re^{p}}{\argmin}
		\left\{\;\left\Vert\;\overset{{\color{white}.}}{y} - X\cdot\zeta\;\right\Vert^{2}\;\right\}
		\;\right)
\;\; = \;\;
	\left\{\;\,
		\Lambda\cdot\beta \overset{{\color{white}.}}{\in} \Re^{q}
		\;\;\left\vert\;\;
		\beta \in \underset{\zeta\,\in\,\Re^{p}}{\argmin}
			\left\{\;\left\Vert\;\overset{{\color{white}.}}{y} - X\cdot\zeta\;\right\Vert^{2}\;\right\}
		\right.
		\;\right\}
\\
&=&
	\mathcal{L}_{\Lambda}\!\left(\;
		\left\{\;\,
			\beta \overset{{\color{white}.}}{\in} \Re^{p}
			\;\,\left\vert\;\;
			\overset{{\color{white}.}}{X} \cdot \beta \;=\; \Pi_{X} \cdot y
			\right.
			\;\right\}
		\;\right)
\;\; = \;\;
	\left\{\;\,
		\Lambda\cdot\beta \overset{{\color{white}.}}{\in} \Re^{q}
		\;\,\left\vert\;\;
		\overset{{\color{white}\vert}}{X} \cdot \beta \;=\; \Pi_{X} \cdot y
		\right.
		\;\right\}
\end{eqnarray*}
\end{definition}

\begin{theorem}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$.
\item
	$y \in \Re^{n}$.
\item
	$\Lambda \in \Re^{q \times p}$,\,
	and
	\,$\mathcal{L}_{\Lambda} : \Re^{p} \longrightarrow \Re^{q}$ is left-multiplication by $\Lambda$,
	i.e. $\mathcal{L}_{\Lambda}(\beta) = \Lambda \cdot \beta$.
\end{itemize}
Then,
\begin{enumerate}
\item
	The set \,$\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)$\,
	of \textbf{$(y\,\vert X)$-least squares estimates} of
	\,$\mathcal{L}_{\Lambda} : \Re^{p} \longrightarrow \Re^{q}$
	can be expressed as follows:
	\begin{equation*}
	\lse\!\left(\,\mathcal{L}_{\Lambda} \,;\, y\,\vert X\,\right)
	\;\; = \;\;
	\end{equation*}
\end{enumerate}
\end{theorem}


          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
