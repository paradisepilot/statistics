
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{Orthogonal projections onto column spaces in $\Re^{n}$}
\setcounter{theorem}{0}
\setcounter{equation}{0}

\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{proposition}[Theorem B.33, p.427, \cite{Christensen2011}]
\mbox{}\vskip 0.1cm\noindent
Suppose $A \in \Re^{n \times n}$.
Then, left multiplication by $A$, namely
\begin{equation*}
\Re^{n} \longrightarrow \Re^{n} : \xi \longmapsto A\cdot\xi\,,
\end{equation*}
defines the orthogonal projection from $\Re^{n}$ onto the column space
$\Col(A) \subset \Re^{n}$ of $A$
if and only if
\begin{equation*}
A \cdot A \; = \; A
\quad\;\;\textnormal{and}\quad\;\;
A^{T} \; = A\,.
\end{equation*}
\end{proposition}
\proof
\vskip 0.1cm
\noindent
\underline{\textbf{(\,$\Longrightarrow$\,)}}\quad
Suppose left multiplication by $A$ defines the orthogonal projection
from $\Re^{n}$ onto $\Col(A)$.
Let $v, w \in \Re^{n}$ be two arbitrary elements of $\Re^{n}$.
Write
\begin{equation*}
v \;=\; v_{1} \,+\, v_{2}
\quad\;\;\textnormal{and}\quad\;\;
w \;=\; w_{1} \,+\, w_{2}\,,
\end{equation*}
where $v_{1}, w_{1} \in \Col(A)$ and $v_{2}, w_{2} \in \Col(A)^{\perp}$.
Note that
\begin{equation*}
(I_{n} - A) \cdot v
\;\;=\;\;
	(I_{n} - A) \cdot (v_{1} + v_{2})
\;\;=\;\;
	v_{1} + v_{2} - A \cdot v_{1} - A \cdot v_{2}
\;\;=\;\;
	v_{1} + v_{2} - v_{1} - 0
\;\;=\;\;
	v_{2}\,,
\end{equation*}
and
\begin{equation*}
A\cdot w
\;\;=\;\;
	A\cdot(w_{1} + w_{2})
\;\;=\;\;
	A \cdot w_{1} + A \cdot w_{2}
\;\;=\;\;
	w_{1} + 0
\;\;=\;\;
	w_{1}\,.
\end{equation*}
Hence, the matrix $A \in \Re^{n \times n}$ satisfies:
\begin{equation*}
w^{T} \cdot A^{T} \cdot (I_{n} - A) \cdot v
\;\;=\;\;
	w_{1}^{T} \cdot v_{2}
\;\;=\;\;
	0\,,
\quad\textnormal{for each \,$v, w \in \Re^{n \times n}$}\,.
\end{equation*}
This implies \,$A^{T} \cdot (I_{n} - A) \,=\, 0 \,\in\, \Re^{n \times n}$;
equivalently, \,$A^{T} \,=\, A^{T} \cdot A$.
Taking the transpose on both sides, we have
\,$A \,=\, (A^{T} \cdot A)^{T} \,=\, A^{T} \cdot (A^{T})^{T} \,=\, A^{T} \cdot A$.
In particular, we have \,$A \,=\, A^{T} \cdot A \,=\, A^{T}$,\, i.e. $A$ is symmetric.
Furthermore, $A \cdot A \,=\, A^{T} \cdot A \,=\, A$, as desired.

\vskip 0.2cm
\noindent
\underline{\textbf{(\,$\Longleftarrow$\,)}}\quad
Suppose $A \in \Re^{n \times n}$ satisfies \,$A \cdot A \,=\, A$ and $A^{T} \,=\, A$.
Next, note that, for each $v \in \Col(A)$, we may write $v = A\cdot\beta$, for some $\beta\in\Re^{n}$.
Hence,
\begin{equation*}
A \cdot v
\;\;=\;\;
	A \cdot A \cdot \beta 
\;\;=\;\;
	A \cdot \beta 
\;\;=\;\;
	v
\end{equation*}
Furthermore, for each $w \in \Col(A)^{\perp}$, we have
\begin{equation*}
A \cdot w \;\;= \;\;A^{T} \cdot w \;\;=\;\; 0\,,
\end{equation*}
where the last equality from the fact that each column of $A$ lies in $\Col(A)$.
Thus, we see that left multiplication by $A$ restricts to the identity map on $\Col(A)$
and it restricts to the zero map on $\Col(A)^{\perp}$. This proves that left multiplication
by $A$ defines the orthogonal projection from $\Re^{n}$ onto $\Col(A)$.
\qed

\begin{proposition}
\mbox{}\vskip 0.1cm\noindent
Suppose:
\begin{itemize}
\item
	$X \in \Re^{n \times p}$\, and \,$\Re^{n}$ is equipped with the (usual) Euclidean inner product.
\item
	$\proj_{\,\Col(X)} : \Re^{n} \longrightarrow \Col(X) \subset \Re^{n}$
	be the orthogonal projection from $\Re^{n}$ onto the linear subspace $\Col(X) \subset \Re^{n}$,
	where $\Col(X)$ denotes the column space of $X \in \Re^{n \times p}$.
\end{itemize}
Then, for each $y \in \Re^{n}$ and $\beta \in \Re^{p}$, we have
\begin{equation*}
\beta \;\;\in\;\; \underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;y - X \cdot \zeta\,\Vert^{2}\;\right\}
\quad\;\;\textnormal{\it if and only if}\quad\;\;
X\cdot\beta \;\; = \;\; \proj_{\,\Col(X)}\!\left(\,y\,\right)\,.
\end{equation*}
\end{proposition}
\proof
First, recall that $\proj_{\,\Col(X)}(\,y\,) \,=\, \Pi_{X} \cdot y \,=\, X\cdot(X^{T}\cdot X)^{\dagger}\cdot X^{T} \cdot y$,
where $(X^{T} \cdot X)^{\dagger}$ is any generalized inverse of $X^{T} \cdot X$.

\vskip 0.5cm
\noindent
\textbf{Claim 1:}
\begin{equation*}
\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\;\; = \;\;
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\end{equation*}
Proof of Claim:
\begin{eqnarray*}
\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)
&=&
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y + \Pi_{X} \cdot y - X\cdot\beta\right)^{T}
	\cdot
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y + \Pi_{X} \cdot y - X\cdot\beta\right)
\\
&=&
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\\
&&
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}
	\cdot
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}
	\cdot
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
\end{eqnarray*}
But,
\begin{eqnarray*}
\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}
\cdot
\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
&=&
	y^{T} \cdot (I_{n} - \Pi_{X}^{T}) \cdot \Pi_{X} \cdot y
	\;-\;
	y^{T} \cdot (I_{n} - \Pi_{X}^{T}) \cdot X\cdot\beta
\\
&=&
	y^{T} \cdot (I_{n} - \Pi_{X}) \cdot \Pi_{X} \cdot y
	\;-\;
	y^{T} \cdot (I_{n} - \Pi_{X}) \cdot X\cdot\beta
\\
&=&
	y^{T} \cdot (\Pi_{X} - \Pi_{X}^{2}) \cdot y
	\;-\;
	y^{T} \cdot (X - \Pi_{X} \cdot X) \cdot\beta
\\
&=&
	y^{T} \cdot 0 \cdot y \;-\; y^{T} \cdot 0 \cdot \beta
\\
&=&
	0
\end{eqnarray*}
Similarly,
\,$\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T} \cdot \left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)$
\,$=$\, $0$.
Thus, we have:
\begin{equation*}
\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\;\; = \;\;
	\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)^{T}\cdot\left(\,y \overset{{\color{white}.}}{-} \Pi_{X} \cdot y\right)
	\;+\;
	\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)^{T}\cdot\left(\Pi_{X} \cdot y \overset{{\color{white}.}}{-} X\cdot\beta\right)
\end{equation*}
This proves Claim 1.

\vskip 0.5cm
\noindent
Note that Claim 1 immediately implies:
\begin{equation*}
\underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;y - X \cdot \zeta\,\Vert^{2}\;\right\}
\;\; = \;\;
\underset{\zeta\,\in\,\Re^{p}}{\argmin}\left\{\;\Vert\;\Pi_{X} \cdot y - X \cdot \zeta\,\Vert^{2}\;\right\}
\;\; = \;\;
\left\{\;
	\left.
	\zeta \overset{{\color{white}\vert}}{\in} \Re^{p}
	\;\;\right\vert\;
	X \cdot \zeta \,=\, \Pi_{X} \cdot y \,=\, \proj_{\,\Col(X)}(\,y\,)
	\;\right\}
\end{equation*}
This completes the proof of the Proposition.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
