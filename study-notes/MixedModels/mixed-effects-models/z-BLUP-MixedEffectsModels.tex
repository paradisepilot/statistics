
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{BLUP for mixed effects models}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Best linear unbiased predictor]
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
\item
	$Y^{(0)} = \left\{\,\left.Y^{(0)}_{\beta} : \Omega \longrightarrow \Re\,\;\right\vert\;\beta\in\Re^{p}\,\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = \left\{\;Y_{\beta} = \left.(\,Y^{(1)}_{\beta}, Y^{(2)}_{\beta}, \ldots, Y^{(n)}_{\beta}\,)
	: \Omega \longrightarrow \Re^{n}\,\;\right\vert\; \beta \in \Re^{p} \;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re^{n}$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\end{itemize}
Then:
\begin{enumerate}
\item
	An affine function
	\,$f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a_{0} + a^{T} \cdot y$\,
	(where $a_{0} \in \Re^{1}$ and $a \in \Re^{n}$) is said to be a
	{\color{red}linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$} if
	\begin{equation*}
	E\!\left[\;\overset{{\color{white}.}}{f}(Y_{\beta})\,\right]
	\;\; = \;\;
		E\!\left[\;\,Y^{(0)}_{\beta}\;\right],
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\item
	A linear unbiased predictor
	\,$f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a_{0} + a^{T} \cdot y$\,
	of \,$Y^{(0)}$ in terms of \,$Y$\, is said to be a
	{\color{red}best linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$} if,
	for each linear unbiased predictor $g : \Re^{n} \longrightarrow \Re^{1}$
	of \,$Y^{(0)}$ in terms of $Y$, we have
	\begin{equation*}
	E\!\left[\; \left(\,Y^{(0)}_{\beta } - \overset{{\color{white}.}}{f}(Y_{\beta})\,\right)^{2}\,\right]
	\;\; \leq \;\;
		E\!\left[\;\left(\,Y^{(0)}_{\beta } - \overset{{\color{white}.}}{g}(Y_{\beta})\,\right)^{2}\;\right],
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\end{enumerate}
\end{definition}

\begin{proposition}
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
	\,$X \in \Re^{n \times p}$.\,
	$x_{0} \in \Re^{1 \times p}$.
\item
	$Y^{(0)} = \left\{\,\left.Y^{(0)}_{\beta} : \Omega \longrightarrow \Re\,\;\right\vert\;\beta\in\Re^{p}\,\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = \left\{\;Y_{\beta} = \left.(\,Y^{(1)}_{\beta}, Y^{(2)}_{\beta}, \ldots, Y^{(n)}_{\beta}\,)
	: \Omega \longrightarrow \Re^{n}\,\;\right\vert\; \beta \in \Re^{p} \;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re^{n}$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	The quantities $X$, $x_{0}$, $Y^{(0)}$ and $Y$ satisfy the following properties:
	\begin{equation*}
	E\!\left[\;\overset{{\color{white}.}}{Y_{\beta}}\,\right] \; = \; X \cdot \beta\,,
	\quad\textnormal{and}\quad
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; = \; x_{0} \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\end{itemize}
If \,$f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a_{0} + a^{T} \cdot y$\,
(where $a_{0} \in \Re^{1}$ and $a \in \Re^{n}$) is a
\textbf{linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$},
then we in fact necessarily have:
\begin{equation*}
a_{0} \; = \; 0\,,
\quad\quad\textnormal{and}\quad\quad
x_{0} \; = \; a^{T} \cdot X
\end{equation*}
\end{proposition}
\proof
Note that $f : \Re^{n} \longrightarrow \Re^{1}$ being
a linear unbiased predictor of $Y^{(0)}$ in terms of $Y$ implies:
\begin{equation*}
x_{0} \cdot \beta
\;\; = \;\;
	E\!\left[\;Y^{(0)}_{\beta}\,\right]
\;\; = \;\;
	E\!\left[\,\overset{{\color{white}.}}{f}(Y_{\beta})\,\right]
\;\; = \;\;
	E\!\left[\,a_{0} + a^{T}\cdot Y_{\beta}\,\right]
\;\; = \;\;
	a_{0} + a^{T}\cdot E\!\left[\;Y_{\beta}\,\right]
\;\; = \;\;
	a_{0} + a^{T}\cdot X \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta\in\Re^{p}$}\,,
\end{equation*}
which in turn implies
\begin{equation*}
a_{0}
\;\; = \;\;
	\left(\,x_{0} - a^{T}\cdot X\,\right) \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta\in\Re^{p}$}\,.
\end{equation*}
Since the left-hand side of the preceding equation does not involve $\beta$
while the right-hand side does, we see that both sides must in fact be zero for the equality to hold.
In other words, we must have $a_{0} = 0$ and that $a \in \Re^{n}$ satisfies $a^{T} \cdot X = x_{0}$.
This completes the proof of the present Proposition.
\qed

\vskip 1.0cm

\begin{theorem}[Theorem 12.2.3, p.294, \cite{Christensen2011}]
\label{formulaBLUP}
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
	\,$X \in \Re^{n \times p}$.\,
	$\rho \in \Re^{n \times 1}$.
\item
	$Y^{(0)} = \left\{\,\left.Y^{(0)}_{\beta} : \Omega \longrightarrow \Re\,\;\right\vert\;\beta\in\Re^{p}\,\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = \left\{\;Y_{\beta} = \left.(\,Y^{(1)}_{\beta}, Y^{(2)}_{\beta}, \ldots, Y^{(n)}_{\beta}\,)
	: \Omega \longrightarrow \Re^{n}\,\;\right\vert\; \beta \in \Re^{p} \;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re^{n}$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	The quantities $X$, $\rho$, $Y^{(0)}$ and $Y$ satisfy the following properties:
	\begin{equation*}
	E\!\left[\;\overset{{\color{white}.}}{Y_{\beta}}\,\right] \; = \; X \cdot \beta\,,
	\quad\textnormal{and}\quad
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; = \; \rho^{T} \cdot X \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\item
	The following are constant in $\beta \in \Re^{p}$:
	\begin{equation*}
	\Re^{p} \longrightarrow \Re^{n \times n} : \beta \longmapsto \Cov\!\left(\,Y_{\beta}\,\right)
	\quad\quad\textnormal{and}\quad\quad
	\Re^{p} \longrightarrow \Re^{n \times 1} : \beta \longmapsto \Cov\!\left(\,Y_{\beta}\,,\,Y^{(0)}_{\beta}\right).
	\end{equation*}
	We will use respectively $\Cov(\,Y\,)\in\Re^{n \times n}$ and $\Cov(\,Y,Y^{(0)}) \in \Re^{n \times 1}$
	to denote the images of the above two constant matrix-valued functions.
%\item
%	Define:
%	\begin{eqnarray*}
%	\xi & := & Z \cdot \gamma + e
%	\\
%	V & := & \Cov(Y) \;\; = \;\; \Cov\!\left(\,Z\cdot\gamma+e\,\right) \;\; = \;\; Z \cdot \Cov(\gamma) \cdot Z^{T} + \Cov(e)
%	\end{eqnarray*}
\end{itemize}
Then, the following statements hold:
\begin{enumerate}
\item
	There exists \,$\delta \in \Re^{n \times 1}$\, satisfying \,$\Cov(\,Y\,) \cdot \delta \,=\, \Cov(\,Y,Y^{(0)})$.
\item
	The map $f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a^{T} \cdot y$\,
	is a \textbf{\color{red}best linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$} if
	\,$\Re^{n} \longrightarrow \Re^{1} : y \longmapsto (a - \delta)^{T} \cdot y$\,
	is a best linear unbiased estimator of 
	\,$\Re^{p} \longrightarrow \Re^{1} : \beta \longmapsto (\rho - \delta)^{T} \cdot X \cdot \beta$\,.
\end{enumerate}
\end{theorem}

\begin{remark}
\mbox{}
\vskip 0.05cm
\noindent
Theorem \ref{formulaBLUP} is applicable to the scenario in which
the quantities $x_{0}$, $X$, $\Cov(Y)$ and $\Cov(\,Y,Y^{(0)})$ are {\color{red}known}, while
{\color{red}$\beta$ is unknown}.
In this scenario, Theorem \ref{formulaBLUP} gives the BLUP of
\,$Y^{(0)}$ in terms of the BLUE of $X\cdot\beta$.
Both the BLUP and BLUE here are affine functions of \,$Y$.

Next, recall that, if one knows all of $\beta$, $\Cov(Y)$ and $\Cov(Y,Y^{(0)})$,
then one can compute, for each $\beta\in\Re^{p}$,
the {\color{red}best linear predictor of \,$Y^{(0)}_{\beta}$ in terms of \,$Y_{\beta}$}:
\begin{equation*}
\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert Y_{\beta}\right)
\;\; = \;\;
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; + \; \delta^{T}\cdot(\;Y_{\beta} - E\!\left[\;Y_{\beta}\,\right]\,)
\;\; = \;\;
	x_{0}^{T}\cdot\beta \; + \; \delta^{T}\cdot(\;Y_{\beta} - X\cdot\beta\,)\,,
\end{equation*}
where $\delta \in \Re^{n \times 1}$ is any element in $\Re^{n}$ which satisfies
$\Cov(Y) \cdot \delta \,=\, \Cov(\,Y,Y^{(0)})$.

Thus the key point of Theorem \ref{formulaBLUP} is that,
if we know only $\Cov(Y)$ and $\Cov(\,Y,Y^{(0)})$ but not $\beta$, then we cannot
compute $\blp(\,Y^{(0)}\vert\,Y)$, but we can instead compute arguably the next best thing:
the BLUP of \,$Y^{(0)}$ in terms of \,$Y$, and that this BLUP happens to involves
the BLUE of the unknown parameter $\beta$.
\end{remark}

\proofof Theorem \ref{formulaBLUP}
\begin{enumerate}
\item
\item
	Suppose the map
	\,$\Re^{n} \longrightarrow \Re^{1} : y \longmapsto (a - \delta)^{T}\cdot y$,\,
	where $a \in \Re^{n}$,\, is a linear unbiased estimator of 
	\,$\Re^{p} \longrightarrow \Re^{1} : \beta \longmapsto (\rho - \delta)^{T} \cdot X \cdot \beta$\,
	in terms of $Y$.

	\vskip 0.3cm
	\noindent
	\textbf{Claim 1:}\quad
	For each \,$a \in \Re^{n \times 1}$, we have:
	\begin{equation*}
		\begin{array}{c}
		\textnormal{the map}
		\\
		f(\;\cdot\;;a) : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a^{T}\cdot y
		\\
		\textnormal{is a linear unbiased predictor}
		\\
		\textnormal{of \,$Y^{(0)}$ in terms of \,$Y$}
		\end{array}
	\quad\Longleftrightarrow\quad\;\;
		a^{T} \cdot X \; = \; \rho^{T} \cdot X
	\;\;\quad\Longleftrightarrow\quad
		\begin{array}{c}
		\textnormal{the map}
		\\
		g(\;\cdot\;;a) : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto (a - \delta)^{T}\cdot y
		\\
		\textnormal{is a linear unbiased estimator of}
		\\
		\Re^{p} \longrightarrow \Re^{1} : \beta \longmapsto (\rho - \delta)^{T} \cdot X \cdot \beta
		\\
		\textnormal{in terms of \,$Y$.}
		\end{array}
	\end{equation*}
	%the map
	%\,$f(\;\cdot\;;a) : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a^{T}\cdot y$\,
	%is a linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$.
	%\vskip 0.2cm
	\noindent
	Proof of Claim 1:\quad
	We first prove the first equivalence:
	\begin{eqnarray*}
	&&
		\textnormal{$f(\;\cdot\;;a)$ is a linear unbiased predictor of $Y^{(0)}$ in terms of $Y$}
	\\
	&\Longleftrightarrow&
		\rho^{T} \cdot X \cdot \beta
		\; = \;
			E\!\left[\;Y^{(0)}_{\beta}\,\right]
		\; = \;
			E\!\left[\;f(\,Y_{\beta};a)\;\right]
		\; = \;
			E\!\left[\,a^{T} \cdot Y_{\beta}\,\right]
		\; = \;
			a^{T} \cdot E\!\left[\;Y_{\beta}\,\right]
		\; = \;
			a^{T} \cdot X \cdot \beta\,,
		\;\; \forall \; \beta \in \Re^{p}
	\\
	&\Longleftrightarrow&
		\rho^{T} \cdot X \cdot \beta
		\; = \;
			a^{T} \cdot X \cdot \beta\,,
		\;\; \forall \; \beta \in \Re^{p}
	\\
	&\Longleftrightarrow&
		\rho^{T} \cdot X \; = \; a^{T} \cdot X
	\end{eqnarray*}
	Now, we prove the second equivalence:
	\begin{eqnarray*}
	&&
		\textnormal{$g(\;\cdot\;;a)$ is a linear unbiased estimator of
		\,$\Re^{p} \longrightarrow \Re^{1} : \beta \longmapsto (\rho - \delta)^{T} \cdot X \cdot \beta$\,
		in terms of $Y$}
	\\
	&\Longleftrightarrow&
		(\rho - \delta)^{T} \cdot X \cdot \beta
		\; = \;
			E\!\left[\;g(\,Y_{\beta};a)\;\right]
		\; = \;
			E\!\left[\,(a - \delta)^{T}\cdot Y_{\beta}\,\right]
		\; = \;
			(a - \delta)^{T} \cdot E\!\left[\;Y_{\beta}\,\right]
		\; = \;
			(a - \delta)^{T} \cdot X \cdot \beta\,,
		\; \forall\;\beta \in \Re^{p}
	\\
	&\Longleftrightarrow&
		(\rho - \delta)^{T} \cdot X \cdot \beta
		\; = \;
			(a - \delta)^{T} \cdot X \cdot \beta\,,
		\;\; \forall\;\beta \in \Re^{p}
	\\
	&\Longleftrightarrow&
		\rho^{T} \cdot X \cdot \beta \; = \; a^{T} \cdot X \cdot \beta\,,
		\;\; \forall\;\beta \in \Re^{p}
	\\
	&\Longleftrightarrow&
		\rho^{T} \cdot X \; = \; a^{T} \cdot X
	\end{eqnarray*}
	This completes the proof of Claim 1.

	\vskip 0.8cm
	\noindent
	Recall that, for each $\beta \in \Re^{p}$, the best linear predictor
	\,$\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert\,Y_{\beta}\right)$\, of
	\,$Y^{(0)}_{\beta}$ in terms of \,$Y_{\beta}$ is given by:
	\begin{equation*}
	\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert\,Y_{\beta}\right)
	\;\; = \;\;
		E\!\left[\;Y^{(0)}_{\beta}\,\right] \; + \; \delta^{T}\cdot(\;Y_{\beta} - E\!\left[\;Y_{\beta}\,\right]\,)
	\;\; = \;\;
		\rho^{T}\cdot X \cdot\beta \; + \; \delta^{T}\cdot(\;Y_{\beta} - \,X\cdot\beta\,\;)\,,
	\end{equation*}
	where $\delta \in \Re^{n \times 1}$ is any element in $\Re^{n}$ which satisfies
	$\Cov(Y) \cdot \delta \,=\, \Cov(Y,Y_{0})$.

	\vskip 0.8cm
	\noindent
	\textbf{Claim 2:}
	\begin{equation*}
	E\!\left[\,\left(\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert Y_{\beta}\right) \overset{{\color{white}.}}{-} f(Y_{\beta})\right)^{2}\,\right]
	\;\; = \;\;
		\Var\!\left[\;(a \overset{{\color{white}.}}{-} \delta)^{T}\cdot Y_{\beta}\;\right]
	\end{equation*}
	Proof of Claim 2:\quad Observe that
	\begin{eqnarray*}
	\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert Y_{\beta}\right) - f(Y_{\beta})
	&=&
		\rho^{T} \cdot X \cdot \beta \; + \; \delta^{T}\cdot(\;Y_{\beta} - \,X\cdot\beta\,) \; - \; a^{T} \cdot Y_{\beta}
	\;\; = \;\;
		(\rho \, - \, \delta)^{T} \cdot X \cdot \beta \; - \; (a - \delta)^{T} \cdot Y_{\beta}
	\\
	&=&
		- \left\{\;
			(a - \delta)^{T} \cdot Y_{\beta} \; - \; E\!\left[\,(a \overset{{\color{white}.}}{-} \delta)^{T} \cdot Y_{\beta}\,\right]
			 \;\right\}\,,
		\quad\textnormal{by Claim 1},
	\end{eqnarray*}
	from which Claim 2 immediately follows.

	\vskip 0.5cm
	\noindent
	\textbf{Claim 3:}\quad
	$f(\;\cdot\;;b) : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto b^{T} \cdot y$\,
	is a best linear unbiased predictor of \,$Y^{(0)}$\, in terms of \,$Y$\, if
	\begin{equation*}
	b \;\; = \;\;
		\underset{a\,\in\,\Re^{n}}{\textnormal{argmin}}
		\left\{\;
			\Var\!\left[\;(a \overset{{\color{white}.}}{-} \delta)^{T}\cdot Y_{\beta}\;\right]
			\;\right\}
	\end{equation*}
	Proof of Claim 3:\quad Recall that
	\begin{eqnarray*}
	E\!\left[\,\left(Y^{(0)}_{\beta} \overset{{\color{white}.}}{-} f(Y_{\beta};a)\right)^{2}\,\right]
	& = &
		E\!\left[\,
			\left(Y^{(0)}_{\beta}
			\overset{{\color{white}.}}{-}
			\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert Y_{\beta}\right)\right)^{2}
			\,\right]
		\; + \;
		E\!\left[\,
			\left(\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert Y_{\beta}\right)
			\overset{{\color{white}.}}{-}
			f(Y_{\beta};a)\right)^{2}
			\,\right]
	\\
	& = &
		E\!\left[\,
			\left(Y^{(0)}_{\beta}
			\overset{{\color{white}.}}{-}
			\blp\!\left(\left.Y^{(0)}_{\beta}\right\vert Y_{\beta}\right)\right)^{2}
			\,\right]
		\; + \;
		\Var\!\left[\;(a \overset{{\color{white}.}}{-} \delta)^{T}\cdot Y_{\beta}\;\right]
	\end{eqnarray*}
	Claim 3 now follows immediately.

	\vskip 0.5cm
	\noindent
	\textbf{Claim 4:}\quad
	$f(Y;0,b) = b^{T}\cdot Y$\, is a best linear unbiased predictor (BLUP) of \,$Y_{0}$ if
	$(b-\delta)^{T} \cdot Y$ is a best linear unbiased estimator (BLUE) of
	$(x_{0} - \delta^{T}\cdot X) \cdot \beta$.

	\vskip 0.2cm
	\noindent
	Proof of Claim 4: \quad Immediate by Claim 2 and Claim 3.
	
\end{enumerate}

%\vskip 0.5cm
%$E\!\left[\,f(Y;0,b)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$
%also implies that
%$(b - \delta)^{T}\cdot Y$ is a linear unbiased estimator of \,$(x_{0} - \delta^{T}\cdot X)\cdot\beta$.
%\vskip 0.2cm
%\noindent
%Proof of Claim 1:\quad

%\vskip 0.5cm
%\noindent
%\textbf{Claim 2:}\quad
%$E\!\left[\,f(Y;0,b)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$
%also implies that
%$(b - \delta)^{T}\cdot Y$ is a linear unbiased estimator of \,$(x_{0} - \delta^{T}\cdot X)\cdot\beta$.
%\vskip 0.2cm
%\noindent
%Proof of Claim 2:\quad
%Simply note that
%\begin{equation*}
%E\!\left[\,(b - \delta)^{T}\cdot Y\,\right]
%\;\; = \;\;
%	(b - \delta)^{T} \cdot E\!\left[\;Y\,\right]
%\;\; = \;\;
%	(b - \delta)^{T} \cdot X \cdot \beta
%\;\; = \;\;
%	(b^{T}\cdot X - \delta^{T} \cdot X) \cdot \beta
%\;\; = \;\;
%	(x_{0} - \delta^{T} \cdot X) \cdot \beta\,,
%\end{equation*}
%where the last equality follows from Claim 1.
%This proves Claim 2.

%\vskip 0.5cm
%\noindent
%Claim 2 and Claim 4 together imply that if
%$(b-\delta)^{T} \cdot Y$ is a best linear unbiased estimator of $(x_{0} - \delta^{T}\cdot X) \cdot \beta$,
%then
%\,$f(Y;0,b) = b^{T} \cdot Y$\, is a best linear unbiased predictor of \,$Y_{0}$.

\vskip 0.5cm
\noindent
This completes the proof of the present Theorem.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
