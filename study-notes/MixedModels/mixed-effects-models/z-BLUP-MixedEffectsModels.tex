
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{BLUP for mixed effects models}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{definition}[Best linear unbiased predictor]
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
\item
	$Y^{(0)} = \left\{\,\left.Y^{(0)}_{\beta} : \Omega \longrightarrow \Re\,\;\right\vert\;\beta\in\Re^{p}\,\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = \left\{\;Y_{\beta} = \left.(\,Y^{(1)}_{\beta}, Y^{(2)}_{\beta}, \ldots, Y^{(n)}_{\beta}\,)
	: \Omega \longrightarrow \Re^{n}\,\;\right\vert\; \beta \in \Re^{p} \;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re^{n}$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\end{itemize}
Then:
\begin{enumerate}
\item
	An affine function
	\,$f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a_{0} + a^{T} \cdot y$\,
	(where $a_{0} \in \Re^{1}$ and $a \in \Re^{n}$) is said to be a
	{\color{red}linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$} if
	\begin{equation*}
	E\!\left[\;\overset{{\color{white}.}}{f}(Y_{\beta})\,\right]
	\;\; = \;\;
		E\!\left[\;\,Y^{(0)}_{\beta}\;\right],
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\item
	A linear unbiased predictor
	\,$f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a_{0} + a^{T} \cdot y$\,
	of \,$Y^{(0)}$ in terms of \,$Y$\, is said to be a
	{\color{red}best linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$} if,
	for each linear unbiased predictor $g : \Re^{n} \longrightarrow \Re^{1}$
	of \,$Y^{(0)}$ in terms of $Y$, we have
	\begin{equation*}
	E\!\left[\; \left(\,Y^{(0)}_{\beta } - \overset{{\color{white}.}}{f}(Y_{\beta})\,\right)^{2}\,\right]
	\;\; \leq \;\;
		E\!\left[\;\left(\,Y^{(0)}_{\beta } - \overset{{\color{white}.}}{g}(Y_{\beta})\,\right)^{2}\;\right],
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\end{enumerate}
\end{definition}

\begin{proposition}
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
	\,$X \in \Re^{n \times p}$.\,
	$x_{0} \in \Re^{1 \times p}$.
\item
	$Y^{(0)} = \left\{\,\left.Y^{(0)}_{\beta} : \Omega \longrightarrow \Re\,\;\right\vert\;\beta\in\Re^{p}\,\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = \left\{\;Y_{\beta} = \left.(\,Y^{(1)}_{\beta}, Y^{(2)}_{\beta}, \ldots, Y^{(n)}_{\beta}\,)
	: \Omega \longrightarrow \Re^{n}\,\;\right\vert\; \beta \in \Re^{p} \;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re^{n}$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	The quantities $X$, $x_{0}$, $Y^{(0)}$ and $Y$ satisfy the following properties:
	\begin{equation*}
	E\!\left[\;\overset{{\color{white}.}}{Y_{\beta}}\,\right] \; = \; X \cdot \beta\,,
	\quad\textnormal{and}\quad
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; = \; x_{0} \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\end{itemize}
If \,$f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a_{0} + a^{T} \cdot y$\,
(where $a_{0} \in \Re^{1}$ and $a \in \Re^{n}$) is a
\textbf{linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$},
then we in fact necessarily have:
\begin{equation*}
a_{0} \; = \; 0\,,
\quad\quad\textnormal{and}\quad\quad
x_{0} \; = \; a^{T} \cdot X
\end{equation*}
\end{proposition}
\proof
Note that $f : \Re^{n} \longrightarrow \Re^{1}$ being
a linear unbiased predictor of $Y^{(0)}$ in terms of $Y$ implies:
\begin{equation*}
x_{0} \cdot \beta
\;\; = \;\;
	E\!\left[\;Y^{(0)}_{\beta}\,\right]
\;\; = \;\;
	E\!\left[\,\overset{{\color{white}.}}{f}(Y_{\beta})\,\right]
\;\; = \;\;
	E\!\left[\,a_{0} + a^{T}\cdot Y_{\beta}\,\right]
\;\; = \;\;
	a_{0} + a^{T}\cdot E\!\left[\;Y_{\beta}\,\right]
\;\; = \;\;
	a_{0} + a^{T}\cdot X \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta\in\Re^{p}$}\,,
\end{equation*}
which in turn implies
\begin{equation*}
a_{0}
\;\; = \;\;
	\left(\,x_{0} - a^{T}\cdot X\,\right) \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta\in\Re^{p}$}\,.
\end{equation*}
Since the left-hand side of the preceding equation does not involve $\beta$
while the right-hand side does, we see that both sides must in fact be zero for the equality to hold.
In other words, we must have $a_{0} = 0$ and that $a \in \Re^{n}$ satisfies $a^{T} \cdot X = x_{0}$.
This completes the proof of the present Proposition.
\qed

\vskip 1.0cm

\begin{theorem}[Theorem 12.2.3, p.294, \cite{Christensen2011}]
\label{formulaBLUP}
\mbox{}
\vskip 0.1cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
	\,$X \in \Re^{n \times p}$.\,
	$\rho \in \Re^{n \times 1}$.
\item
	$Y^{(0)} = \left\{\,\left.Y^{(0)}_{\beta} : \Omega \longrightarrow \Re\,\;\right\vert\;\beta\in\Re^{p}\,\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = \left\{\;Y_{\beta} = \left.(\,Y^{(1)}_{\beta}, Y^{(2)}_{\beta}, \ldots, Y^{(n)}_{\beta}\,)
	: \Omega \longrightarrow \Re^{n}\,\;\right\vert\; \beta \in \Re^{p} \;\right\}$
	is a family, indexed by $\beta \in \Re^{p}$, of \,$\Re^{n}$-valued random variables
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	The quantities $X$, $\rho$, $Y^{(0)}$ and $Y$ satisfy the following properties:
	\begin{equation*}
	E\!\left[\;\overset{{\color{white}.}}{Y_{\beta}}\,\right] \; = \; X \cdot \beta\,,
	\quad\textnormal{and}\quad
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; = \; \rho^{T} \cdot X \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta \in \Re^{p}$}\,.
	\end{equation*}
\item
	The following are constant in $\beta \in \Re^{p}$:
	\begin{equation*}
	\Re^{p} \longrightarrow \Re^{n \times n} : \beta \longmapsto \Cov\!\left(\,Y_{\beta}\,\right)
	\quad\quad\textnormal{and}\quad\quad
	\Re^{p} \longrightarrow \Re^{n \times 1} : \beta \longmapsto \Cov\!\left(\,Y_{\beta}\,,\,Y^{(0)}_{\beta}\right).
	\end{equation*}
	We will use respectively $\Cov(\,Y\,)\in\Re^{n \times n}$ and $\Cov(\,Y,Y^{(0)}) \in \Re^{n \times 1}$
	to denote the images of the above two constant matrix-valued functions.
%\item
%	Define:
%	\begin{eqnarray*}
%	\xi & := & Z \cdot \gamma + e
%	\\
%	V & := & \Cov(Y) \;\; = \;\; \Cov\!\left(\,Z\cdot\gamma+e\,\right) \;\; = \;\; Z \cdot \Cov(\gamma) \cdot Z^{T} + \Cov(e)
%	\end{eqnarray*}
\end{itemize}
Then, the following statements hold:
\begin{enumerate}
\item
	There exists \,$\delta \in \Re^{n \times 1}$\, satisfying \,$\Cov(\,Y\,) \cdot \delta \,=\, \Cov(\,Y,Y^{(0)})$.
\item
	The map $f : \Re^{n} \longrightarrow \Re^{1} : y \longmapsto a^{T} \cdot y$\,
	is a \textbf{\color{red}best linear unbiased predictor of \,$Y^{(0)}$ in terms of \,$Y$} if
	\,$\Re^{n} \longrightarrow \Re^{1} : y \longmapsto (a - \delta)^{T} \cdot y$\,
	is a best linear unbiased estimator of 
	\,$\Re^{p} \longrightarrow \Re^{1} : \beta \longmapsto (\rho - \delta)^{T} \cdot X \cdot \beta$\,.
\end{enumerate}
\end{theorem}

\begin{remark}
\mbox{}
\vskip 0.05cm
\noindent
Theorem \ref{formulaBLUP} is applicable to the scenario in which
the quantities $x_{0}$, $X$, $\Cov(Y)$ and $\Cov(Y,Y_{0})$ are {\color{red}known}, while
{\color{red}$\beta$ is unknown}.
In this scenario, Theorem \ref{formulaBLUP} gives the BLUP of
\,$Y^{(0)}$ in terms of the BLUE of $X\cdot\beta$.
Both the BLUP and BLUE here are affine functions of \,$Y$.

Next, recall that, if one knows all of $\beta$, $\Cov(Y)$ and $\Cov(Y,Y^{(0)})$,
then one can compute, for each $\beta\in\Re^{p}$,
the {\color{red}best linear predictor of \,$Y^{(0)}_{\beta}$ in terms of \,$Y_{\beta}$}:
\begin{equation*}
\blp(\,Y^{(0)}_{\beta}\vert\,Y_{\beta})
\;\; = \;\;
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; + \; \delta^{T}\cdot(\;Y_{\beta} - E\!\left[\;Y_{\beta}\,\right]\,)
\;\; = \;\;
	x_{0}^{T}\cdot\beta \; + \; \delta^{T}\cdot(\;Y_{\beta} - X\cdot\beta\,)\,,
\end{equation*}
where $\delta \in \Re^{n \times 1}$ is any element in $\Re^{n}$ which satisfies
$\Cov(Y) \cdot \delta \,=\, \Cov(Y,Y^{(0)})$.

Thus the key point of Theorem \ref{formulaBLUP} is that,
if we know only $\Cov(Y)$ and $\Cov(Y,Y^{(0)})$ but not $\beta$, then we cannot
compute $\blp(\,Y^{(0)}\vert\,Y)$, but we can instead compute arguably the next best thing:
the BLUP of \,$Y^{(0)}$ in terms of \,$Y$, and that this BLUP happens to involves
the BLUE of the unknown parameter $\beta$.
\end{remark}

\proofof Theorem \ref{formulaBLUP}

\vskip 0.3cm
%\noindent
%Let $f(Y;b_{0},b) \,:=\, b_{0} + b^{T}\cdot Y$, where $b_{0} \in \Re$ and $b \in \Re^{n}$,
%be a linear unbiased predictor of $Y_{0}$.

%\vskip 0.3cm
%\noindent
%\textbf{Claim 1:}\quad $E\!\left[\,f(Y;b_{0},b)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$
%\;\;$\Longrightarrow$\;\; $b_{0} = 0$\,, \;and\; $b \in \Re^{n}$ satisfies $b^{T}\cdot X = x_{0}$.
%\mbox{}\vskip 0.2cm\noindent
%Proof of Claim 1:\quad Observe that $E\!\left[\,f(Y)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$ implies:
%\begin{equation*}
%x_{0} \cdot \beta
%\;\; = \;\;
%	E\!\left[\;Y_{0}\,\right]
%\;\; = \;\;
%	E\!\left[\,f(Y)\,\right]
%\;\; = \;\;
%	E\!\left[\,b_{0} + b^{T}\cdot Y\,\right]
%\;\; = \;\;
%	b_{0} + b^{T}\cdot E\!\left[\;Y\,\right]
%\;\; = \;\;
%	b_{0} + b^{T}\cdot X \cdot \beta\,,
%	\quad
%	\textnormal{for each \,$\beta\in\Re^{n}$}\,,
%\end{equation*}
%which in turn implies
%\begin{equation*}
%b_{0}
%\;\; = \;\;
%	\left(\,x_{0} - b^{T}\cdot X\,\right) \cdot \beta\,,
%	\quad
%	\textnormal{for each \,$\beta\in\Re^{p}$}\,.
%\end{equation*}
%Since the left-hand side of the preceding equation does not involve $\beta$
%while the right-hand side does, we see that both sides must in fact be zero for the equality to hold.
%In other words, we must have $b_{0} = 0$ and that $b \in \Re^{n}$ satisfies $b^{T} \cdot X = x_{0}$.
%This proves Claim 1.

\noindent
Recall that, for each $\beta \in \Re^{p}$, the best linear predictor
\,$\blp\!\left(\,\left.Y^{(0)}_{\beta}\right\vert\,Y_{\beta}\right)$\, of
\,$Y^{(0)}_{\beta}$ in terms of \,$Y_{\beta}$ is given by:
\begin{equation*}
\blp\!\left(\,\left.Y^{(0)}_{\beta}\right\vert\,Y_{\beta}\right)
\;\; = \;\;
	E\!\left[\;Y^{(0)}_{\beta}\,\right] \; + \; \delta^{T}\cdot(\;Y_{\beta} - E\!\left[\;Y_{\beta}\,\right]\,)
\;\; = \;\;
	x_{0}\cdot\beta \; + \; \delta^{T}\cdot(\;Y_{\beta} - \,X\cdot\beta\,\;)\,,
\end{equation*}
where $\delta \in \Re^{n \times 1}$ is any element in $\Re^{n}$ which satisfies
$\Cov(Y) \cdot \delta \,=\, \Cov(Y,Y_{0})$.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
$E\!\left[\,f(Y;0,b)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$
also implies that
$(b - \delta)^{T}\cdot Y$ is a linear unbiased estimator of \,$(x_{0} - \delta^{T}\cdot X)\cdot\beta$.
\vskip 0.2cm
\noindent
Proof of Claim 2:\quad
Simply note that
\begin{equation*}
E\!\left[\,(b - \delta)^{T}\cdot Y\,\right]
\;\; = \;\;
	(b - \delta)^{T} \cdot E\!\left[\;Y\,\right]
\;\; = \;\;
	(b - \delta)^{T} \cdot X \cdot \beta
\;\; = \;\;
	(b^{T}\cdot X - \delta^{T} \cdot X) \cdot \beta
\;\; = \;\;
	(x_{0} - \delta^{T} \cdot X) \cdot \beta\,,
\end{equation*}
where the last equality follows from Claim 1.
This proves Claim 2.

\vskip 0.5cm
\noindent
\textbf{Claim 3:}
\begin{equation*}
E\!\left[\,\left(\blp(\,Y_{0}\vert Y) \overset{{\color{white}.}}{-} f(Y;0,b)\right)^{2}\,\right]
\;\; = \;\;
	\Var\!\left[\;(b \overset{{\color{white}.}}{-} \delta)^{T}\cdot Y\;\right]
\end{equation*}
Proof of Claim 3:\quad Observe that
\begin{eqnarray*}
\blp(\,Y_{0}\vert Y) - f(Y;0,b)
&=&
	x_{0}\cdot\beta \; + \; \delta^{T}\cdot(\;Y - \,X\cdot\beta\,) \; - \; b^{T} \cdot Y
\;\; = \;\;
	(x_{0} \; - \; \delta^{T} \cdot X) \cdot \beta \; - \; (b - \delta)^{T} \cdot Y
\\
&=&
	- \left\{\;
		(b - \delta)^{T} \cdot Y \; - \; E\!\left[\,(b \overset{{\color{white}.}}{-} \delta)^{T} \cdot Y\,\right]
		 \;\right\}\,,
\end{eqnarray*}
from which Claim 3 immediately follows.

\vskip 0.5cm
\noindent
\textbf{Claim 4:}\quad
$f(Y;0,b)$ is a best linear unbiased predictor of \,$Y_{0}$ if
\begin{equation*}
b \;\; = \;\;
	\underset{c\,\in\,\Re^{n}}{\textnormal{argmin}}
	\left\{\;
		%E\!\left[\,\left(\blp(\,Y_{0}\vert Y) \overset{{\color{white}.}}{-} f(Y;0,c)\right)^{2}\,\right]
		\Var\!\left[\;(c \overset{{\color{white}.}}{-} \delta)^{T}\cdot Y\;\right]
		\;\right\}
\end{equation*}
Proof of Claim 4:\quad Recall that
\begin{eqnarray*}
E\!\left[\,\left(Y_{0} \overset{{\color{white}.}}{-} f(Y;0,b)\right)^{2}\,\right]
& = &
	E\!\left[\,\left(Y_{0} \overset{{\color{white}.}}{-} \blp(\,Y_{0}\vert Y)\right)^{2}\,\right]
	\; + \;
	E\!\left[\,\left(\blp(\,Y_{0}\vert Y) \overset{{\color{white}.}}{-} f(Y;0,b)\right)^{2}\,\right]
\\
& = &
	E\!\left[\,\left(Y_{0} \overset{{\color{white}.}}{-} \blp(\,Y_{0}\vert Y)\right)^{2}\,\right]
	\; + \;
	\Var\!\left[\;(b \overset{{\color{white}.}}{-} \delta)^{T}\cdot Y\;\right]
\end{eqnarray*}
Claim 4 now follows immediately.

\vskip 0.5cm
\noindent
\textbf{Claim 5:}\quad
$f(Y;0,b) = b^{T}\cdot Y$\, is a best linear unbiased predictor (BLUP) of \,$Y_{0}$ if
$(b-\delta)^{T} \cdot Y$ is a best linear unbiased estimator (BLUE) of
$(x_{0} - \delta^{T}\cdot X) \cdot \beta$.

\vskip 0.2cm
\noindent
Proof of Claim 5: \quad Immediate by Claim 2 and Claim 4.
%\vskip 0.5cm
%\noindent
%Claim 2 and Claim 4 together imply that if
%$(b-\delta)^{T} \cdot Y$ is a best linear unbiased estimator of $(x_{0} - \delta^{T}\cdot X) \cdot \beta$,
%then
%\,$f(Y;0,b) = b^{T} \cdot Y$\, is a best linear unbiased predictor of \,$Y_{0}$.

\vskip 0.5cm
\noindent
This completes the proof of the present Theorem.
\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
