
          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\section{BLUP for mixed effects models}
\setcounter{theorem}{0}
\setcounter{equation}{0}

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

\begin{theorem}[Theorem 12.2.3, p.294, \cite{Christensen2011}]
\label{AMixedModelEquivalentFixedModel}
\mbox{}
\vskip 0.2cm
\noindent
Suppose:
\begin{itemize}
\item
	$(\Omega,\mathcal{A},\mu)$ is a probability space.
\item
	$Y_{0} : \Omega \longrightarrow \Re$ is an $\Re$-valued random variable
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$Y = (Y_{1}, Y_{2}, \ldots, Y_{n}) : \Omega \longrightarrow \Re^{n}$ is an $\Re^{n}$-valued random variable
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	$x_{0} \in \Re^{1 \times p}$, $X \in \Re^{n \times p}$, and $\beta \in \Re^{p \times 1}$.
\item
	$e = (e_{1}, e_{2}, \ldots, e_{n}) : \Omega \longrightarrow \Re^{n}$ is an $\Re^{n}$-valued random variable
	defined on $(\Omega,\mathcal{A},\mu)$.
\item
	The quantities $Y$, $X$, $\beta$, $Z$, $\gamma$, and $e$ satisfy the following three properties:
	\begin{equation*}
	\begin{array}{ccll}
	Y & = & X \cdot \beta + e, & \textnormal{$\mu$-almost everywhere}
	\\
	E\!\left[\,e\,\right] &\overset{{\color{white}\vert}}{=}& 0
	\\
	E\!\left[\,Y_{0}\,\right] &\overset{{\color{white}\vert}}{=}& x_{0} \cdot \beta
	\\
	x_{0} &\overset{{\color{white}\vert}}{=}& \rho^{T} \cdot X, & \textnormal{for some \,$\rho \in \Re^{n \times 1}$}
	\end{array}
	\end{equation*}
\item
	The following covariance matrix exist:
	\begin{equation*}
	V \;:=\; \Cov(Y) \;=\; \Cov(e) \;\in\; \Re^{n \times n}
	\end{equation*}
\item
	Recall that the best linear unbiased predictor of \,$Y_{0}$ in terms of \,$Y$ is given by:
	\begin{eqnarray*}
	\blp(\,Y_{0}\vert Y)
	& = &
		E\!\left[\;Y_{0}\,\right] \; + \; \delta^{T}\cdot(\;Y - E\!\left[\;Y\,\right]\,)
	\\
	& = &
		x_{0}^{T}\cdot\beta \; + \; \delta^{T}\cdot(\;Y - X\cdot\beta\,)\,,
	\end{eqnarray*}
	where $\delta \in \Re^{n \times 1}$ is any element in $\Re^{n}$ which satisfies
	$\Cov(Y) \cdot \delta \,=\, \Cov(Y,Y_{0})$.
\item
	Define:
	\begin{eqnarray*}
	\xi & := & Z \cdot \gamma + e
	\\
	V & := & \Cov(Y) \;\; = \;\; \Cov\!\left(\,Z\cdot\gamma+e\,\right) \;\; = \;\; Z \cdot \Cov(\gamma) \cdot Z^{T} + \Cov(e)
	\end{eqnarray*}
\end{itemize}
Then, the quantities $Y$, $X$, $\beta$, $\xi$ and $V$ together satisfy:
	\begin{equation*}
	\begin{array}{ccll}
	Y & = & X \cdot \beta + \xi, & \textnormal{$\mu$-almost everywhere}
	\\
	E\!\left[\,\xi\,\right] &\overset{{\color{white}\vert}}{=}& 0
	\\
	\Cov\!\left(\xi\right) &\overset{{\color{white}\vert}}{=}& V
	\end{array}
	\end{equation*}
\end{theorem}
\proof
Let $f(Y;b_{0},b) \,:=\, b_{0} + b^{T}\cdot Y$, where $b_{0} \in \Re$ and $b \in \Re^{n}$, be a linear unbiased predictor of $Y_{0}$.

\vskip 0.3cm
\noindent
\textbf{Claim 1:}\quad $E\!\left[\,f(Y;b_{0},b)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$
\;\;$\Longrightarrow$\;\; $b_{0} = 0$\,, \;and\; $b \in \Re^{n}$ satisfies $b^{T}\cdot X = x_{0}$.
\mbox{}\vskip 0.2cm\noindent
Proof of Claim 1:\quad Observe that $E\!\left[\,f(Y)\,\right]$ $=$ $E\!\left[\;Y_{0}\,\right]$ implies:
\begin{equation*}
x_{0} \cdot \beta
\;\; = \;\;
	E\!\left[\;Y_{0}\,\right]
\;\; = \;\;
	E\!\left[\,f(Y)\,\right]
\;\; = \;\;
	E\!\left[\,b_{0} + b^{T}\cdot Y\,\right]
\;\; = \;\;
	b_{0} + b^{T}\cdot E\!\left[\;Y\,\right]
\;\; = \;\;
	b_{0} + b^{T}\cdot X \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta\in\Re^{n}$}\,,
\end{equation*}
which in turn implies
\begin{equation*}
b_{0}
\;\; = \;\;
	\left(\,x_{0} - b^{T}\cdot X\,\right) \cdot \beta\,,
	\quad
	\textnormal{for each \,$\beta\in\Re^{p}$}\,.
\end{equation*}
Since the left-hand side of the preceding equation does not involve $\beta$
while the right-hand side does, we see that both sides must in fact be zero for the equality to hold.
In other words, we must have $b_{0} = 0$ and that $b \in \Re^{n}$ satisfies $b^{T} \cdot X = x_{0}$.
This proves Claim 1.

\vskip 0.5cm
\noindent
Recall that the best linear predictor of \,$Y_{0}$ in terms of \,$Y$ is given by:
\begin{eqnarray*}
\blp(\,Y_{0}\vert Y)
& = &
	E\!\left[\;Y_{0}\,\right] \; + \; \delta^{T}\cdot(\;Y - E\!\left[\;Y\,\right]\,)
\\
& = &
	\,x_{0}\cdot\beta{\color{white}.} \; + \; \delta^{T}\cdot(\;Y - \,X\cdot\beta\,\;)\,,
\end{eqnarray*}
where $\delta \in \Re^{n \times 1}$ is any element in $\Re^{n}$ which satisfies
$\Cov(Y) \cdot \delta \,=\, \Cov(Y,Y_{0})$.

\vskip 0.5cm
\noindent
\textbf{Claim 2:}\quad
$(b - \delta)^{T}\cdot Y$ is a linear unbiased estimator of \,$(x_{0} - \delta^{T}\cdot X)\cdot\beta$.
\vskip 0.2cm
\noindent
Proof of Claim 2:\quad
Simply note that
\begin{equation*}
E\!\left[\,(b - \delta)^{T}\cdot Y\,\right]
\;\; = \;\;
	(b - \delta)^{T} \cdot E\!\left[\;Y\,\right]
\;\; = \;\;
	(b - \delta)^{T} \cdot X \cdot \beta
\;\; = \;\;
	(b^{T}\cdot X - \delta^{T} \cdot X) \cdot \beta
\;\; = \;\;
	(x_{0} - \delta^{T} \cdot X) \cdot \beta\,,
\end{equation*}
where the last equality follows from Claim 1.
This proves Claim 2.

\vskip 0.5cm
\noindent
\textbf{Claim 3:}\quad
$f(Y;0,b)$ is a best linear unbiased predictor of \,$Y_{0}$ if
\begin{equation*}
b \;\; = \;\;
	\underset{c\,\in\,\Re^{n}}{\textnormal{argmin}}
	\left\{\;E\!\left[\,\left(\blp(\,Y_{0}\vert Y) \overset{{\color{white}.}}{-} c^{T} \cdot Y\right)^{2}\,\right]\;\right\}
\end{equation*}
Proof of Claim 3:\quad Recall that
\begin{equation*}
E\!\left[\,\left(Y_{0} \overset{{\color{white}.}}{-} b^{T}\cdot Y\right)^{2}\,\right]
\;\; = \;\;
	E\!\left[\,\left(Y_{0} \overset{{\color{white}.}}{-} \blp(\,Y_{0}\vert Y)\right)^{2}\,\right]
	\; + \;
	E\!\left[\,\left(\blp(\,Y_{0}\vert Y) \overset{{\color{white}.}}{-} b^{T}\cdot Y\right)^{2}\,\right]
\end{equation*}
Claim 3 now follows immediately.

\qed

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%

%\renewcommand{\theenumi}{\alph{enumi}}
%\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}
\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{\textnormal{(\theenumi)}$\;\;$}

          %%%%% ~~~~~~~~~~~~~~~~~~~~ %%%%%
